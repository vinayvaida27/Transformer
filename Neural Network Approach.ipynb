{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Alternative Approach of Bi-gram model into Neural Network",
   "id": "e3f3a175cfe194e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Our Neural network model is still going to be Bi-gram character level model",
   "id": "8f0f1d110bc81ea8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T14:42:56.672507Z",
     "start_time": "2025-07-09T14:42:56.641805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Creating the training set of the Bi-gram\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "words = open('names.txt').read().splitlines()\n",
    "chars = (sorted(list(set(''.join(words)))))# Now we are taking all the names join them as massive string and throws out all the duplicates using set function and get the list of strings used\n",
    "\n",
    "stoi = { s:i+1 for i,s in enumerate(chars)}# String to integer(stoi) is a dictionary, where we are converting above used string characters  to integer.\n",
    "stoi['.'] = 0 #Now we want to add '.' to this as well to our character to integer dictionary.\n",
    "itos = { i:s for s,i in stoi.items()}## Now Similarly we need to create the integer to string converter"
   ],
   "id": "3b41a1336e9e86be",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:02:28.147844Z",
     "start_time": "2025-07-09T15:02:28.141756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the training set of bigrams(x,y)\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.'] #we are taking the character string, because we want to see how the distribution is\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "        print(ch1, ch2)\n",
    "\n",
    "#we are constructing xs and ys a tensor\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ],
   "id": "f81f01551302b915",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:02:29.010887Z",
     "start_time": "2025-07-09T15:02:29.006372Z"
    }
   },
   "cell_type": "code",
   "source": "xs",
   "id": "166630df11b541a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:02:29.818977Z",
     "start_time": "2025-07-09T15:02:29.814005Z"
    }
   },
   "cell_type": "code",
   "source": "ys",
   "id": "d6d9ca1765a2a17a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:02:31.606269Z",
     "start_time": "2025-07-09T15:02:31.601353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\" so what we the xs and ys tell you is\n",
    "1)as we are using bi-gram when one character is input, we need to get the another charter as o/p\n",
    "2)when xs[0] is input we have to have high probability to get the o/p ys[0] as next character i.e., when the input is 0 we need to get 5 as the next character with high probability i.e.,\n",
    "3) in other terms when (.) is input to model we need to provide high probability to get the character (e) next because in .emma. the first 2 character are . and e\n",
    "\n",
    "4) similarly when we need to have high probabilities\n",
    "  a) for this example .emma. -> (.) as input (e) must has high probability o/p     ie.,  -> (0) as i/p -> high probability o/p must be ->(5)\n",
    "                             -> (e) as input (m) must has high probability o/p     ie.,  -> (5) as i/p -> high probability o/p must be ->(13)\n",
    "                             -> (m) as input (m) must has high probability o/p     ie.,  -> (13) as i/p -> high probability o/p must be ->(13)\n",
    "                             -> (m) as input (a) must has high probability o/p     ie.,  -> (13) as i/p -> high probability o/p must be ->(1)\n",
    "                             -> (a) as input (.) must has high probability o/p     ie.,  -> (1) as i/p -> high probability o/p must be ->(0)\n",
    "\"\"\""
   ],
   "id": "25921696dfe71610",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' so what we the xs and ys tell you is\\n1)as we are using bi-gram when one character is input, we need to get the another charter as o/p\\n2)when xs[0] is input we have to have high probability to get the o/p ys[0] as next character i.e., when the input is 0 we need to get 5 as the next character with high probability i.e.,\\n3) in other terms when (.) is input to model we need to provide high probability to get the character (e) next because in .emma. the first 2 character are . and e\\n\\n4) similarly when we need to have high probabilities\\n  a) for this example .emma. -> (.) as input (e) must has high probability o/p     ie.,  -> (0) as i/p -> high probabily o/p must be ->(5)\\n                             -> (e) as input (m) must has high probability o/p     ie.,  -> (5) as i/p -> high probabily o/p must be ->(13)\\n                             -> (m) as input (m) must has high probability o/p     ie.,  -> (13) as i/p -> high probabily o/p must be ->(13)\\n                             -> (m) as input (a) must has high probability o/p     ie.,  -> (13) as i/p -> high probabily o/p must be ->(1)\\n                             -> (a) as input (.) must has high probability o/p     ie.,  -> (1) as i/p -> high probabily o/p must be ->(0)\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:06:08.684981Z",
     "start_time": "2025-07-09T15:06:08.679346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## now we need to assume how we need to train this numbers to neural networks\n",
    "\n",
    "## we need to use one hot encoding to encode this number vector\n",
    "\n",
    "import torch.nn.functional as F\n",
    "xs_encoded = F.one_hot(xs, num_classes=27)\n",
    "xs_encoded"
   ],
   "id": "2686ce01b2529c2e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:51:31.108720Z",
     "start_time": "2025-07-09T15:51:31.103325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# the neural networks like to have the flot number as input so the encoded values must in the float time\n",
    "xs_encoded = xs_encoded.float()\n",
    "xs_encoded"
   ],
   "id": "f98fc48c14273a71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Randomly initialization of weights",
   "id": "4417199022798878"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:49:24.758373Z",
     "start_time": "2025-07-09T15:49:24.750669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "W = torch.randn((27,27))\n",
    "W"
   ],
   "id": "38fa7a002b8b4e86",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0086e+00,  5.8980e-02, -1.7584e+00, -2.7064e-01, -1.9299e-01,\n",
       "          1.0229e+00, -9.2017e-01,  8.9978e-01, -4.0465e-01,  1.4029e-01,\n",
       "          2.4143e-02,  2.0946e+00, -5.3770e-01,  4.0817e-01,  8.5065e-01,\n",
       "          3.0688e-01, -5.8246e-01, -6.2027e-01,  1.4835e+00, -7.5611e-01,\n",
       "         -6.9630e-01, -4.3281e-01,  1.3918e+00, -1.2550e-01, -9.8927e-01,\n",
       "          8.7319e-01, -2.6244e-01],\n",
       "        [ 8.8267e-01,  6.3231e-01,  6.8623e-01, -4.6905e-01, -4.4160e-01,\n",
       "          4.0951e-01, -9.6335e-01,  1.2914e+00,  2.8186e-01,  2.9014e-01,\n",
       "         -2.4149e-01, -1.0776e+00, -1.2465e+00,  8.2353e-01,  1.6169e-01,\n",
       "         -3.3944e-01, -5.4989e-01,  7.5682e-01, -3.1158e-01,  1.1534e+00,\n",
       "         -2.8485e-01, -7.4605e-01,  3.8226e-01, -5.1608e-01,  1.0015e+00,\n",
       "          1.4688e+00,  4.3623e-01],\n",
       "        [ 1.4937e+00, -8.8609e-01, -5.9392e-03, -1.4784e+00,  1.0096e+00,\n",
       "          2.1784e-01,  1.6418e+00,  5.8012e-01, -4.0046e-01,  4.3673e-02,\n",
       "          3.5640e-01,  1.6198e+00, -4.0474e-02, -3.0481e-01, -1.0508e+00,\n",
       "          1.7493e-01,  1.2024e+00, -5.6560e-01, -4.4000e-01, -4.9897e-01,\n",
       "          1.0347e+00, -2.7633e+00, -2.3213e-01,  1.1336e+00, -6.3784e-01,\n",
       "         -2.1028e-01,  1.7908e-01],\n",
       "        [-6.2721e-01,  1.0315e-01, -1.7712e+00,  1.5783e+00,  5.1874e-01,\n",
       "         -2.3740e-01,  2.9402e-01,  7.2722e-01, -1.0021e+00, -1.8647e-01,\n",
       "          2.2508e+00, -1.7873e-01,  1.2033e-01,  6.9929e-01, -1.3514e-01,\n",
       "         -1.2909e+00,  1.2419e+00,  5.6642e-01, -5.5445e-01,  1.2875e+00,\n",
       "          1.3552e+00,  2.4856e-01, -1.4295e+00, -7.9462e-01, -1.0163e+00,\n",
       "          4.7313e-01,  1.6209e+00],\n",
       "        [ 2.1976e+00, -4.1083e-01, -7.6394e-01, -3.5155e-01,  5.1715e-01,\n",
       "          8.2644e-01,  1.5517e+00, -1.2821e+00,  3.5890e-01,  7.2187e-01,\n",
       "         -3.8107e-01, -8.1395e-01, -2.4358e-01,  5.3889e-01,  2.7039e-01,\n",
       "          1.2660e+00, -1.4132e-01,  1.2035e-02,  1.1786e-01,  2.0185e-01,\n",
       "          8.6918e-01, -3.8382e-01,  1.3806e+00, -5.7482e-02,  1.0569e-01,\n",
       "          5.1879e-01, -4.6821e-01],\n",
       "        [ 2.1285e-01, -3.6751e-01,  4.2756e-01,  6.8599e-01,  2.7290e+00,\n",
       "         -1.3649e+00, -1.4858e+00,  1.1199e+00, -7.2529e-01, -7.9395e-01,\n",
       "         -4.1709e-01,  2.4849e-01, -2.0380e+00, -2.0876e+00, -6.0041e-01,\n",
       "          7.0728e-01,  4.5430e-01, -5.5198e-01,  2.1249e+00, -1.2828e+00,\n",
       "         -9.4565e-01,  4.2430e-01,  1.5663e+00,  1.1979e+00,  5.9750e-01,\n",
       "          5.8491e-01, -2.8889e-01],\n",
       "        [ 1.4049e+00, -4.2487e-01,  5.5795e-02, -2.8339e-01,  6.2440e-01,\n",
       "          7.0995e-01, -7.2126e-01,  1.2472e+00, -3.3483e-01,  1.7635e-01,\n",
       "          8.0405e-01,  2.4438e+00, -4.3379e-01, -7.1079e-01, -1.7922e-01,\n",
       "         -8.7545e-01,  6.6594e-01,  8.2449e-02, -1.4507e+00,  1.2740e+00,\n",
       "         -1.1715e+00,  7.2833e-01,  2.2328e-01,  6.9895e-01, -2.6857e-01,\n",
       "         -1.0724e+00, -1.3163e+00],\n",
       "        [ 1.6639e+00, -3.0763e-01,  1.6245e+00, -2.5231e+00, -3.2292e-01,\n",
       "         -5.0400e-01,  4.8026e-01, -3.0915e-01,  8.9702e-02, -1.6477e+00,\n",
       "         -1.5902e+00, -1.4078e-01,  1.2198e+00,  9.1815e-01, -2.9091e-01,\n",
       "          2.5154e-01,  7.7047e-01, -7.2208e-02,  9.2687e-01, -2.1091e-01,\n",
       "         -6.8917e-01,  1.3949e+00,  7.1621e-01, -8.3045e-01,  1.9998e+00,\n",
       "          3.4594e-01, -1.6148e-01],\n",
       "        [ 5.9921e-01, -7.4093e-01,  7.1904e-01, -3.3862e-01, -1.4801e+00,\n",
       "         -1.8510e+00,  4.0930e-02,  2.7290e+00, -1.6244e-01,  5.9250e-01,\n",
       "         -1.6422e+00,  1.5029e+00, -1.5572e+00,  1.8123e+00,  3.1453e-01,\n",
       "          2.4488e-01,  4.3958e-01, -3.6820e-01, -2.8320e+00, -2.2893e+00,\n",
       "          2.1584e-01,  2.9212e+00, -1.0909e+00, -3.6732e-01, -6.5247e-01,\n",
       "         -2.6811e-02, -8.8135e-01],\n",
       "        [-3.2358e-01,  8.1099e-01,  9.2468e-01,  7.7765e-01,  6.5523e-01,\n",
       "          6.1579e-02, -1.0042e-01,  2.3173e-01, -1.5182e+00, -9.1141e-01,\n",
       "          3.8883e-01,  7.7356e-01, -1.1723e-01,  2.2383e-01, -2.3061e+00,\n",
       "          1.5443e+00,  6.4252e-01, -1.2704e+00, -9.9955e-01, -1.0260e+00,\n",
       "          5.8747e-01,  2.6061e-02,  3.1045e-01, -4.5319e-01,  3.5416e-01,\n",
       "         -9.1117e-03,  4.5633e-02],\n",
       "        [ 1.5969e+00, -1.7453e+00, -1.6440e+00,  5.0823e-01,  2.1820e+00,\n",
       "         -1.9364e-01, -1.4087e+00, -3.4569e-01,  7.7016e-01,  1.6368e+00,\n",
       "         -5.6210e-01,  1.2147e+00,  9.3928e-01, -2.0780e+00,  1.4327e+00,\n",
       "          1.1018e+00,  7.5215e-01,  1.1099e+00, -1.3353e-01,  4.1375e-01,\n",
       "         -4.5680e-01,  8.7553e-01,  2.4438e+00,  4.7302e-02, -3.4862e-01,\n",
       "         -4.7731e-02,  9.2618e-01],\n",
       "        [ 7.5774e-01,  4.6246e-01, -5.8623e-02, -1.0057e+00,  4.6119e-01,\n",
       "         -1.0608e+00,  3.0429e-01, -4.1851e-01, -2.1248e-01, -1.8291e+00,\n",
       "         -4.1439e-02, -1.6112e-01, -1.7565e+00,  9.1368e-01, -8.1712e-01,\n",
       "         -1.0231e+00,  1.5766e+00,  2.8497e-01,  1.6784e+00,  3.1609e-01,\n",
       "         -4.4856e-01,  4.3808e-01,  1.1810e-01,  7.5281e-01, -7.1353e-01,\n",
       "         -1.6645e+00, -5.5147e-01],\n",
       "        [ 1.9698e+00,  1.1070e+00, -5.3247e-01, -1.1111e-01, -3.7933e-01,\n",
       "          1.6182e+00, -1.7247e+00, -6.7264e-01, -1.7284e+00,  2.8853e-01,\n",
       "          1.1066e+00, -6.7778e-02,  5.9709e-01, -8.1281e-01,  4.9001e-02,\n",
       "         -2.2928e-01,  7.2167e-01,  8.6598e-01,  1.8175e+00, -7.8647e-01,\n",
       "          7.5065e-02, -2.1782e-01,  9.4096e-01,  1.3526e+00, -1.0547e+00,\n",
       "          1.8108e+00,  3.4446e-01],\n",
       "        [-2.0329e+00,  1.4039e-01,  2.8447e+00, -3.5985e-01,  1.2112e+00,\n",
       "          8.5867e-01,  3.0139e-01, -1.7417e+00,  4.5785e-01, -1.3648e+00,\n",
       "          4.2123e-01, -9.6245e-01,  3.6976e-01, -1.0630e+00, -1.3633e+00,\n",
       "          2.3991e+00, -1.5942e+00, -7.5012e-01, -6.3898e-01,  1.2018e+00,\n",
       "         -1.0455e+00, -3.2845e-01,  3.8634e-01, -3.5042e-02,  2.0510e-01,\n",
       "         -3.0737e-01,  1.0788e+00],\n",
       "        [-1.2169e+00, -1.5401e-01,  6.9945e-01, -1.0422e+00, -7.0852e-01,\n",
       "          2.2851e-01, -2.6398e-01,  3.3976e-01, -1.5309e+00,  1.0246e+00,\n",
       "         -8.7025e-01,  8.7413e-02,  3.9562e-01, -1.0234e+00,  1.0501e+00,\n",
       "          1.1963e+00,  9.3109e-01,  1.0578e+00,  1.8570e-01,  5.4864e-01,\n",
       "          1.2261e+00,  8.7650e-03,  1.5390e-02, -8.8259e-01,  2.2642e+00,\n",
       "          1.5869e-01, -5.7096e-01],\n",
       "        [ 8.4991e-01, -2.6551e-02,  1.6628e+00,  2.8275e+00,  2.8120e-01,\n",
       "          1.2357e+00, -4.3667e-01,  1.3632e+00,  1.2180e+00,  5.3523e-04,\n",
       "         -5.5004e-01, -1.6779e-02, -5.0677e-01, -5.5780e-01,  1.1204e+00,\n",
       "          2.4942e-01, -5.0714e-03, -6.1843e-02, -1.3655e+00,  1.4891e+00,\n",
       "          8.7243e-01, -4.6126e-01, -6.2945e-01, -1.0494e+00,  2.6237e-01,\n",
       "         -4.8641e-01,  4.4983e-01],\n",
       "        [ 7.5609e-01,  5.9602e-01, -9.8637e-01,  6.3416e-01,  1.3486e+00,\n",
       "          8.1796e-01, -7.4881e-01,  9.2896e-01,  1.0836e-01, -1.1675e+00,\n",
       "          1.7941e+00,  7.3467e-01,  3.9310e-01, -3.5218e-01,  5.8431e-01,\n",
       "          9.8067e-01, -7.5686e-01, -7.5648e-01,  3.5859e-01,  5.5671e-01,\n",
       "         -9.4428e-01,  1.1965e-01,  7.6438e-01, -4.2735e-01, -1.9155e+00,\n",
       "         -5.0351e-01, -1.3137e+00],\n",
       "        [ 9.4880e-01,  1.2334e-01, -3.4948e-01, -2.0210e-01, -1.0322e+00,\n",
       "         -1.3272e+00,  1.4086e+00,  1.0066e+00, -1.2939e+00, -1.0190e-01,\n",
       "         -5.0364e-01,  3.2857e-02, -1.3884e+00,  6.6095e-01,  1.4449e+00,\n",
       "         -8.7653e-01,  4.6354e-01, -4.1319e-01,  5.6472e-01, -2.7338e-01,\n",
       "         -9.0427e-01,  1.4009e-01, -6.9034e-01, -5.3148e-01,  3.9826e-01,\n",
       "          1.6194e+00, -3.5906e-01],\n",
       "        [-1.0405e+00,  3.4193e-02,  7.3600e-01,  1.2887e+00,  4.7847e-01,\n",
       "         -2.5170e+00,  4.0762e-01,  1.4067e+00, -8.5804e-01,  2.4017e-01,\n",
       "          1.5567e+00,  1.1361e+00,  3.0839e-02, -9.5707e-01,  4.4358e-01,\n",
       "         -8.7291e-01,  5.8137e-01, -6.7786e-01,  1.0694e+00, -2.3317e-01,\n",
       "         -9.2763e-01, -9.3109e-01,  8.4170e-01,  1.0984e+00, -1.8909e-01,\n",
       "          1.4748e+00, -2.0971e-01],\n",
       "        [ 9.5452e-01,  4.6718e-01,  2.9395e-01,  1.6043e+00,  5.7186e-01,\n",
       "          1.5373e+00, -1.6127e+00, -6.4214e-01,  1.5289e+00,  1.2039e+00,\n",
       "          1.1607e-02,  6.0337e-01,  1.0088e+00, -4.7395e-02, -2.3797e-01,\n",
       "         -7.8949e-01,  1.0691e-02,  1.4091e-01,  6.6321e-01, -4.4348e-01,\n",
       "         -5.2076e-01, -1.6371e-01,  9.9938e-01, -1.1917e-01, -5.5250e-01,\n",
       "         -5.5234e-01,  1.1358e+00],\n",
       "        [ 8.4426e-01, -1.9868e+00, -9.4015e-01, -1.5346e+00, -6.6695e-01,\n",
       "         -1.2196e-01, -3.0869e-01,  6.4346e-02, -7.5782e-02, -1.2078e+00,\n",
       "         -4.4639e-01,  9.0913e-01, -9.4506e-01,  1.3791e+00, -1.0697e+00,\n",
       "          4.4170e-01, -5.4536e-01,  1.1650e+00,  1.1974e+00, -7.8159e-01,\n",
       "          3.2186e-01, -1.2969e-02, -4.4127e-01,  4.9397e-01, -1.4292e+00,\n",
       "         -5.2622e-01, -1.9129e+00],\n",
       "        [ 6.5442e-02, -9.7925e-01,  2.2523e+00,  9.2066e-01,  1.1100e+00,\n",
       "          4.7160e-01, -3.9830e-02,  7.8022e-02, -9.1394e-02, -1.8237e+00,\n",
       "          1.1844e-01,  1.2368e-01, -3.8439e-01, -2.8905e-01,  4.3619e-02,\n",
       "         -1.5430e+00,  1.3833e-01, -3.1595e+00, -9.7882e-01,  3.1471e-01,\n",
       "          4.8430e-01,  9.1244e-02, -1.0876e+00,  9.3907e-01, -1.0577e+00,\n",
       "         -2.0662e-01,  1.9294e-01],\n",
       "        [ 2.0256e+00,  1.1547e-01, -5.0822e-01, -7.2110e-01,  1.5710e+00,\n",
       "         -9.9955e-01, -7.2095e-01,  1.2137e-01,  4.3266e-01,  3.0318e-01,\n",
       "          6.4328e-01,  6.8081e-01, -3.6880e-01,  1.7386e+00, -1.3206e+00,\n",
       "          2.2618e+00, -4.9047e-01,  3.3595e-01, -4.8543e-01,  8.5226e-01,\n",
       "         -1.3934e-01,  6.4915e-01,  1.9817e-01, -3.6110e-01,  1.1075e+00,\n",
       "         -4.0945e-01,  1.0460e-01],\n",
       "        [ 1.2115e+00,  1.7713e-01,  6.5975e-01,  1.8226e+00, -9.3789e-01,\n",
       "          1.0606e+00,  2.1075e-01,  9.7431e-01,  1.6225e+00, -1.9923e-01,\n",
       "          5.6643e-01,  1.8749e+00,  5.6794e-01,  1.1251e-01, -1.0917e+00,\n",
       "          1.0564e+00, -1.0074e+00,  1.9032e+00,  3.4840e-01, -1.5226e+00,\n",
       "         -5.3770e-01,  6.6799e-01, -8.5934e-01, -1.5939e+00,  1.3104e-01,\n",
       "         -2.0703e+00, -1.3113e+00],\n",
       "        [ 7.8005e-01, -7.5058e-03, -1.3157e+00,  6.9087e-01,  1.8305e-01,\n",
       "         -1.4750e+00, -7.2224e-01, -1.3716e+00,  3.8897e-01, -7.4106e-01,\n",
       "          1.8989e+00, -1.5058e+00, -6.6812e-02,  1.1609e-01,  2.6217e+00,\n",
       "          6.9177e-01,  8.6362e-01,  1.3284e+00,  1.4394e+00,  7.1938e-01,\n",
       "          1.1771e+00, -6.5740e-01, -8.9313e-01,  1.0290e+00,  2.6825e-01,\n",
       "          8.4072e-01, -8.4409e-01],\n",
       "        [ 8.3815e-01, -4.9547e-01,  1.1172e-01,  2.6525e-01, -6.9650e-01,\n",
       "          4.3523e-01,  1.0164e+00, -1.5315e-01,  1.1015e+00,  1.6719e+00,\n",
       "         -7.2912e-01, -7.1187e-01, -4.2703e-02, -1.2026e+00,  7.7171e-02,\n",
       "          3.7435e-01,  1.9732e-01, -5.4018e-01, -9.0765e-01,  1.1611e+00,\n",
       "         -1.2048e+00,  4.5258e-01,  1.0473e+00, -9.9489e-01,  2.5947e-01,\n",
       "         -1.0894e+00, -2.2559e-01],\n",
       "        [ 3.8166e-01, -1.2770e+00,  1.8801e+00,  4.1694e-01, -1.4451e+00,\n",
       "         -5.8594e-01, -1.6866e+00, -2.4614e-01,  9.2800e-01, -5.5083e-01,\n",
       "         -2.4714e+00, -2.2175e+00, -8.4460e-01, -5.8225e-02, -2.3747e+00,\n",
       "          1.4215e-01, -1.3300e-01, -2.5216e-01, -1.3799e+00,  1.2917e+00,\n",
       "         -6.7831e-01,  6.4919e-01, -1.7330e-01, -5.7347e-01, -1.3165e-01,\n",
       "          3.7662e-01,  1.9297e+00]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:49:29.113952Z",
     "start_time": "2025-07-09T15:49:29.108737Z"
    }
   },
   "cell_type": "code",
   "source": "xs_encoded @ W #@ is is matrix multiplication operator in torch",
   "id": "407f83e5241428c7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0086,  0.0590, -1.7584, -0.2706, -0.1930,  1.0229, -0.9202,  0.8998,\n",
       "         -0.4046,  0.1403,  0.0241,  2.0946, -0.5377,  0.4082,  0.8506,  0.3069,\n",
       "         -0.5825, -0.6203,  1.4835, -0.7561, -0.6963, -0.4328,  1.3918, -0.1255,\n",
       "         -0.9893,  0.8732, -0.2624],\n",
       "        [ 0.2128, -0.3675,  0.4276,  0.6860,  2.7290, -1.3649, -1.4858,  1.1199,\n",
       "         -0.7253, -0.7939, -0.4171,  0.2485, -2.0380, -2.0876, -0.6004,  0.7073,\n",
       "          0.4543, -0.5520,  2.1249, -1.2828, -0.9456,  0.4243,  1.5663,  1.1979,\n",
       "          0.5975,  0.5849, -0.2889],\n",
       "        [-2.0329,  0.1404,  2.8447, -0.3599,  1.2112,  0.8587,  0.3014, -1.7417,\n",
       "          0.4578, -1.3648,  0.4212, -0.9624,  0.3698, -1.0630, -1.3633,  2.3991,\n",
       "         -1.5942, -0.7501, -0.6390,  1.2018, -1.0455, -0.3285,  0.3863, -0.0350,\n",
       "          0.2051, -0.3074,  1.0788],\n",
       "        [-2.0329,  0.1404,  2.8447, -0.3599,  1.2112,  0.8587,  0.3014, -1.7417,\n",
       "          0.4578, -1.3648,  0.4212, -0.9624,  0.3698, -1.0630, -1.3633,  2.3991,\n",
       "         -1.5942, -0.7501, -0.6390,  1.2018, -1.0455, -0.3285,  0.3863, -0.0350,\n",
       "          0.2051, -0.3074,  1.0788],\n",
       "        [ 0.8827,  0.6323,  0.6862, -0.4690, -0.4416,  0.4095, -0.9633,  1.2914,\n",
       "          0.2819,  0.2901, -0.2415, -1.0776, -1.2465,  0.8235,  0.1617, -0.3394,\n",
       "         -0.5499,  0.7568, -0.3116,  1.1534, -0.2849, -0.7461,  0.3823, -0.5161,\n",
       "          1.0015,  1.4688,  0.4362]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# we are doing matrixs multiplication to get the o/p of the 1st layer",
   "id": "76cba3c35702c884"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:53:27.823017Z",
     "start_time": "2025-07-09T15:53:27.818344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## xs_encode = [5*27] matrix\n",
    "## W =[27 * 1 ] Matrix\n",
    "## Xs Encode * W = [5*27] * [27*27] = [5*27]\n",
    "(xs_encoded @ W) [1,1]"
   ],
   "id": "d7126c0cc0e0ca03",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3675)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "I will run though an example to explain how matrix multiplication is done\n",
    "\n",
    "1. we consider the row one of matix xs_encoded and multiple it with the column one of matrix W this is nothing but the dot product of an element\n",
    "2. we can do it by writing down the function xs_encoded @ w similfies this process much fatser\n",
    "regular example beloe\n",
    "\"\"\""
   ],
   "id": "2b52f25df65a1ae0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T15:56:35.290650Z",
     "start_time": "2025-07-09T15:56:35.283929Z"
    }
   },
   "cell_type": "code",
   "source": "(xs_encoded[1] * W[:,1]).sum()",
   "id": "cce7321d14f3bc41",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3675)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#you can notice we got the same result\n",
    "\"\"\"\n",
    "1. we want to provide probability distribution for each of the 27 characters\n",
    "2. but we have to come out with precise semantics for exactly how we can interpret this numbers.\n",
    "3. probabilities has only positive numbers and counts are also integers and positive, counts are not really good to o/p from neural network\n",
    "4.so what neural net going to o/p? How are we going to interpret the number? the Neural network give us log count instead of 27 numbers not like tensor matrix with counts in it.\n",
    "5. instead what NN gives is log count then we take exponential  we get the values between [0 and infinity]\n",
    "\n",
    "\n",
    "see below for exponenetial\n",
    "\n",
    "\n",
    "\"\"\""
   ],
   "id": "520cb13f41e43961"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T16:18:13.685446Z",
     "start_time": "2025-07-09T16:18:13.679136Z"
    }
   },
   "cell_type": "code",
   "source": "(xs_encoded @ W).exp()",
   "id": "3f668db715f86036",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3647,  1.0608,  0.1723,  0.7629,  0.8245,  2.7814,  0.3984,  2.4591,\n",
       "          0.6672,  1.1506,  1.0244,  8.1221,  0.5841,  1.5041,  2.3412,  1.3592,\n",
       "          0.5585,  0.5378,  4.4084,  0.4695,  0.4984,  0.6487,  4.0219,  0.8821,\n",
       "          0.3718,  2.3945,  0.7692],\n",
       "        [ 1.2372,  0.6925,  1.5335,  1.9857, 15.3177,  0.2554,  0.2263,  3.0646,\n",
       "          0.4842,  0.4521,  0.6590,  1.2821,  0.1303,  0.1240,  0.5486,  2.0285,\n",
       "          1.5751,  0.5758,  8.3725,  0.2773,  0.3884,  1.5285,  4.7890,  3.3130,\n",
       "          1.8176,  1.7948,  0.7491],\n",
       "        [ 0.1310,  1.1507, 17.1971,  0.6978,  3.3574,  2.3600,  1.3517,  0.1752,\n",
       "          1.5807,  0.2554,  1.5238,  0.3820,  1.4474,  0.3454,  0.2558, 11.0133,\n",
       "          0.2031,  0.4723,  0.5278,  3.3260,  0.3515,  0.7200,  1.4716,  0.9656,\n",
       "          1.2276,  0.7354,  2.9412],\n",
       "        [ 0.1310,  1.1507, 17.1971,  0.6978,  3.3574,  2.3600,  1.3517,  0.1752,\n",
       "          1.5807,  0.2554,  1.5238,  0.3820,  1.4474,  0.3454,  0.2558, 11.0133,\n",
       "          0.2031,  0.4723,  0.5278,  3.3260,  0.3515,  0.7200,  1.4716,  0.9656,\n",
       "          1.2276,  0.7354,  2.9412],\n",
       "        [ 2.4173,  1.8820,  1.9862,  0.6256,  0.6430,  1.5061,  0.3816,  3.6378,\n",
       "          1.3256,  1.3366,  0.7855,  0.3404,  0.2875,  2.2785,  1.1755,  0.7122,\n",
       "          0.5770,  2.1315,  0.7323,  3.1690,  0.7521,  0.4742,  1.4656,  0.5969,\n",
       "          2.7223,  4.3442,  1.5469]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## we can notice all the values are +ve and this are counts",
   "id": "c8b539e83f89f8a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T16:23:43.302197Z",
     "start_time": "2025-07-09T16:23:43.296194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logits = xs_encoded @ W #which are technically called log counts\n",
    "counts = logits.exp() #Equivalent to N Matrix with counts we made earlier\n",
    "# now we want to normalise the values by row/layer\n",
    "\n",
    "probs = counts/counts.sum(1, keepdims = True)\n",
    "probs"
   ],
   "id": "c68ce500fb5deef8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0089, 0.0258, 0.0042, 0.0185, 0.0200, 0.0676, 0.0097, 0.0598, 0.0162,\n",
       "         0.0280, 0.0249, 0.1974, 0.0142, 0.0366, 0.0569, 0.0330, 0.0136, 0.0131,\n",
       "         0.1072, 0.0114, 0.0121, 0.0158, 0.0978, 0.0214, 0.0090, 0.0582, 0.0187],\n",
       "        [0.0224, 0.0125, 0.0278, 0.0360, 0.2775, 0.0046, 0.0041, 0.0555, 0.0088,\n",
       "         0.0082, 0.0119, 0.0232, 0.0024, 0.0022, 0.0099, 0.0367, 0.0285, 0.0104,\n",
       "         0.1517, 0.0050, 0.0070, 0.0277, 0.0868, 0.0600, 0.0329, 0.0325, 0.0136],\n",
       "        [0.0023, 0.0205, 0.3062, 0.0124, 0.0598, 0.0420, 0.0241, 0.0031, 0.0281,\n",
       "         0.0045, 0.0271, 0.0068, 0.0258, 0.0061, 0.0046, 0.1961, 0.0036, 0.0084,\n",
       "         0.0094, 0.0592, 0.0063, 0.0128, 0.0262, 0.0172, 0.0219, 0.0131, 0.0524],\n",
       "        [0.0023, 0.0205, 0.3062, 0.0124, 0.0598, 0.0420, 0.0241, 0.0031, 0.0281,\n",
       "         0.0045, 0.0271, 0.0068, 0.0258, 0.0061, 0.0046, 0.1961, 0.0036, 0.0084,\n",
       "         0.0094, 0.0592, 0.0063, 0.0128, 0.0262, 0.0172, 0.0219, 0.0131, 0.0524],\n",
       "        [0.0607, 0.0472, 0.0499, 0.0157, 0.0161, 0.0378, 0.0096, 0.0913, 0.0333,\n",
       "         0.0336, 0.0197, 0.0085, 0.0072, 0.0572, 0.0295, 0.0179, 0.0145, 0.0535,\n",
       "         0.0184, 0.0796, 0.0189, 0.0119, 0.0368, 0.0150, 0.0683, 0.1091, 0.0388]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T16:24:26.026951Z",
     "start_time": "2025-07-09T16:24:26.022316Z"
    }
   },
   "cell_type": "code",
   "source": "probs[0]",
   "id": "9efdbc91831a3b5e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0089, 0.0258, 0.0042, 0.0185, 0.0200, 0.0676, 0.0097, 0.0598, 0.0162,\n",
       "        0.0280, 0.0249, 0.1974, 0.0142, 0.0366, 0.0569, 0.0330, 0.0136, 0.0131,\n",
       "        0.1072, 0.0114, 0.0121, 0.0158, 0.0978, 0.0214, 0.0090, 0.0582, 0.0187])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T16:44:26.334098Z",
     "start_time": "2025-07-09T16:44:26.327099Z"
    }
   },
   "cell_type": "code",
   "source": "probs[0].sum() # it will be always one",
   "id": "f1ff48a5f68bceea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## explanation\n",
    "\"\"\"\n",
    "1) input_char = .emma ->stoi function to get [0, 5, 13, 13, 1]\n",
    "\n",
    "a) Xs = [0, 5, 13, 13, 1] #which is input\n",
    "b) Xs_encoded = tensor with onehot encoding 5 numbers in input  * 27 characters  so encoded tensor matrix is [5*27]\n",
    "c) we feed only the first character as input to the neural network\n",
    "d) it has the hidden layer which does softmax function ( i.e., Ek/sum ek )\n",
    "e) we didn't directly use softmax we calculated it like this... log counts = encoded vector* weights\n",
    "f) we got the log count we did inverse function to counts then we did layer normalization.. above 2 steps ar enothing but softmax function\n",
    "g)and we got the o/p as probs...\n",
    "\n",
    "2) breakdown of probs.. probs [0] means when . is passed as input to the model what is the most likely of the next outcome out of 27 characters.....  we got the highest probabilty at 0.0676 which is the index 6 position which is e\n",
    "pron[1] gives as when e is passed what is the most likely out outome of all 27 charcters... and so on,,,,\n",
    "\n",
    "3) i will write whole blockl of code clearly in below step.\n",
    "\n",
    "4) if we tune weights W the probabilities coming out will definitely  gonna  change\n",
    "\n",
    "\"\"\""
   ],
   "id": "5bf779a406578416"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T16:47:00.474503Z",
     "start_time": "2025-07-09T16:47:00.470309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## neatly written organised together\n",
    "\n",
    "xs"
   ],
   "id": "8b74e7c8cc465688",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T16:47:04.609787Z",
     "start_time": "2025-07-09T16:47:04.604769Z"
    }
   },
   "cell_type": "code",
   "source": "ys",
   "id": "167c41bbc16ffe7b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T16:48:50.581763Z",
     "start_time": "2025-07-09T16:48:50.577822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Randomly initialize 27 neuron weights . each neuron recieve 27 inputs\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g)"
   ],
   "id": "450f5a3e7f3b930",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Forward pass of neural networks",
   "id": "1c00b3c06ce31f82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T17:17:17.550235Z",
     "start_time": "2025-07-09T17:17:17.547174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xs_encoded = F.one_hot(xs, num_classes=27).float()\n",
    "logits = xs_encoded @ W # gives the log-counts\n",
    "counts = logits.exp() # inverse of log = log then we get the counts\n",
    "probs = counts/counts.sum(1, keepdims = True) # probabilities for next charcter"
   ],
   "id": "3775d68823ff5774",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T17:16:42.268501Z",
     "start_time": "2025-07-09T17:16:42.260134Z"
    }
   },
   "cell_type": "code",
   "source": [
    " ## now we want to calculate the loss\n",
    "\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    x= xs[i].item()\n",
    "    y = ys[i].item()\n",
    "    print('--------')\n",
    "    print('--------')\n",
    "    print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "    print('input to the neural net:', x)\n",
    "    print('output probabilities from the neural net:', probs[i])\n",
    "    print('label (actual next character):', y)\n",
    "    p = probs[i, y]\n",
    "    print('probability assigned by the net to the the correct character:', p.item())\n",
    "    logp = torch.log(p)\n",
    "    print('log likelihood:', logp.item())\n",
    "    nll = -logp\n",
    "    print('negative log likelihood:', nll.item())\n",
    "    nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())\n"
   ],
   "id": "95797e22527d7ba5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "--------\n",
      "bigram example 1: .e (indexes 0,5)\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "label (actual next character): 5\n",
      "probability assigned by the net to the the correct character: 0.01228625513613224\n",
      "log likelihood: -4.399273872375488\n",
      "negative log likelihood: 4.399273872375488\n",
      "--------\n",
      "--------\n",
      "bigram example 2: em (indexes 5,13)\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.018050700426101685\n",
      "log likelihood: -4.014570713043213\n",
      "negative log likelihood: 4.014570713043213\n",
      "--------\n",
      "--------\n",
      "bigram example 3: mm (indexes 13,13)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.026691533625125885\n",
      "log likelihood: -3.623408794403076\n",
      "negative log likelihood: 3.623408794403076\n",
      "--------\n",
      "--------\n",
      "bigram example 4: ma (indexes 13,1)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the the correct character: 0.07367686182260513\n",
      "log likelihood: -2.6080665588378906\n",
      "negative log likelihood: 2.6080665588378906\n",
      "--------\n",
      "--------\n",
      "bigram example 5: a. (indexes 1,0)\n",
      "input to the neural net: 1\n",
      "output probabilities from the neural net: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the the correct character: 0.014977526850998402\n",
      "log likelihood: -4.201204299926758\n",
      "negative log likelihood: 4.201204299926758\n",
      "=========\n",
      "average negative log likelihood, i.e. loss = 3.7693049907684326\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\"\n",
    "# g = torch.Generator().manual_seed(2147483647)\n",
    "# W = torch.randn((27,27), generator=g)\n",
    "if keep on changing the weights the loss is going to change\n",
    "\n",
    "but we cannot keep on changing the w value to find the optimal weights there fore we need to do back propogation  weights in proper way\n",
    "\n",
    "with average negative likely hood\n",
    "\n",
    "\"\"\""
   ],
   "id": "630e1647f5a64483"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T17:34:12.574473Z",
     "start_time": "2025-07-09T17:34:12.567486Z"
    }
   },
   "cell_type": "code",
   "source": "probs",
   "id": "e45a069653dfed75",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
       "         0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
       "         0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459],\n",
       "        [0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
       "         0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
       "         0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472],\n",
       "        [0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
       "         0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
       "         0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
       "        [0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
       "         0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
       "         0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
       "        [0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
       "         0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
       "         0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    " (.) as input (e) must has high probability o/p     ie.,  -> (0) as i/p -> high probability o/p must be ->(5)\n",
    "                             -> (e) as input (m) must has high probability o/p     ie.,  -> (5) as i/p -> high probability o/p must be ->(13)\n",
    "                             -> (m) as input (m) must has high probability o/p     ie.,  -> (13) as i/p -> high probability o/p must be ->(13)\n",
    "                             -> (m) as input (a) must has high probability o/p     ie.,  -> (13) as i/p -> high probability o/p must be ->(1)\n",
    "                             -> (a) as input (.) must has high probability o/p     ie.,  -> (1) as i/p -> high probability o/p must be ->(0)\n",
    "\n",
    "\n",
    "                             so desired o/p variables are 5, 13,13,1,0\n",
    "for the input vectors\n",
    "\n",
    "lets see what our model has predicted the for our input vector\n",
    "\"\"\""
   ],
   "id": "768a1ddba8e8b71c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T17:44:22.993916Z",
     "start_time": "2025-07-09T17:44:22.988260Z"
    }
   },
   "cell_type": "code",
   "source": "probs[0,5],probs[1,13], probs[2,13], probs[3,13], probs [4,0]",
   "id": "7881cb8f12edd0e7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0123),\n",
       " tensor(0.0181),\n",
       " tensor(0.0267),\n",
       " tensor(0.0267),\n",
       " tensor(0.0150))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T17:47:13.447912Z",
     "start_time": "2025-07-09T17:47:13.443198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we can see none of the prediction probability is hih , and we are in classification model\n",
    "\n",
    "# like we are Classifying which one among the 5 output it closely match with\n",
    "\n",
    "probs[torch.arange(5), ys]"
   ],
   "id": "dce8d770f42881ea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0123, 0.0181, 0.0267, 0.0737, 0.0150])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T17:52:49.034677Z",
     "start_time": "2025-07-09T17:52:49.028394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Negative log likely hood estimation can be achieved by\n",
    "loss = -probs[torch.arange(5), ys].log().mean()\n",
    "loss\n"
   ],
   "id": "3d44d53d749d7cfe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7693)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T17:53:33.624046Z",
     "start_time": "2025-07-09T17:53:33.618443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#------- clean optimization step-------------------------------------\n",
    "\n",
    "#input\n",
    "xs"
   ],
   "id": "fb9ff9d99378e015",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T17:53:35.534899Z",
     "start_time": "2025-07-09T17:53:35.530519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#output\n",
    "ys"
   ],
   "id": "40b83be47265830a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T18:01:49.632010Z",
     "start_time": "2025-07-09T18:01:49.628731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# random weights\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "Wg = torch.randn((27,27), generator=g, requires_grad = True)"
   ],
   "id": "a005a9e2c98667f5",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T18:05:08.082009Z",
     "start_time": "2025-07-09T18:05:08.075859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Forward pass of NN\n",
    "xs_encoded = F.one_hot(xs,num_classes=27).float()\n",
    "logits = xs_encoded @ Wg\n",
    "counts = logits.exp()\n",
    "probs = counts/counts.sum(1, keepdims = True)\n",
    "loss = -probs[torch.arange(5), ys].log().mean()  ## Negative log likely hood estimation can be achieved by\n",
    "loss"
   ],
   "id": "f33ac0aa16988418",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6693, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T18:05:08.372563Z",
     "start_time": "2025-07-09T18:05:08.369093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Backward pass\n",
    "Wg.grad = None # set to zero the gradient\n",
    "loss.backward()"
   ],
   "id": "eeb43abc8a5ad9d4",
   "outputs": [],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T18:05:08.869709Z",
     "start_time": "2025-07-09T18:05:08.865709Z"
    }
   },
   "cell_type": "code",
   "source": "Wg.data += -0.1 * Wg.grad # this cell update the weights evertime",
   "id": "76a2bdc3f24e4b0f",
   "outputs": [],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T18:08:44.750547Z",
     "start_time": "2025-07-09T18:08:44.632085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## putting loop and we can change the  loss function so the random weights keeps on updating\n",
    "\n",
    "# --------- !!! OPTIMIZATION !!! yay, but this time actually --------------\n",
    "\n",
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ],
   "id": "e0bb3410c6440deb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T18:10:44.127375Z",
     "start_time": "2025-07-09T18:10:33.472691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# gradient descent\n",
    "for k in range(500):\n",
    "\n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() # negative log likely   hood\n",
    "  print(loss.item())\n",
    "\n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "\n",
    "  # update\n",
    "  W.data += -50 * W.grad"
   ],
   "id": "db42878142667651",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.480414390563965\n",
      "2.480414390563965\n",
      "2.480414390563965\n",
      "2.4804141521453857\n",
      "2.480414390563965\n",
      "2.4804141521453857\n",
      "2.480414390563965\n",
      "2.4804141521453857\n",
      "2.4804141521453857\n",
      "2.4804141521453857\n",
      "2.4804139137268066\n",
      "2.4804136753082275\n",
      "2.4804139137268066\n",
      "2.4804136753082275\n",
      "2.4804139137268066\n",
      "2.4804136753082275\n",
      "2.4804136753082275\n",
      "2.4804134368896484\n",
      "2.4804134368896484\n",
      "2.4804136753082275\n",
      "2.4804136753082275\n",
      "2.4804136753082275\n",
      "2.4804131984710693\n",
      "2.4804131984710693\n",
      "2.4804134368896484\n",
      "2.4804131984710693\n",
      "2.4804129600524902\n",
      "2.4804131984710693\n",
      "2.4804131984710693\n",
      "2.4804129600524902\n",
      "2.4804131984710693\n",
      "2.4804131984710693\n",
      "2.4804131984710693\n",
      "2.4804129600524902\n",
      "2.4804129600524902\n",
      "2.480412721633911\n",
      "2.480412483215332\n",
      "2.480412483215332\n",
      "2.480412721633911\n",
      "2.480412483215332\n",
      "2.480412483215332\n",
      "2.480412721633911\n",
      "2.480412483215332\n",
      "2.480412483215332\n",
      "2.480412483215332\n",
      "2.480412721633911\n",
      "2.480412483215332\n",
      "2.480412244796753\n",
      "2.480412483215332\n",
      "2.480412483215332\n",
      "2.480412244796753\n",
      "2.480412006378174\n",
      "2.480412006378174\n",
      "2.480412006378174\n",
      "2.4804117679595947\n",
      "2.480412006378174\n",
      "2.480412006378174\n",
      "2.480412006378174\n",
      "2.480412006378174\n",
      "2.4804117679595947\n",
      "2.4804117679595947\n",
      "2.4804117679595947\n",
      "2.4804115295410156\n",
      "2.4804117679595947\n",
      "2.4804117679595947\n",
      "2.4804115295410156\n",
      "2.4804115295410156\n",
      "2.4804117679595947\n",
      "2.4804112911224365\n",
      "2.4804112911224365\n",
      "2.4804112911224365\n",
      "2.4804115295410156\n",
      "2.4804112911224365\n",
      "2.4804112911224365\n",
      "2.4804110527038574\n",
      "2.4804110527038574\n",
      "2.4804112911224365\n",
      "2.4804110527038574\n",
      "2.4804110527038574\n",
      "2.4804110527038574\n",
      "2.4804112911224365\n",
      "2.4804112911224365\n",
      "2.4804110527038574\n",
      "2.4804108142852783\n",
      "2.4804108142852783\n",
      "2.4804110527038574\n",
      "2.4804110527038574\n",
      "2.4804110527038574\n",
      "2.4804110527038574\n",
      "2.4804108142852783\n",
      "2.4804110527038574\n",
      "2.480410575866699\n",
      "2.480410575866699\n",
      "2.480410575866699\n",
      "2.480410575866699\n",
      "2.480410099029541\n",
      "2.48041033744812\n",
      "2.48041033744812\n",
      "2.48041033744812\n",
      "2.48041033744812\n",
      "2.48041033744812\n",
      "2.480410099029541\n",
      "2.48041033744812\n",
      "2.48041033744812\n",
      "2.48041033744812\n",
      "2.480410099029541\n",
      "2.480410099029541\n",
      "2.480410099029541\n",
      "2.48041033744812\n",
      "2.480410099029541\n",
      "2.480410099029541\n",
      "2.480410099029541\n",
      "2.480409860610962\n",
      "2.480409860610962\n",
      "2.480410099029541\n",
      "2.480410099029541\n",
      "2.480410099029541\n",
      "2.480409622192383\n",
      "2.480409622192383\n",
      "2.4804093837738037\n",
      "2.4804093837738037\n",
      "2.480409622192383\n",
      "2.480409622192383\n",
      "2.480409622192383\n",
      "2.4804093837738037\n",
      "2.4804093837738037\n",
      "2.4804093837738037\n",
      "2.4804093837738037\n",
      "2.480409622192383\n",
      "2.4804093837738037\n",
      "2.4804093837738037\n",
      "2.480409622192383\n",
      "2.4804091453552246\n",
      "2.4804091453552246\n",
      "2.4804091453552246\n",
      "2.4804091453552246\n",
      "2.4804093837738037\n",
      "2.4804093837738037\n",
      "2.4804091453552246\n",
      "2.4804091453552246\n",
      "2.4804091453552246\n",
      "2.4804089069366455\n",
      "2.4804089069366455\n",
      "2.4804089069366455\n",
      "2.4804091453552246\n",
      "2.4804091453552246\n",
      "2.4804091453552246\n",
      "2.4804091453552246\n",
      "2.4804086685180664\n",
      "2.4804086685180664\n",
      "2.4804084300994873\n",
      "2.4804084300994873\n",
      "2.4804084300994873\n",
      "2.4804086685180664\n",
      "2.4804086685180664\n",
      "2.4804086685180664\n",
      "2.4804086685180664\n",
      "2.4804086685180664\n",
      "2.4804084300994873\n",
      "2.4804084300994873\n",
      "2.4804084300994873\n",
      "2.4804084300994873\n",
      "2.4804086685180664\n",
      "2.4804086685180664\n",
      "2.4804086685180664\n",
      "2.4804086685180664\n",
      "2.4804084300994873\n",
      "2.4804084300994873\n",
      "2.4804084300994873\n",
      "2.4804084300994873\n",
      "2.4804084300994873\n",
      "2.4804084300994873\n",
      "2.4804084300994873\n",
      "2.4804084300994873\n",
      "2.4804084300994873\n",
      "2.480408191680908\n",
      "2.4804084300994873\n",
      "2.480408191680908\n",
      "2.480408191680908\n",
      "2.480408191680908\n",
      "2.480408191680908\n",
      "2.48040771484375\n",
      "2.48040771484375\n",
      "2.480407953262329\n",
      "2.480407953262329\n",
      "2.480407953262329\n",
      "2.480407953262329\n",
      "2.480407953262329\n",
      "2.480407476425171\n",
      "2.480407476425171\n",
      "2.480407476425171\n",
      "2.480407476425171\n",
      "2.480407476425171\n",
      "2.480407476425171\n",
      "2.480407476425171\n",
      "2.48040771484375\n",
      "2.48040771484375\n",
      "2.48040771484375\n",
      "2.48040771484375\n",
      "2.480407476425171\n",
      "2.480407476425171\n",
      "2.480407476425171\n",
      "2.480407476425171\n",
      "2.480407238006592\n",
      "2.480407238006592\n",
      "2.480407238006592\n",
      "2.480407238006592\n",
      "2.480407238006592\n",
      "2.480407238006592\n",
      "2.480407476425171\n",
      "2.480407476425171\n",
      "2.480407476425171\n",
      "2.480407238006592\n",
      "2.480407238006592\n",
      "2.480407238006592\n",
      "2.480407238006592\n",
      "2.4804069995880127\n",
      "2.4804069995880127\n",
      "2.4804069995880127\n",
      "2.4804069995880127\n",
      "2.4804069995880127\n",
      "2.4804069995880127\n",
      "2.4804069995880127\n",
      "2.480407238006592\n",
      "2.480407238006592\n",
      "2.480407238006592\n",
      "2.4804067611694336\n",
      "2.4804067611694336\n",
      "2.4804067611694336\n",
      "2.4804067611694336\n",
      "2.4804067611694336\n",
      "2.4804065227508545\n",
      "2.4804065227508545\n",
      "2.4804067611694336\n",
      "2.4804065227508545\n",
      "2.4804065227508545\n",
      "2.4804065227508545\n",
      "2.4804065227508545\n",
      "2.4804065227508545\n",
      "2.4804067611694336\n",
      "2.4804067611694336\n",
      "2.4804067611694336\n",
      "2.4804067611694336\n",
      "2.4804067611694336\n",
      "2.4804062843322754\n",
      "2.4804062843322754\n",
      "2.4804062843322754\n",
      "2.4804062843322754\n",
      "2.4804062843322754\n",
      "2.4804062843322754\n",
      "2.4804062843322754\n",
      "2.4804062843322754\n",
      "2.4804060459136963\n",
      "2.4804062843322754\n",
      "2.4804060459136963\n",
      "2.4804060459136963\n",
      "2.4804060459136963\n",
      "2.4804060459136963\n",
      "2.4804062843322754\n",
      "2.4804062843322754\n",
      "2.4804062843322754\n",
      "2.4804062843322754\n",
      "2.4804060459136963\n",
      "2.4804060459136963\n",
      "2.4804060459136963\n",
      "2.4804060459136963\n",
      "2.4804060459136963\n",
      "2.4804060459136963\n",
      "2.4804060459136963\n",
      "2.4804060459136963\n",
      "2.4804060459136963\n",
      "2.480405807495117\n",
      "2.480405807495117\n",
      "2.480405807495117\n",
      "2.480405807495117\n",
      "2.480405807495117\n",
      "2.480405807495117\n",
      "2.480405807495117\n",
      "2.480405807495117\n",
      "2.4804060459136963\n",
      "2.4804060459136963\n",
      "2.4804060459136963\n",
      "2.4804060459136963\n",
      "2.4804060459136963\n",
      "2.480405569076538\n",
      "2.4804060459136963\n",
      "2.480405569076538\n",
      "2.480405569076538\n",
      "2.480405569076538\n",
      "2.480405569076538\n",
      "2.480405569076538\n",
      "2.480405569076538\n",
      "2.48040509223938\n",
      "2.48040509223938\n",
      "2.48040509223938\n",
      "2.48040509223938\n",
      "2.48040509223938\n",
      "2.48040509223938\n",
      "2.48040509223938\n",
      "2.48040509223938\n",
      "2.48040509223938\n",
      "2.48040509223938\n",
      "2.48040509223938\n",
      "2.48040509223938\n",
      "2.480405330657959\n",
      "2.480405330657959\n",
      "2.480405330657959\n",
      "2.480405330657959\n",
      "2.480405330657959\n",
      "2.480405330657959\n",
      "2.480405330657959\n",
      "2.480405330657959\n",
      "2.480405330657959\n",
      "2.480405330657959\n",
      "2.48040509223938\n",
      "2.48040509223938\n",
      "2.48040509223938\n",
      "2.48040509223938\n",
      "2.48040509223938\n",
      "2.48040509223938\n",
      "2.48040509223938\n",
      "2.48040509223938\n",
      "2.48040509223938\n",
      "2.4804046154022217\n",
      "2.48040509223938\n",
      "2.4804046154022217\n",
      "2.480404853820801\n",
      "2.4804046154022217\n",
      "2.4804046154022217\n",
      "2.4804046154022217\n",
      "2.4804046154022217\n",
      "2.4804046154022217\n",
      "2.4804046154022217\n",
      "2.4804046154022217\n",
      "2.480404853820801\n",
      "2.480404853820801\n",
      "2.480404853820801\n",
      "2.480404853820801\n",
      "2.480404853820801\n",
      "2.480404853820801\n",
      "2.480404853820801\n",
      "2.480404853820801\n",
      "2.480404853820801\n",
      "2.480404853820801\n",
      "2.4804043769836426\n",
      "2.480404853820801\n",
      "2.480404853820801\n",
      "2.4804043769836426\n",
      "2.4804043769836426\n",
      "2.4804043769836426\n",
      "2.4804043769836426\n",
      "2.4804043769836426\n",
      "2.4804043769836426\n",
      "2.4804043769836426\n",
      "2.4804043769836426\n",
      "2.4804041385650635\n",
      "2.4804041385650635\n",
      "2.4804041385650635\n",
      "2.4804041385650635\n",
      "2.4804041385650635\n",
      "2.4804041385650635\n",
      "2.4804043769836426\n",
      "2.4804041385650635\n",
      "2.4804041385650635\n",
      "2.4804039001464844\n",
      "2.4804039001464844\n",
      "2.4804041385650635\n",
      "2.4804041385650635\n",
      "2.4804039001464844\n",
      "2.4804041385650635\n",
      "2.4804039001464844\n",
      "2.4804039001464844\n",
      "2.4804041385650635\n",
      "2.4804041385650635\n",
      "2.4804041385650635\n",
      "2.4804041385650635\n",
      "2.4804041385650635\n",
      "2.4804041385650635\n",
      "2.4804041385650635\n",
      "2.4804039001464844\n",
      "2.4804039001464844\n",
      "2.4804039001464844\n",
      "2.4804039001464844\n",
      "2.4804039001464844\n",
      "2.4804039001464844\n",
      "2.4804039001464844\n",
      "2.4804039001464844\n",
      "2.4804039001464844\n",
      "2.4804039001464844\n",
      "2.4804039001464844\n",
      "2.4804039001464844\n",
      "2.4804036617279053\n",
      "2.4804039001464844\n",
      "2.4804036617279053\n",
      "2.4804036617279053\n",
      "2.4804036617279053\n",
      "2.4804036617279053\n",
      "2.4804036617279053\n",
      "2.4804036617279053\n",
      "2.4804036617279053\n",
      "2.4804036617279053\n",
      "2.4804036617279053\n",
      "2.4804036617279053\n",
      "2.4804036617279053\n",
      "2.4804036617279053\n",
      "2.4804036617279053\n",
      "2.4804036617279053\n",
      "2.4804036617279053\n",
      "2.4804036617279053\n",
      "2.4804036617279053\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.4804036617279053\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480403423309326\n",
      "2.480402946472168\n",
      "2.480402946472168\n",
      "2.480402946472168\n",
      "2.480402946472168\n",
      "2.480402946472168\n",
      "2.480402946472168\n",
      "2.480402946472168\n",
      "2.480402946472168\n",
      "2.480402946472168\n",
      "2.480402946472168\n",
      "2.480402946472168\n",
      "2.480402946472168\n",
      "2.480402946472168\n",
      "2.480402946472168\n",
      "2.480402946472168\n",
      "2.480402946472168\n",
      "2.480402946472168\n",
      "2.480402946472168\n",
      "2.480402946472168\n",
      "2.480402946472168\n",
      "2.480402946472168\n",
      "2.480402946472168\n",
      "2.480402708053589\n",
      "2.480402708053589\n",
      "2.480402946472168\n",
      "2.480402708053589\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n"
     ]
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T19:25:19.030979Z",
     "start_time": "2025-07-10T19:25:19.011629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logits = xenc @ W\n",
    "\n",
    "\"\"\"\n",
    "logits = xenc @ W\n",
    "Meaning of steps we are assumming that this is one hot encoding multiplied weights which is nothing but the forward feed neural network\n",
    "\n",
    "what happens here is\n",
    "\n",
    "a). if we consider one hot vector has only 1 at 5 th position(e character will have) i.e., [0,0,0,0,1,0,0] multiplied to weights matrix which is column matrix with [w1, w2, w3, w4,w5, w6,w7]\n",
    "the result will be [0,0,0,0,w5,0,0] so all the other weights literally became zero  logits will becomes only the 5th row of the weights\n",
    "\n",
    "\n",
    "b)itis also something happend at the matrix we generated before which gives us the matrix visulaization  and returning the count, so we can say the countmtrix and exp(logits) is something similar\n",
    "\n",
    "\n",
    "c) then we are normalizing and extracting the loss by -ve log likelyhood literally same as above\n",
    "\n",
    "\n",
    "d) so the array/Tensor W(weights) after the optimization = count matrix we generated at starting\n",
    "\n",
    "\n",
    "e) but for the count vector/matrix we added N+1 to smooth out the count matrix to ignore zero values-, what happens if we increase N+1 with N+100 > this smooth out the Prediction and make uniform distribution\n",
    "\n",
    "f) so in our case if we make weights to zero, i.e., w = 0\n",
    "\n",
    "    logits = xenc @ W # predict log-counts => logits = 0\n",
    "    counts = logits.exp() # counts, equivalent to N => counts = 1( because e^0 =1)\n",
    "    probs = counts / counts.sum(1, keepdims=True) # prob = 1\n",
    "\n",
    "    *** so if we initialize the weights to zero we are getting the probability nearly uniform, this process is nothing but the label smoothing also called as regularixzation\n",
    "\n",
    "    we can take entries of all weights and square them and take mean  and add to loss which acts as regularization with learning rate\n",
    "\"\"\""
   ],
   "id": "2d17cdcce063fe55",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMeaning of steps we are assumming that this is one hot encoding multiplied weights which is nothing but the forward feed neural network\\n\\nwhat happens here is \\n'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Regularization with learning rate",
   "id": "3f9b34ef0b9eb47e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T19:46:19.531029Z",
     "start_time": "2025-07-10T19:46:08.806838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# gradient descent\n",
    "for k in range(500):\n",
    "\n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean() #(0.01*(W**2).mean()) this is regularization with small learning rate   hood\n",
    "  print(loss.item())\n",
    "\n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "\n",
    "  # update\n",
    "  W.data += -50 * W.grad"
   ],
   "id": "13f45e24fa5469c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804024696350098\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804024696350098\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804022312164307\n",
      "2.4804019927978516\n",
      "2.4804022312164307\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804017543792725\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804019927978516\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804012775421143\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804012775421143\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804017543792725\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804017543792725\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.480401039123535\n",
      "2.4804012775421143\n",
      "2.480401039123535\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.480401039123535\n",
      "2.480401039123535\n",
      "2.4804012775421143\n",
      "2.4804012775421143\n",
      "2.480401039123535\n",
      "2.480401039123535\n",
      "2.480401039123535\n",
      "2.480401039123535\n",
      "2.480401039123535\n",
      "2.480401039123535\n",
      "2.480401039123535\n",
      "2.480401039123535\n",
      "2.480401039123535\n",
      "2.480401039123535\n",
      "2.480401039123535\n",
      "2.480401039123535\n",
      "2.480401039123535\n",
      "2.480401039123535\n",
      "2.480401039123535\n",
      "2.480401039123535\n",
      "2.480401039123535\n",
      "2.480400800704956\n",
      "2.480401039123535\n",
      "2.480401039123535\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480401039123535\n",
      "2.480401039123535\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400800704956\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400800704956\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400323867798\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400562286377\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400562286377\n",
      "2.480400562286377\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400562286377\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.4804000854492188\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.4804000854492188\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.4804000854492188\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.4804000854492188\n",
      "2.480400323867798\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.480400323867798\n",
      "2.480400323867798\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.480400323867798\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n",
      "2.4804000854492188\n"
     ]
    }
   ],
   "execution_count": 135
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "\n",
    "if we increase ( N+ 100)  it will make the prediction more smooth and more uniform distrubution similarly  its nothing but increaseing the Learning_rate of this Regularization (0.01* (W**2).mean())\n",
    "\n",
    "because as weights grow more the loss keep on increasing  then we are unable to\n",
    "\n",
    "\"\"\""
   ],
   "id": "f7b9f0fc231f7c5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T19:56:40.010537Z",
     "start_time": "2025-07-10T19:56:39.971523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(50):\n",
    "\n",
    "    out = []\n",
    "    ix = 0\n",
    "\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float() # input to the network: one-hot encoding\n",
    "        logits = xenc @ W # predict log-counts\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "        ix = torch.multinomial(probs, num_samples= 1, replacement = True, generator = g).item()\n",
    "        out.append(itos[ix])\n",
    "\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))\n"
   ],
   "id": "77c38ff762f201ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexze.\n",
      "momasurailezityha.\n",
      "konimittain.\n",
      "llayn.\n",
      "ka.\n",
      "da.\n",
      "staiyaubrtthrigotai.\n",
      "moliellavo.\n",
      "ke.\n",
      "teda.\n",
      "ka.\n",
      "emimmsade.\n",
      "enkaviyny.\n",
      "ftlspehinivenvtahlasu.\n",
      "dsor.\n",
      "br.\n",
      "jol.\n",
      "pen.\n",
      "aisan.\n",
      "ja.\n",
      "feniee.\n",
      "zem.\n",
      "deru.\n",
      "firit.\n",
      "gaikajahahbevare.\n",
      "kiysthelenaririenah.\n",
      "keen.\n",
      "x.\n",
      "al.\n",
      "kalmahavazeeromysos.\n",
      "laitenimieegariseriyen.\n",
      "k.\n",
      "illeleldole.\n",
      "meenisammigama.\n",
      "mmin.\n",
      "asharficalcalh.\n",
      "ciayn.\n",
      "asaza.\n",
      "elanely.\n",
      "chay.\n",
      "rana.\n",
      "ai.\n",
      "yviamisashougen.\n",
      "l.\n",
      "beyncaro.\n",
      "allan.\n",
      "annutetoradrilia.\n",
      "rddemkaha.\n",
      "cahfahevara.\n",
      "jala.\n"
     ]
    }
   ],
   "execution_count": 143
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## So this is the same model but we approached it differently with Neural network the o/p will really look same because of the input data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Topics Covered\n",
    "\n",
    "1. we introduce bi gram language model.\n",
    "2. we learned how we can train our model and how we can sample our model.\n",
    "3. how we can evaluate the quality of the model using -ve loge likely hood loss\n",
    "4. we trained our model in two completely different ways that actually give the same results from ame model.\n",
    "5. in first way we counted all the frequency/ counts of bi grams, in second way we used negative log likely hood loss as guide to optimize the frequency/counts to optimize the counts matrix so that loss is minimized in gradient based frame form\n",
    "6. We noticed both of them gave the same result\n",
    "7. gradient based framework is simiple, we use only single layer of neurons i.e., Xenc * Weights to calculate the logits , In future we goona increase the neural network layer , but the normalising and gradient based frame work remains same.\n",
    "8. so we complexify the neural networks all the way to transformers\n",
    "\n",
    "\"\"\""
   ],
   "id": "72d3d468a1e9f865"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
