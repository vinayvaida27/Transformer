{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "## MLP Training and Optimizing the Activation, Gradients and Batch Normalization",
   "id": "425be9d2aa3fba8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T15:58:03.155963Z",
     "start_time": "2025-08-26T15:57:58.848984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "id": "df771cc524a9b52d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T15:58:03.197867Z",
     "start_time": "2025-08-26T15:58:03.163583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:5]"
   ],
   "id": "a49e4ae610b9fd2f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T15:58:03.212439Z",
     "start_time": "2025-08-26T15:58:03.207870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {\n",
    "    s: i+1 for i,s in enumerate(chars)\n",
    "}\n",
    "stoi['.'] = 0\n",
    "itos = { i:s for s,i in stoi.items() }"
   ],
   "id": "6fa53208bd300d8a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T15:58:03.241584Z",
     "start_time": "2025-08-26T15:58:03.235988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size =len(itos)\n",
    "vocab_size"
   ],
   "id": "fed3dd7553c3c403",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T15:58:03.618255Z",
     "start_time": "2025-08-26T15:58:03.247414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):\n",
    "  X, Y = [], []\n",
    "\n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1]) #80% training size\n",
    "Xdev, Ydev = build_dataset(words[n1:n2]) #10% validation/dev\n",
    "Xte, Yte = build_dataset(words[n2:])#10% test size"
   ],
   "id": "7961d960a006bc18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T15:58:03.632511Z",
     "start_time": "2025-08-26T15:58:03.624703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
    "b1 = torch.randn(n_hidden,                        generator=g)\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g)\n",
    "b2 = torch.randn(vocab_size,                      generator=g)\n",
    "\n",
    "\n",
    "parameters = [C, W1, W2,b1, b2]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ],
   "id": "943d9be09c20ce5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11897\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T16:01:57.994050Z",
     "start_time": "2025-08-26T15:58:03.676638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hpreact = embcat @ W1+b1 #+ b1 # hidden layer pre-activation\n",
    "\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "\n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n"
   ],
   "id": "e7809c3c2b3d1b2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 27.8817\n",
      "  10000/ 200000: 2.8137\n",
      "  20000/ 200000: 2.5107\n",
      "  30000/ 200000: 2.8047\n",
      "  40000/ 200000: 2.0610\n",
      "  50000/ 200000: 2.6743\n",
      "  60000/ 200000: 2.3679\n",
      "  70000/ 200000: 2.0702\n",
      "  80000/ 200000: 2.2550\n",
      "  90000/ 200000: 2.2825\n",
      " 100000/ 200000: 2.0023\n",
      " 110000/ 200000: 2.3095\n",
      " 120000/ 200000: 1.8914\n",
      " 130000/ 200000: 2.4168\n",
      " 140000/ 200000: 2.2163\n",
      " 150000/ 200000: 2.1790\n",
      " 160000/ 200000: 2.0718\n",
      " 170000/ 200000: 1.7637\n",
      " 180000/ 200000: 2.0273\n",
      " 190000/ 200000: 1.8378\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T16:01:58.499933Z",
     "start_time": "2025-08-26T16:01:58.416870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  h = torch.tanh(embcat @ W1 + b1) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ],
   "id": "4c05f70dbff553f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.1256096363067627\n",
      "val 2.1673810482025146\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T16:01:58.514722Z",
     "start_time": "2025-08-26T16:01:58.506805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## if you notice at  this posittion (0/ 200000: 22.0248) the loss is 22.0248 which is  very high\n",
    "\n",
    "#in practical if we assume the uniform distribution of the model like what character comes next to model is A_Z has eqeual probability\n",
    "\n",
    "# so we know loss = -ve (log(likelyhood)\n",
    "\n",
    "-torch.tensor(1/27).log() #3.29\n",
    "\n",
    "#the starting loss must be 3.29 which we would expect but the initial loss is too high\n",
    "\n",
    "## to make the initial loss minimum we need to decrease the logits to decrease logits we need to assign the initial weigth os neural network to low\n"
   ],
   "id": "53c900ce8d9662ec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2958)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Why the Initial Loss is Higher Than Expected\n",
    "\n",
    "#### 1. What we expect\n",
    "- **Task:** Predict the next character among 27 possible classes (A–Z + special token).\n",
    "- If predictions start as **uniform random**:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\log \\left(\\frac{1}{27}\\right) \\approx 3.29\n",
    "$$\n",
    "\n",
    "So the **expected starting loss** is ~3.29.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. What we observed\n",
    "- Actual initial loss: **~22.0**\n",
    "- Much higher than the random/uniform baseline.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Why this happens\n",
    "- Loss uses **softmax(logits)** to convert raw outputs → probabilities.\n",
    "- If **initial weights are too large**, the logits are also large.\n",
    "- Softmax then produces **very peaked probabilities** (e.g., 0.999 for one class, ~1e-10 for others).\n",
    "- If the correct class happens to have probability ~1e-10:\n",
    "\n",
    "$$\n",
    "-\\log(1e^{-10}) \\approx 23\n",
    "$$\n",
    "\n",
    "This explains why the observed loss is much higher.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Role of initialization\n",
    "- If weights are too big → overconfident wrong guesses → high loss.\n",
    "- If weights are small → logits ≈ 0 → softmax ≈ uniform → loss ≈ 3.29.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. How to fix\n",
    "- **Scale down initial weights** so that logits start small.\n",
    "- This makes predictions close to uniform.\n",
    "- Ensures initial loss is around the expected **3.29**, not 22.\n",
    "\n",
    "---\n",
    "\n",
    " **Summary in plain words**\n",
    "- The loss is high (22) because the model starts out *very sure but wrong*.\n",
    "- Ideally, we want it to start *unsure* (uniform guessing).\n",
    "- Small initial weights → small logits → uniform predictions → stable starting loss (~3.29).\n"
   ],
   "id": "ac602fc37d725a5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Updating the Weigths to make the initial loss minimized",
   "id": "218c278cb2e25e63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T16:01:58.581490Z",
     "start_time": "2025-08-26T16:01:58.573974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
    "b1 = torch.randn(n_hidden,                        generator=g)\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g)\n",
    "\n",
    "\n",
    "parameters = [C, W1, W2,b1, b2]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ],
   "id": "bc71c42ef177e1d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11897\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-26T16:01:58.614089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hpreact = embcat @ W1+b1 #+ b1 # hidden layer pre-activation\n",
    "\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "\n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n"
   ],
   "id": "1374c67311edf21d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.8073\n",
      "  10000/ 200000: 2.1480\n",
      "  20000/ 200000: 2.4800\n",
      "  30000/ 200000: 2.5773\n",
      "  40000/ 200000: 2.0152\n",
      "  50000/ 200000: 2.3846\n",
      "  60000/ 200000: 2.3748\n",
      "  70000/ 200000: 2.0512\n",
      "  80000/ 200000: 2.2471\n",
      "  90000/ 200000: 2.0470\n",
      " 100000/ 200000: 1.8059\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  h = torch.tanh(embcat @ W1 + b1) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ],
   "id": "1b5f9d2347098785",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# still if we notice at zeroth iteration the loss is 3.8959 which is little higher than we expected tensor(3.2958)\n",
    "\n",
    "# the weights initialization is good, but we need to look at the activation function wht it is doing there is it squasihng the val;ues too high or too low\n",
    "\n",
    "# this line\n",
    "#  hpreact = embcat @ W1+b1 #+ b1 # hidden layer pre-activation\n",
    "\n",
    "#  h = torch.tanh(hpreact)\n",
    "#  we need to see how the tanh is squashing\n"
   ],
   "id": "9a862b87733f06a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#  Understanding Loss at Zeroth Iteration & the Effect of `tanh`\n",
    "\n",
    "At **iteration 0**, we observe:\n",
    "\n",
    "- **Expected loss:** ~3.29\n",
    "- **Observed loss:** ~3.89\n",
    "\n",
    "This suggests that something in the **activation function** (`tanh`) may be influencing how values are being squashed.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Hidden Layer Computation\n",
    "\n",
    "The hidden layer pre-activation is:\n",
    "\n",
    "$$\n",
    "h_{\\text{preact}} = \\text{embcat} \\cdot W_1 + b_1\n",
    "$$\n",
    "\n",
    "After applying the activation:\n",
    "\n",
    "$$\n",
    "h = \\tanh(h_{\\text{preact}})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Behavior of the `tanh` Activation\n",
    "\n",
    "The **tanh function** is defined as:\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "- Range: $[-1, 1]$\n",
    "- Squashes large positive values → close to **+1**\n",
    "- Squashes large negative values → close to **-1**\n",
    "- Small values around 0 are approximately **linear**:\n",
    "\n",
    "$$\n",
    "\\tanh(x) \\approx x \\quad \\text{when } |x| \\ll 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Why This Matters for Loss\n",
    "\n",
    "- If $|h_{\\text{preact}}|$ is **large**, `tanh` saturates → gradients vanish.\n",
    "- If $|h_{\\text{preact}}|$ is **too small**, outputs are near 0 → weak signal.\n",
    "- At initialization, if weights push $h_{\\text{preact}}$ into saturation, the **model starts with high loss**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Checking Initialization vs. Activation\n",
    "\n",
    "1. **Distribution of pre-activations:**\n",
    "\n",
    "$$\n",
    "h_{\\text{preact}} \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "   - If $\\sigma$ too high → saturation.\n",
    "   - If $\\sigma$ too low → all outputs $\\approx 0$.\n",
    "\n",
    "2. **After `tanh`:**\n",
    "\n",
    "$$\n",
    "h = \\tanh(h_{\\text{preact}})\n",
    "$$\n",
    "\n",
    "   - Mean shifts toward 0.\n",
    "   - Variance reduced compared to input variance.\n",
    "\n",
    "This variance shrinkage can cause the network to initially under-express features, leading to **slightly higher-than-expected loss**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Debugging Strategy\n",
    "\n",
    "✅ Plot histogram of $h_{\\text{preact}}$ and $\\tanh(h_{\\text{preact}})$\n",
    "✅ Compare variance before/after activation\n",
    "✅ Try different activations (`ReLU`, `LeakyReLU`) for comparison\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "The **tanh activation squashes values**, which may cause the network to **start with weaker signals**, explaining why the observed loss (3.89) is higher than the expected baseline (3.29).\n",
    "Understanding how activations interact with initialization is crucial to ensure stable training.\n"
   ],
   "id": "faae222ac3eb27e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
    "b1 = torch.randn(n_hidden,                        generator=g)\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g)\n",
    "\n",
    "\n",
    "parameters = [C, W1, W2,b1, b2]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hpreact = embcat @ W1+b1 #+ b1 # hidden layer pre-activation\n",
    "\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "\n",
    "\n",
    "\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "\n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "  break"
   ],
   "id": "fb3571528e9c734c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "h_{\\text{preact}} = \\text{embcat} \\cdot W_1 + b_1\n",
    "$$"
   ],
   "id": "67eca01b112f6f2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "#hpreact = embcat @ W1+b1\n",
    "plt.hist(hpreact.view(-1).tolist(),50);"
   ],
   "id": "8bef343e851775c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:56:38.963933Z",
     "start_time": "2025-08-25T14:56:38.958951Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "h = \\tanh(h_{\\text{preact}})\n",
    "$$\n"
   ],
   "id": "6e38a614eb657309"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.hist(h.view(-1).tolist(),50);",
   "id": "1ff7ac27c9ba3599",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:53:53.516332Z",
     "start_time": "2025-08-25T14:53:53.509631Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Effect of `tanh` Squashing on Initialization\n",
    "\n",
    "When using the `tanh` activation, we observe that most hidden pre-activation values ($h_{\\text{preact}}$) are squashed toward the extreme ends of **-1 and 1**. This saturation reduces gradient flow, making optimization harder. The issue arises when the pre-activation:\n",
    "\n",
    "$$\n",
    "h_{\\text{preact}} = \\text{embcat} \\cdot W_1 + b_1\n",
    "$$\n",
    "\n",
    "is too large in magnitude, causing outputs to cluster at the boundaries. To optimize training more effectively, we must reduce these extreme values at initialization. By lowering weight scales, we keep $h_{\\text{preact}}$ closer to zero, ensuring smoother gradients and better learning dynamics in the early stages.\n"
   ],
   "id": "5848668e24154bed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:18:22.429664500Z",
     "start_time": "2025-08-25T13:47:55.300237Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "If the values of $h_{\\text{preact}}$ are too large, even after scaling, the extremes still squash toward **-1 and 1** under `tanh`.\n",
    "\n",
    "To address this, we must directly reduce the magnitude of:\n",
    "\n",
    "$$\n",
    "h_{\\text{preact}} = \\text{embcat} \\cdot W_1 + b_1\n",
    "$$\n",
    "\n",
    "This can be achieved by **scaling down $W_1$ and $b_1$ during initialization**.\n",
    "- Smaller weights → logits closer to zero\n",
    "- Outputs avoid saturation\n",
    "- Gradients remain stronger\n",
    "\n",
    "Thus, careful initialization ensures smoother optimization and prevents the network from starting in a saturated, high-loss state.\n",
    "\n"
   ],
   "id": "d009eaf7d9b49e47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#This plot shows us how many dead neurons are present\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(h.abs()>0.99, cmap='gray', interpolation = 'nearest')"
   ],
   "id": "99ae04e8dba640e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The heatmap `imshow(h.abs()>0.99)` marks saturated units in white. A dense, speckled field of white pixels means many neurons have $|h|>0.99$, yielding near-zero gradients. These “stuck” units barely update during backpropagation, slowing learning and increasing initial loss beyond the uniform baseline.\n",
    "\n",
    "### Root Cause of Saturation\n",
    "\n",
    "The saturation arises from the pre-activation values being too large in magnitude. Specifically:\n",
    "\n",
    "$$\n",
    "h_{\\text{preact}} = \\text{embcat} \\cdot W_1 + b_1\n",
    "$$\n",
    "\n",
    "When $W_1$ and $b_1$ are initialized with high variance, the distribution of $h_{\\text{preact}}$ spreads far from zero. Since the `tanh` activation compresses large positive or negative values toward **+1** or **−1**, most outputs end up saturated. In this state, the gradient $\\tanh'(x) = 1 - \\tanh^2(x)$ becomes nearly zero, causing vanishing gradients and preventing neurons from updating effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### How to Avoid This\n",
    "\n",
    "To avoid saturation, we need to control the scale of pre-activations:\n",
    "\n",
    "- **Reduce initialization variance:** Use Xavier/Glorot or Kaiming initialization to keep values closer to zero.\n",
    "- **Smaller weights and biases:** Directly scale down $W_1$ and $b_1$ at initialization.\n",
    "- **Normalization layers:** Apply BatchNorm or LayerNorm to re-center and rescale activations.\n",
    "- **Alternative activations:** Consider ReLU or LeakyReLU, which do not saturate symmetrically at both ends.\n",
    "- **Data preprocessing:** Normalize input embeddings so they don’t amplify weights excessively.\n",
    "\n",
    "By ensuring activations stay in the near-linear region of `tanh`, gradients remain strong, neurons stay “alive,” and the model can learn effectively.\n"
   ],
   "id": "4829413e886b557b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## By changing the weigth of w1 and b1, we can decrease the logits i.e., h may be at that point we can reduce dead neurons which you can see in the above image",
   "id": "ed54d0b7dfffb367"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#\n",
    "\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) *0.2\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
    "\n",
    "\n",
    "parameters = [C, W1, W2,b1, b2]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hpreact = embcat @ W1+b1 #+ b1 # hidden layer pre-activation\n",
    "\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "\n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "  break"
   ],
   "id": "c01f4e6f86d5a7aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.hist(h.view(-1).tolist(),50);",
   "id": "1aaac28aac4b92ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.hist(hpreact.view(-1).tolist(),50);",
   "id": "3578b7d1684584f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(h.abs()>0.99, cmap='gray', interpolation = 'nearest')"
   ],
   "id": "d973dadb811b4556",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#the above plot look great so we will run the dull optimzation\n",
    "\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) *0.2\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
    "\n",
    "\n",
    "parameters = [C, W1, W2,b1, b2]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hpreact = embcat @ W1+b1 #+ b1 # hidden layer pre-activation\n",
    "\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "\n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())"
   ],
   "id": "26ab1d061f7bb95d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  h = torch.tanh(embcat @ W1 + b1) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ],
   "id": "3cee6ae146f98112",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T15:22:20.110110Z",
     "start_time": "2025-08-25T15:22:20.103267Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Effect of Initialization and Activation Tuning on Training\n",
    "\n",
    "- **Random initialization:**\n",
    "  - Train loss: **2.1256**\n",
    "  - Validation loss: **2.1674**\n",
    "\n",
    "- **Adjusted initial weights (smaller scale):**\n",
    "  - Lower starting loss observed\n",
    "  - Train loss: **2.0681**\n",
    "  - Validation loss: **2.1301**\n",
    "\n",
    "- **Optimized `tanh` layer (reducing saturation):**\n",
    "  - Further improvement in convergence\n",
    "  - Train loss: **2.0356**\n",
    "  - Validation loss: **2.1027**\n",
    "\n",
    "---\n",
    "\n",
    "### Observation\n",
    "\n",
    "Each refinement (better initialization and reducing `tanh` saturation) lowered training and validation loss. However, when applying more **aggressive training**, overall accuracy unexpectedly went **down**. This suggests that while initialization and activation tuning improve stability and reduce loss, careful balance in training dynamics (learning rate, regularization, and overfitting control) is crucial to sustain accuracy improvements.\n"
   ],
   "id": "8b3e606810a22223"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Example\n",
    "\n",
    "x= torch.randn(1000,10)\n",
    "W =torch.randn(10,200)\n",
    "y = x @ W\n",
    "print(x.mean(), x.std())\n",
    "print(y.mean(), y.std())\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(121)\n",
    "plt.hist(x.view(-1).tolist(), 50, density=True);\n",
    "plt.subplot(122)\n",
    "plt.hist(y.view(-1).tolist(), 50, density=True);"
   ],
   "id": "8ca3558797002404",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T15:49:32.951943Z",
     "start_time": "2025-08-25T15:49:32.944080Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Why the Variances Differ\n",
    "\n",
    "- **Left plot (x):** Input sampled from a standard normal distribution with mean ~0 and variance ≈ 1.\n",
    "- **Right plot (y):** After multiplying by weights, variance grows larger because each output sums contributions from many inputs. The variance of outputs scales with the **fan_in** (number of input connections).\n",
    "\n",
    "---\n",
    "\n",
    "### Fix with Kaiming Initialization\n",
    "\n",
    "To keep variance stable, we rescale weights:\n",
    "\n",
    "$$\n",
    "\\sigma_W = \\frac{\\text{gain}}{\\sqrt{\\text{fan\\_in}}}\n",
    "$$\n",
    "\n",
    "- For ReLU: gain = $\\sqrt{2}$\n",
    "- For tanh: gain ≈ $5/3$\n",
    "- For linear: gain = 1\n",
    "\n",
    "This ensures output variance remains close to 1, avoiding exploding or vanishing activations.\n"
   ],
   "id": "7f0faaec8b57f536"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Example\n",
    "\n",
    "x= torch.randn(1000,10)\n",
    "W =torch.randn(10,200) / 10 ** 0.5 # square root of fan in\n",
    "y = x @ W\n",
    "print(x.mean(), x.std())\n",
    "print(y.mean(), y.std())\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(121)\n",
    "plt.hist(x.view(-1).tolist(), 50, density=True);\n",
    "plt.subplot(122)\n",
    "plt.hist(y.view(-1).tolist(), 50, density=True);"
   ],
   "id": "26818e1074c0443f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T15:48:49.982085Z",
     "start_time": "2025-08-25T15:48:49.976893Z"
    }
   },
   "cell_type": "markdown",
   "source": "# Kaiming (He) Initialization Paper",
   "id": "f7658f5e84521771"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T15:22:20.250827Z",
     "start_time": "2025-08-25T15:22:20.246355Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Understanding Kaiming (He) Initialization – Beginner Friendly\n",
    "\n",
    "When we build a neural network, we need to **set the initial weights** before training begins. If we pick numbers randomly (like multiplying by 0.01 or 0.001), training may still work, but it often becomes unstable when the network is deep. The problem is that activations (and gradients) can either **explode** (grow very large) or **vanish** (shrink to almost zero) as they pass through many layers. Both make learning very hard.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Do Activations Explode or Vanish?\n",
    "\n",
    "Let’s say each neuron takes multiple inputs, multiplies them by weights, and sums them up:\n",
    "\n",
    "$$\n",
    "h_{\\text{preact}} = x \\cdot W\n",
    "$$\n",
    "\n",
    "If the weights are too large, the outputs blow up. If they’re too small, the outputs collapse near zero. Ideally, we want the **spread** (variance or standard deviation) of activations to remain roughly the same across layers. This way, the signal flows smoothly forward, and gradients flow backward without dying out.\n",
    "\n",
    "---\n",
    "\n",
    "### The Idea of Kaiming Initialization\n",
    "\n",
    "Kaiming (or He) Initialization gives us a **formula** for choosing the right scale for the weights so that the variance stays stable:\n",
    "\n",
    "$$\n",
    "W \\sim \\mathcal{N}\\left(0, \\frac{2}{\\text{fan\\_in}}\\right)\n",
    "$$\n",
    "\n",
    "- **fan_in** = number of inputs to the neuron (e.g., if a layer takes 100 inputs, fan_in = 100).\n",
    "- The **2** in the numerator comes from how ReLU activations drop half of the values to zero.\n",
    "- For other activations like `tanh`, we use a different “gain” factor, but the principle is the same:\n",
    "  keep the outputs well-behaved by dividing by $\\sqrt{\\text{fan\\_in}}$.\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Helps\n",
    "\n",
    "Instead of guessing “magic numbers” like 0.01, Kaiming initialization automatically **scales weights based on layer size and activation type**. This ensures:\n",
    "- Signals don’t shrink or explode as they pass through layers.\n",
    "- Gradients flow better during backpropagation.\n",
    "- Large, deep networks train more reliably.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Takeaway for Beginners:**\n",
    "Kaiming Initialization is like a recipe that tells us how big or small the starting weights should be, based on the number of inputs and the activation function. This avoids the trial-and-error of picking constants and makes deep networks much easier to train.\n"
   ],
   "id": "abc23e9cb33a3acc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#the above plot look great so we will run the dull optimzation\n",
    "\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
    "\n",
    "\n",
    "parameters = [C, W1, W2,b1, b2]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hpreact = embcat @ W1+b1 #+ b1 # hidden layer pre-activation\n",
    "\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "\n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())"
   ],
   "id": "5db99799825e19c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  h = torch.tanh(embcat @ W1 + b1) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ],
   "id": "618d745c621d70f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Initialization Then vs. Now\n",
    "\n",
    "Around 7 years ago, weight initialization was very **fragile**. Small mistakes in setting gains, scaling, or activation choices could completely break training, especially in deep networks. Researchers had to carefully track histograms and tune values by hand to avoid exploding or vanishing activations.\n",
    "\n",
    "---\n",
    "\n",
    "### Modern Improvements\n",
    "\n",
    "Today, training is much more stable because of:\n",
    "- **Residual connections (ResNets):** make gradient flow easier.\n",
    "- **Normalization layers:** BatchNorm, LayerNorm, GroupNorm stabilize activations.\n",
    "- **Better optimizers:** Adam, RMSProp handle noisy gradients far better than plain SGD.\n",
    "\n",
    "---\n",
    "\n",
    "### Practical Rule\n",
    "\n",
    "In practice, we usually just **normalize weights by the square root of the fan-in** (Kaiming/Xavier initialization). Frameworks like PyTorch implement this automatically, so we don’t need to manually fine-tune “magic numbers.”\n"
   ],
   "id": "d427a286f6a20b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Using batch Normalization paper from 2015",
   "id": "2f2bcd49a1b5812"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T16:56:52.169530Z",
     "start_time": "2025-08-25T16:56:52.159550Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Batch Normalization\n",
    "\n",
    "\n",
    "#### 1. The Problem\n",
    "When training deep neural networks, activations (the values flowing between layers) can become **too large** or **too small**:\n",
    "- If too small → neurons stop learning (tanh becomes flat near 0).\n",
    "- If too large → neurons saturate (tanh squashes to ±1, gradients vanish).\n",
    "\n",
    "We want activations to stay **well-behaved**: roughly mean 0 and variance 1.\n",
    "Before 2015, people had to carefully tune initialization and gains for every layer — this was fragile and hard to scale.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. The Big Idea (BatchNorm)\n",
    "Batch Normalization (BN) says: instead of *hoping* activations stay nice, we can **normalize them directly** during training.\n",
    "\n",
    "For each batch of activations $h_{\\text{preact}}$ (before applying activation functions):\n",
    "\n",
    "1. Compute the **mean**:\n",
    "   $$\n",
    "   \\mu = \\frac{1}{m}\\sum_{i=1}^m h_i\n",
    "   $$\n",
    "\n",
    "2. Compute the **standard deviation**:\n",
    "   $$\n",
    "   \\sigma = \\sqrt{\\frac{1}{m}\\sum_{i=1}^m (h_i - \\mu)^2}\n",
    "   $$\n",
    "\n",
    "3. Standardize activations:\n",
    "   $$\n",
    "   \\hat{h} = \\frac{h - \\mu}{\\sigma + \\epsilon}\n",
    "   $$\n",
    "\n",
    "   (ε is a tiny number to avoid dividing by zero).\n",
    "\n",
    "Now, every batch of activations has **mean 0** and **variance 1**. This keeps values stable for learning.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Adding Flexibility (Scale and Shift)\n",
    "If we always forced mean 0 and variance 1, the network might lose flexibility.\n",
    "So BN introduces two trainable parameters for each neuron:\n",
    "- **γ (gain)** → rescales activations\n",
    "- **β (bias/shift)** → shifts activations\n",
    "\n",
    "Final output:\n",
    "$$\n",
    "y = \\gamma \\hat{h} + \\beta\n",
    "$$\n",
    "\n",
    "At the start: γ = 1, β = 0 → pure normalization.\n",
    "During training, backpropagation learns the best γ and β values for each neuron.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Why It Works\n",
    "- Stabilizes activations → avoids exploding/vanishing values.\n",
    "- Makes deep networks easier to train.\n",
    "- Acts as a **regularizer**: because the mean/std come from the whole batch, each example’s output depends slightly on the others, adding noise and preventing overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Training vs. Inference\n",
    "- **During training**: BN computes mean/std from the current batch.\n",
    "- **During inference (deployment)**: BN uses a **running average** of mean/std collected during training. This way, single inputs can be passed without depending on a batch.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Practical Notes\n",
    "- Layers before BN don’t need their own bias (BN will subtract it out).\n",
    "- BN is often added **after every linear or convolution layer**.\n",
    "- While newer methods (LayerNorm, GroupNorm, etc.) exist, BN is still widely used because it works reliably.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Summary in plain words:**\n",
    "Batch Normalization is like giving each layer of a neural network a \"reset button\" that keeps its outputs well-scaled (not too big, not too small). It normalizes activations batch by batch, then lets the network learn how much to stretch (γ) or shift (β) them. This makes training deep networks much more stable and faster.\n"
   ],
   "id": "37a5628b758fecc9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T12:55:29.968955Z",
     "start_time": "2025-08-26T12:50:18.984380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#the above plot look great so we will run the dull optimzation\n",
    "\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
    "\n",
    "bngain = torch.ones((1,n_hidden))\n",
    "bnbias = torch.zeros((1,n_hidden))\n",
    "#-----------------------------------------------------------------------\n",
    "#buffers\n",
    "bnmean_running = torch.zeros((1,n_hidden))\n",
    "bnstd_running = torch.ones((1,n_hidden))\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "parameters = [C, W1, W2, b2,bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "#-----------------------------------------------------------------------\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  #-----------------------------------------------------------------------\n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n",
    "  #-----------------------------------------------------------------------\n",
    "  #Batch Normalization\n",
    "  #Calculating the batch mean and std devation for training , and we want to pass this average to training to make prediction\n",
    "\n",
    "  bnmeani = hpreact.mean(0, keepdim =True)\n",
    "  bnstdi = hpreact.std(0,keepdim = True)\n",
    "\n",
    "  # we want to make the hp react not to be too small or too large at intialization making it gausian\n",
    "  #hpreact = (hpreact - hpreact.mean(0, keepdim =True))/(hpreact.std(0, keepdim = True))\n",
    "  # we  want hp react to Gaussian  only at initialization, but not at entire training\n",
    "  hpreact = bngain * (hpreact - bnmeani)/(bnstdi) + bnbias\n",
    "\n",
    "  with torch.no_grad():\n",
    "      bnmean_running = 0.999* bnmean_running + 0.001*bnmeani\n",
    "      bnstd_running = 0.999* bnstd_running + 0.001*bnstdi\n",
    "  #-----------------------------------------------------------------------\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "  #-----------------------------------------------------------------------\n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  #-----------------------------------------------------------------------\n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())"
   ],
   "id": "838834f30547f322",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12097\n",
      "      0/ 200000: 3.3147\n",
      "  10000/ 200000: 2.1984\n",
      "  20000/ 200000: 2.3375\n",
      "  30000/ 200000: 2.4359\n",
      "  40000/ 200000: 2.0119\n",
      "  50000/ 200000: 2.2595\n",
      "  60000/ 200000: 2.4775\n",
      "  70000/ 200000: 2.1020\n",
      "  80000/ 200000: 2.2788\n",
      "  90000/ 200000: 2.1862\n",
      " 100000/ 200000: 1.9474\n",
      " 110000/ 200000: 2.3010\n",
      " 120000/ 200000: 1.9837\n",
      " 130000/ 200000: 2.4523\n",
      " 140000/ 200000: 2.3839\n",
      " 150000/ 200000: 2.1987\n",
      " 160000/ 200000: 1.9733\n",
      " 170000/ 200000: 1.8668\n",
      " 180000/ 200000: 1.9973\n",
      " 190000/ 200000: 1.8347\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T12:55:35.762822Z",
     "start_time": "2025-08-26T12:55:35.640166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean_running)/bnstd_running + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ],
   "id": "3804f33c97a1bfa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.066636800765991\n",
      "val 2.105013132095337\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###  Summary\n",
    "\n",
    "Batch Normalization (BN) is a layer that helps keep the **activations** (the values flowing through a neural network) well-scaled during training. BN is used to control the stats of activation.\n",
    "\n",
    "we use this after the layers which has multiplication llike linear layer or convolution layer\n",
    "\n",
    "Normally, activations can become too large or too small, which slows down or even breaks learning. BN fixes this by taking each batch of activations, calculating their **mean** and **standard deviation**, and then re-scaling them so they have mean 0 and variance 1. After this normalization, the layer also applies two trainable parameters: a **gain** (γ) to stretch the values and a **bias** (β) to shift them, so the network can still learn the best scale and offset.\n",
    "\n",
    "In addition, BN keeps track of a **running mean** and **running standard deviation** across training. These running statistics are not learned through gradients but updated over time like a moving average. At test time (inference), the model uses these stored averages instead of recomputing from each batch. This makes it possible to give the model **one input at a time** and still get correct results. In short: BN stabilizes training, speeds up learning, and makes very deep networks easier to train.\n"
   ],
   "id": "713060df0d94cb4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Effect of `Batch Normalization` layer on Training\n",
    "\n",
    "- **Random initialization:**\n",
    "  - Train loss: **2.1256**\n",
    "  - Validation loss: **2.1674**\n",
    "\n",
    "- **Adjusted initial weights (smaller scale):**\n",
    "  - Lower starting loss observed\n",
    "  - Train loss: **2.0681**\n",
    "  - Validation loss: **2.1301**\n",
    "\n",
    "- **Optimized `tanh` layer (reducing saturation):**\n",
    "  - Further improvement in convergence\n",
    "  - Train loss: **2.0356**\n",
    "  - Validation loss: **2.1027**\n",
    "\n",
    "- **Using `Batch Normalization` layer to make the hp react gausian distribution during training and make the neural network move after intitalization:**\n",
    "  - Train loss: **2.066**\n",
    "  - Validation loss: **2.1084** as we are in single layer NN it does not increase the accurary. When we have multiple hidden layers the loss is going to decrease"
   ],
   "id": "89f3e25c094eeff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T13:01:51.451642Z",
     "start_time": "2025-08-26T13:01:51.442953Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### ResNet50 and Batch Normalization\n",
    "\n",
    "**1. What ResNet50 looks like**\n",
    "ResNet50 is a very popular neural network used for image classification. At a high level, you feed in an image, and after passing through many layers, the network predicts what’s inside the image (like a cat or a dog). Instead of being one giant layer, it is made of *blocks* that repeat many times. Each block typically contains:\n",
    "- A **convolutional layer** (like a linear layer but applied to small overlapping patches of the image, not the whole thing at once).\n",
    "- A **batch normalization (BN) layer**, which normalizes the values so they don’t explode or vanish.\n",
    "- A **nonlinearity** such as ReLU, which helps the network capture complex patterns.\n",
    "\n",
    "This combination of **[weight layer → normalization → nonlinearity]** is repeated many times, making up the backbone of ResNet. ResNet also adds *residual connections* (skip links) so gradients can flow more easily, which makes training very deep networks practical.\n",
    "\n",
    "---\n",
    "\n",
    "**2. How PyTorch builds it**\n",
    "In PyTorch, a ResNet block is defined in code as a `Bottleneck` class. Inside it:\n",
    "- **Conv layers** are defined using `nn.Conv2d`. Notice they set `bias=False`. Why? Because BN subtracts the mean and adds its own bias later, so having a bias before BN is redundant.\n",
    "- **BatchNorm layers** are created with `nn.BatchNorm2d` for images or `nn.BatchNorm1d` for vectors. BN has:\n",
    "  - Trainable parameters: **gain (γ)** and **bias (β)**.\n",
    "  - Buffers: **running mean** and **running variance**, which track statistics over time (not learned by gradients).\n",
    "  - Hyperparameters:\n",
    "    - `eps`: a small number to avoid dividing by zero.\n",
    "    - `momentum`: controls how quickly the running averages update (smaller momentum works better if your batch size is small).\n",
    "    - `affine=True`: allows BN to learn γ and β.\n",
    "    - `track_running_stats=True`: keeps running statistics for inference.\n",
    "\n",
    "For initialization, PyTorch sets weights based on the **fan-in** (number of inputs per neuron) so that outputs remain stable. By default, weights are drawn from a uniform distribution scaled by \\(1/\\sqrt{\\text{fan\\_in}}\\), which ensures activations don’t blow up.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In plain words:**\n",
    "ResNet50 is built from repeating patterns of convolution → batch normalization → nonlinearity. Convolutions let the network understand images, BN keeps values stable, and nonlinearities let it learn complex functions. PyTorch automates much of this, carefully initializing weights and providing sensible defaults for BN so you don’t have to hand-tune everything.\n",
    "\n"
   ],
   "id": "89a600a8588947bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###  Lecture Summary\n",
    "\n",
    "- **Importance of Activations and Gradients**\n",
    "  - In deep networks, it is crucial to understand the *statistics* of activations and gradients.\n",
    "  - Poorly scaled activations can cause *confident mispredictions* leading to very high (\"hockey stick\") losses.\n",
    "  - Controlling activations prevents values from collapsing to zero or exploding to infinity.\n",
    "\n",
    "- **Weight Initialization**\n",
    "  - At initialization, we want activations to be *roughly Gaussian* across layers.\n",
    "  - Proper scaling of weights and biases can help maintain stable distributions.\n",
    "  - This works well for small/medium networks but becomes hard to manage for very deep networks with many types of layers.\n",
    "\n",
    "- **Normalization Layers**\n",
    "  - To solve scaling problems in deep networks, normalization layers were introduced.\n",
    "  - **Batch Normalization (BN)** (introduced ~2015) was the first and most influential.\n",
    "  - BN normalizes each batch by subtracting its mean and dividing by its standard deviation.\n",
    "  - It adds trainable **gain (γ)** and **bias (β)** parameters so the network can still learn useful transformations.\n",
    "\n",
    "- **Running Statistics**\n",
    "  - BN also maintains **running mean** and **running standard deviation** as buffers.\n",
    "  - These are not trained by gradients, but updated during training with a running average.\n",
    "  - At inference, these stored stats are used so single inputs can be processed consistently.\n",
    "\n",
    "- **Drawbacks of BatchNorm**\n",
    "  - BN couples training examples within a batch, causing “jitter” and making debugging harder.\n",
    "  - It has been known to introduce tricky bugs in practice.\n",
    "  - Alternatives like **Layer Normalization, Group Normalization, and Instance Normalization** are now common.\n",
    "\n",
    "- **Impact**\n",
    "  - Despite its issues, BN was a breakthrough that made training very deep networks feasible and stable.\n",
    "  - The key takeaway: controlling activation statistics is essential for good performance in deep learning.\n"
   ],
   "id": "422892f7b7c76a86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generating New Names",
   "id": "840945f243a6ae66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T13:31:38.761405Z",
     "start_time": "2025-08-26T13:31:38.678714Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carlah.\n",
      "quinlee.\n",
      "fregrachlett.\n",
      "skaelyn.\n",
      "jazonte.\n",
      "deviah.\n",
      "jacqui.\n",
      "nee.\n",
      "kentzirian.\n",
      "kaleigh.\n",
      "ham.\n",
      "pristen.\n",
      "juli.\n",
      "lilea.\n",
      "jadiquinterri.\n",
      "jaristin.\n",
      "jenni.\n",
      "sabell.\n",
      "edi.\n",
      "abette.\n",
      "jasfa.\n",
      "alpaelind.\n",
      "husy.\n",
      "jamelo.\n",
      "bostyn.\n",
      "jenia.\n",
      "jerren.\n",
      "kawsa.\n",
      "luco.\n",
      "zoessi.\n",
      "jian.\n",
      "ronie.\n",
      "montrynn.\n",
      "adeleighori.\n",
      "rylen.\n",
      "aya.\n",
      "kenzi.\n",
      "job.\n",
      "maddurreylanylah.\n",
      "kender.\n",
      "zani.\n",
      "adre.\n",
      "kai.\n",
      "skylie.\n",
      "ameleah.\n",
      "kypercy.\n",
      "oraly.\n",
      "jaxx.\n",
      "montrick.\n",
      "jazlindz.\n"
     ]
    }
   ],
   "execution_count": 24,
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(50):\n",
    "\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # ------------\n",
    "      # forward pass:\n",
    "      # Embedding\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + b1\n",
    "      hpreact = bngain * (hpreact - bnmean_running) * (bnstd_running + 1e-5)**-0.5 + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "      # ------------\n",
    "      # Sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ],
   "id": "c01fefe6a8421cea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
