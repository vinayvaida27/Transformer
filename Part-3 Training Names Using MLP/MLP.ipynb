{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:09.565817Z",
     "start_time": "2025-07-16T14:07:09.561845Z"
    }
   },
   "cell_type": "markdown",
   "source": "# Multi Layer Neural network",
   "id": "790ff87ed45e745e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:12.191078Z",
     "start_time": "2025-07-16T14:07:12.187419Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous experiments, we explored simple **bigram models** and **single-layer perceptrons**.\n",
    "While these models were useful for understanding the basics, they failed to capture **longer contexts** in names and often produced unrealistic generations.\n",
    "\n",
    "In this notebook, we extend the approach to a **Multi-Layer Perceptron (MLP)** that uses embeddings and hidden layers to learn richer representations of context.\n",
    "\n",
    "Key steps include:\n",
    "- Converting raw text (names) into input-output training pairs using a **context window (block size)**.\n",
    "- Encoding characters into a **low-dimensional embedding space**.\n",
    "- Passing embeddings through a **hidden layer** with non-linear activation (`tanh`).\n",
    "- Using an **output layer** to predict probabilities for the next character.\n",
    "- Training the model with **cross-entropy loss** and **gradient descent**, updating parameters iteratively.\n",
    "- Experimenting with **mini-batch training** and **learning rate scheduling** to improve efficiency.\n",
    "\n",
    "This workflow builds the foundation for **modern neural language models**, bridging from simple count-based models to deep learning methods.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Problem with the Bigram Model\n",
    "- The bigram model captures only **2 characters** of context.\n",
    "- This gives us:\n",
    "\n",
    "$$\n",
    "27 \\times 27 = 729 \\text{ combinations}\n",
    "$$\n",
    "\n",
    "If we extend to **3 characters of context**, the possibilities grow rapidly:\n",
    "\n",
    "$$\n",
    "27 \\times 27 \\times 27 = 19{,}683 \\text{ combinations}\n",
    "$$\n",
    "\n",
    "As context length increases, creating count vectors, normalizing, and training become increasingly difficult.\n",
    "\n",
    "---\n",
    "\n",
    "### Our Approach\n",
    "We follow the modeling approach from **Bengio et al., 2003**:\n",
    "[A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n"
   ],
   "id": "996af49d0f9c3650"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:33.705131Z",
     "start_time": "2025-08-26T14:03:30.153004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "id": "a2f91463e9020a1c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:33.765931Z",
     "start_time": "2025-08-26T14:03:33.715122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:5]"
   ],
   "id": "cdab35aafdd2ac69",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:33.779175Z",
     "start_time": "2025-08-26T14:03:33.772872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {\n",
    "    s: i+1 for i,s in enumerate(chars)\n",
    "}\n",
    "stoi['.'] = 0\n",
    "itos = { i:s for s,i in stoi.items() }\n"
   ],
   "id": "20a56c1d5b400d4e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating dataset for Neural Networks from Text File to Input Tensor and Output Tensor",
   "id": "67379305a8347179"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:33.814426Z",
     "start_time": "2025-08-26T14:03:33.805155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## data set for neural network\n",
    "\n",
    "block_size = 3 #Context_length: how many characters do we take to predict the next one?\n",
    "X,Y =[],[] # X are the input to neural network and Y are  the labels of the neural network\n",
    "\n",
    "for w in words[:1]:\n",
    "\n",
    "    print(w)\n",
    "    context = [0] * block_size\n",
    "\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "\n",
    "        print(\"input data : \",''.join(itos[i] for i in context), \", Output :\", '=', itos[ix])\n",
    "        print('context :', context)\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print(\"input tensor :\", X)\n",
    "\n",
    "print(\"output tensor :\", Y)"
   ],
   "id": "df30e971691caba6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "input data :  ... , Output : = e\n",
      "context : [0, 0, 0]\n",
      "input data :  ..e , Output : = m\n",
      "context : [0, 0, 5]\n",
      "input data :  .em , Output : = m\n",
      "context : [0, 5, 13]\n",
      "input data :  emm , Output : = a\n",
      "context : [5, 13, 13]\n",
      "input data :  mma , Output : = .\n",
      "context : [13, 13, 1]\n",
      "\n",
      "\n",
      "input tensor : tensor([[ 0,  0,  0],\n",
      "        [ 0,  0,  5],\n",
      "        [ 0,  5, 13],\n",
      "        [ 5, 13, 13],\n",
      "        [13, 13,  1]])\n",
      "output tensor : tensor([ 5, 13, 13,  1,  0])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:16.018547Z",
     "start_time": "2025-07-16T14:07:16.015039Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Example: Using the word *emma*\n",
    "\n",
    "To illustrate how the input-output pairs are formed, we take the word **\"emma\"** with context size = 3.\n",
    "At each step, the model looks at the **previous 3 characters** (context) and predicts the **next character**.\n",
    "\n",
    "---\n",
    "\n",
    "### Input-Output Mapping\n",
    "\n",
    "| Input Data | Output | stoi encoding |\n",
    "|------------|--------|---------------|\n",
    "| `...`      | `e`    | [0, 0, 0]     |\n",
    "| `..e`      | `m`    | [0, 0, 5]     |\n",
    "| `.em`      | `m`    | [0, 5, 13]    |\n",
    "| `emm`      | `a`    | [5, 13, 13]   |\n",
    "| `mma`      | `.`    | [13, 13, 1]   |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaway\n",
    "- The model uses **3-character context** to predict the next character.\n",
    "- Special tokens like `\".\"` are used to mark the **start** and **end** of words.\n",
    "- This process transforms raw text into **training pairs** (input → output) that the neural network learns from.\n"
   ],
   "id": "a9fc9355f19e4e55"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Block size = 5",
   "id": "86a2e1a58372fa34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:33.846896Z",
     "start_time": "2025-08-26T14:03:33.839574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_size = 5 #Context_length: how many characters do we take to predict the next one?\n",
    "X,Y =[],[] # x are inputs and Y are labels\n",
    "\n",
    "for w in words[:1]:\n",
    "\n",
    "    print( 'word is :',w)\n",
    "\n",
    "    print('\\n')\n",
    "    print('input -->  output')\n",
    "    context = [0] * block_size\n",
    "\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "\n",
    "        print(''.join(itos[i] for i in context), \"--> \", itos[ix])\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print(\"input tensor :\", X)\n",
    "\n",
    "print(\"output tensor :\", Y)"
   ],
   "id": "bbf8520f0eb7ca04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word is : emma\n",
      "\n",
      "\n",
      "input -->  output\n",
      "..... -->  e\n",
      "....e -->  m\n",
      "...em -->  m\n",
      "..emm -->  a\n",
      ".emma -->  .\n",
      "\n",
      "\n",
      "input tensor : tensor([[ 0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  5],\n",
      "        [ 0,  0,  0,  5, 13],\n",
      "        [ 0,  0,  5, 13, 13],\n",
      "        [ 0,  5, 13, 13,  1]])\n",
      "output tensor : tensor([ 5, 13, 13,  1,  0])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:33.878628Z",
     "start_time": "2025-08-26T14:03:33.872795Z"
    }
   },
   "cell_type": "code",
   "source": "X",
   "id": "45abbded1ac2a39a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  5],\n",
       "        [ 0,  0,  0,  5, 13],\n",
       "        [ 0,  0,  5, 13, 13],\n",
       "        [ 0,  5, 13, 13,  1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:34.843541Z",
     "start_time": "2025-08-26T14:03:33.933640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(Y)"
   ],
   "id": "6fe0f90d2162c136",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    0\n",
       "0   5\n",
       "1  13\n",
       "2  13\n",
       "3   1\n",
       "4   0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### above we are trying to take 5 character as input and predicting what comes next as o/p",
   "id": "9695ecaf3199aba3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:41.935126Z",
     "start_time": "2025-08-26T14:03:41.926955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_size = 3 #Context_length: how many characters do we take to predict the next one?\n",
    "X,Y =[],[] # x are inputs and Y are labels\n",
    "\n",
    "for w in words[:5]:\n",
    "\n",
    "    #print(w)\n",
    "    context = [0] * block_size\n",
    "\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "\n",
    "        #print(\"input data : \",''.join(itos[i] for i in context), \", Output :\", '=', itos[ix])\n",
    "        #print('context :', context)\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "X"
   ],
   "id": "ab013fef48297968",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:42.154431Z",
     "start_time": "2025-08-26T14:03:42.147433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(Y)"
   ],
   "id": "9fbc192d9e0b8e45",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     0\n",
       "0    5\n",
       "1   13\n",
       "2   13\n",
       "3    1\n",
       "4    0\n",
       "5   15\n",
       "6   12\n",
       "7    9\n",
       "8   22\n",
       "9    9\n",
       "10   1\n",
       "11   0\n",
       "12   1\n",
       "13  22\n",
       "14   1\n",
       "15   0\n",
       "16   9\n",
       "17  19\n",
       "18   1\n",
       "19   2\n",
       "20   5\n",
       "21  12\n",
       "22  12\n",
       "23   1\n",
       "24   0\n",
       "25  19\n",
       "26  15\n",
       "27  16\n",
       "28   8\n",
       "29   9\n",
       "30   1\n",
       "31   0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:23.534970Z",
     "start_time": "2025-07-16T14:07:23.530657Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Training Data is Ready\n",
    "\n",
    "1. The training data with **block_size/context = 3** is now prepared.\n",
    "2. The corresponding outputs are also ready for the model to train on.\n"
   ],
   "id": "fc92b982c2cb0ccf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:42.603073Z",
     "start_time": "2025-08-26T14:03:42.598424Z"
    }
   },
   "cell_type": "code",
   "source": "(X.shape, X.dtype), (Y.shape, Y.dtype)",
   "id": "f6abc7661359b0cd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((torch.Size([32, 3]), torch.int64), (torch.Size([32]), torch.int64))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:23.908205Z",
     "start_time": "2025-07-16T14:07:23.905205Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Tensor Setup\n",
    "\n",
    "- Our tensor matrix **X** has `228,146` rows and `3` columns, with both columns having `dtype = integer`.\n",
    "\n",
    "- In the reference paper, they used **17,000 words**, each embedded in a **30-dimensional space**.\n",
    "\n",
    "- In our case, we have only **27 possible characters**, so we will use a **2-dimensional space**.\n",
    "  - This means we will have a tensor of shape **27 × 2** to represent all characters.\n"
   ],
   "id": "31047ca8203dff5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:43.010717Z",
     "start_time": "2025-08-26T14:03:43.003766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# let embedded the 2 random numbers with 27*2\n",
    "\n",
    "C = torch.randn((27,2)) # this is used as an embedding layer\n",
    "C"
   ],
   "id": "90c4cf80a2bf2f3c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7449, -0.4585],\n",
       "        [-1.7828,  0.3992],\n",
       "        [ 1.8466,  0.9018],\n",
       "        [ 0.6683,  0.0556],\n",
       "        [ 1.8014,  0.9805],\n",
       "        [ 0.6096, -0.6466],\n",
       "        [ 0.5734,  0.1692],\n",
       "        [ 0.9185, -1.8598],\n",
       "        [-0.1182,  0.5036],\n",
       "        [-2.0337, -0.2660],\n",
       "        [-0.6742,  0.6095],\n",
       "        [ 0.9076, -0.1282],\n",
       "        [-0.1066,  2.7571],\n",
       "        [-0.1990,  1.0903],\n",
       "        [-1.4460, -1.3872],\n",
       "        [ 0.3560,  0.1844],\n",
       "        [ 1.7420, -2.9093],\n",
       "        [ 0.4414,  0.7655],\n",
       "        [ 0.0964,  0.0126],\n",
       "        [-0.1465, -1.2818],\n",
       "        [-0.7004,  0.0200],\n",
       "        [ 1.1540,  0.5290],\n",
       "        [-0.7146, -0.7522],\n",
       "        [-1.5216,  0.0821],\n",
       "        [-1.1048, -1.3288],\n",
       "        [ 2.2363,  1.0640],\n",
       "        [-0.4536,  0.6131]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:43.189659Z",
     "start_time": "2025-08-26T14:03:43.183198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Embedding of number 5 in C encoding\n",
    "C[5]"
   ],
   "id": "8d1c6c7ebbdd3d61",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6096, -0.6466])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Example of Encoding\n",
    "\n",
    "Suppose we want to encode the number **5** when the total number of classes = **27 (characters)**.\n",
    "We use one-hot encoding and then multiply it with the weight matrix.\n",
    "\n",
    "$$\n",
    "[1 \\times 27] \\; @ \\; [27 \\times 2] \\; = \\; [1 \\times 2] \\; \\text{tensor}\n",
    "$$\n",
    "\n",
    "- **[1 × 27]** → One-hot encoded vector for class index = 5\n",
    "- **[27 × 2]** → Weight matrix (each character embedded in 2D space)\n",
    "- **[1 × 2]** → Encoded vector in the 2D embedding space\n"
   ],
   "id": "39ff5cb489a643dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:43.556663Z",
     "start_time": "2025-08-26T14:03:43.549900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Example of encoding the number 5 with number of classes = 27(charac) and multiplying it with the weights matrix\n",
    "# [1*27] @ [27*2] = [1*2] tensor\n",
    "\n",
    "(F.one_hot(torch.tensor(5), num_classes=27).float()) @ C"
   ],
   "id": "f3a337bff25f2141",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6096, -0.6466])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:24.340097Z",
     "start_time": "2025-07-16T14:07:24.337599Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### First Layer Computation\n",
    "\n",
    "If we look up the embedding matrix **C** at the given input index **X**,\n",
    "we directly obtain the **result from the first layer**:\n",
    "\n",
    "$$\n",
    "\\text{Output} = C[X]\n",
    "$$\n",
    "\n",
    "- **C** → Embedding matrix\n",
    "- **X** → Input index (or context indices)\n",
    "- **C[X]** → Corresponding embedding vector(s)\n"
   ],
   "id": "a96dabf986022ee2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:43.654979Z",
     "start_time": "2025-08-26T14:03:43.648808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "C = torch.randn((27,2))\n",
    "C ## c is noting buit one type of encoding"
   ],
   "id": "9f163d4210a00987",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2942,  0.2740],\n",
       "        [ 0.2644, -1.6462],\n",
       "        [-0.7768, -0.1596],\n",
       "        [-0.9592, -1.1740],\n",
       "        [-0.9742,  0.3060],\n",
       "        [ 0.3096,  0.0227],\n",
       "        [-0.0236, -0.5425],\n",
       "        [-0.5322, -0.3443],\n",
       "        [-2.1710,  0.4784],\n",
       "        [ 0.6612, -0.0670],\n",
       "        [-1.0901, -0.3320],\n",
       "        [ 1.0183, -2.0291],\n",
       "        [-0.7829, -1.5884],\n",
       "        [-1.1424,  0.3848],\n",
       "        [-0.9952,  0.4301],\n",
       "        [-0.3984, -0.9173],\n",
       "        [ 0.4635, -0.0327],\n",
       "        [-0.7100,  0.9021],\n",
       "        [-0.8982, -0.8947],\n",
       "        [-0.0979,  0.9512],\n",
       "        [ 0.3273, -0.8611],\n",
       "        [-1.9008,  1.1420],\n",
       "        [-2.1613, -0.3094],\n",
       "        [-1.4496,  0.9121],\n",
       "        [ 1.7011,  0.2139],\n",
       "        [-1.2487,  0.3933],\n",
       "        [ 0.0188,  2.1263]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:43.772364Z",
     "start_time": "2025-08-26T14:03:43.767028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we want to encode one number 5 we can simply pass this number to the Embedding tensor C and get the output i.e.,\n",
    "\n",
    "C[5] #Give embedding of 5 in 2d Vector Space"
   ],
   "id": "655e7724225f6ce6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3096, 0.0227])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:44.135981Z",
     "start_time": "2025-08-26T14:03:44.130592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# what if the embedding is like list\n",
    "print(C[[2,3,4]]) # we got the embedding of 3 integer vector as below\n"
   ],
   "id": "7af03145d23088db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7768, -0.1596],\n",
      "        [-0.9592, -1.1740],\n",
      "        [-0.9742,  0.3060]])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:44.288237Z",
     "start_time": "2025-08-26T14:03:44.283402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# what if we give the tensor input to get embedding\n",
    "\n",
    "print(C[torch.tensor([5,6,7,7,7,7,7,7,])]) # still we are able to get the embedding by giving tensor matrix as input"
   ],
   "id": "684794359b52802b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3096,  0.0227],\n",
      "        [-0.0236, -0.5425],\n",
      "        [-0.5322, -0.3443],\n",
      "        [-0.5322, -0.3443],\n",
      "        [-0.5322, -0.3443],\n",
      "        [-0.5322, -0.3443],\n",
      "        [-0.5322, -0.3443],\n",
      "        [-0.5322, -0.3443]])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:24.703118Z",
     "start_time": "2025-07-16T14:07:24.699023Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Explanation of **C** (Embedding Matrix)\n",
    "\n",
    "- **C** is the embedding matrix.\n",
    "- When you pass a **single integer**, you get the embedding vector for that specific index.\n",
    "- When you pass a **list of integers**, you get the embeddings for each element in the list.\n",
    "- When you pass a **tensor of indices**, you get the embeddings for the entire tensor.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula\n",
    "\n",
    "If **C** has shape:\n",
    "\n",
    "$$\n",
    "[\\text{vocab\\_size} \\times \\text{embedding\\_dim}]\n",
    "$$\n",
    "\n",
    "and **X** is an index or collection of indices, then:\n",
    "\n",
    "$$\n",
    "\\text{Output} = C[X]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "\n",
    "- **Single index:**\n",
    "  $$ C[5] \\;\\;\\to\\;\\; [1 \\times \\text{embedding\\_dim}] $$\n",
    "\n",
    "- **List of indices:**\n",
    "  $$ C[[5, 13, 1]] \\;\\;\\to\\;\\; [3 \\times \\text{embedding\\_dim}] $$\n",
    "\n",
    "- **Tensor of shape (batch, context):**\n",
    "  $$ C[X] \\;\\;\\to\\;\\; [\\text{batch} \\times \\text{context} \\times \\text{embedding\\_dim}] $$\n"
   ],
   "id": "5369743b8b0648b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:44.356304Z",
     "start_time": "2025-08-26T14:03:44.347117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding = C[X]\n",
    "embedding"
   ],
   "id": "84e3f5464cbff43f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2942,  0.2740],\n",
       "         [ 0.2942,  0.2740],\n",
       "         [ 0.2942,  0.2740]],\n",
       "\n",
       "        [[ 0.2942,  0.2740],\n",
       "         [ 0.2942,  0.2740],\n",
       "         [ 0.3096,  0.0227]],\n",
       "\n",
       "        [[ 0.2942,  0.2740],\n",
       "         [ 0.3096,  0.0227],\n",
       "         [-1.1424,  0.3848]],\n",
       "\n",
       "        [[ 0.3096,  0.0227],\n",
       "         [-1.1424,  0.3848],\n",
       "         [-1.1424,  0.3848]],\n",
       "\n",
       "        [[-1.1424,  0.3848],\n",
       "         [-1.1424,  0.3848],\n",
       "         [ 0.2644, -1.6462]],\n",
       "\n",
       "        [[ 0.2942,  0.2740],\n",
       "         [ 0.2942,  0.2740],\n",
       "         [ 0.2942,  0.2740]],\n",
       "\n",
       "        [[ 0.2942,  0.2740],\n",
       "         [ 0.2942,  0.2740],\n",
       "         [-0.3984, -0.9173]],\n",
       "\n",
       "        [[ 0.2942,  0.2740],\n",
       "         [-0.3984, -0.9173],\n",
       "         [-0.7829, -1.5884]],\n",
       "\n",
       "        [[-0.3984, -0.9173],\n",
       "         [-0.7829, -1.5884],\n",
       "         [ 0.6612, -0.0670]],\n",
       "\n",
       "        [[-0.7829, -1.5884],\n",
       "         [ 0.6612, -0.0670],\n",
       "         [-2.1613, -0.3094]],\n",
       "\n",
       "        [[ 0.6612, -0.0670],\n",
       "         [-2.1613, -0.3094],\n",
       "         [ 0.6612, -0.0670]],\n",
       "\n",
       "        [[-2.1613, -0.3094],\n",
       "         [ 0.6612, -0.0670],\n",
       "         [ 0.2644, -1.6462]],\n",
       "\n",
       "        [[ 0.2942,  0.2740],\n",
       "         [ 0.2942,  0.2740],\n",
       "         [ 0.2942,  0.2740]],\n",
       "\n",
       "        [[ 0.2942,  0.2740],\n",
       "         [ 0.2942,  0.2740],\n",
       "         [ 0.2644, -1.6462]],\n",
       "\n",
       "        [[ 0.2942,  0.2740],\n",
       "         [ 0.2644, -1.6462],\n",
       "         [-2.1613, -0.3094]],\n",
       "\n",
       "        [[ 0.2644, -1.6462],\n",
       "         [-2.1613, -0.3094],\n",
       "         [ 0.2644, -1.6462]],\n",
       "\n",
       "        [[ 0.2942,  0.2740],\n",
       "         [ 0.2942,  0.2740],\n",
       "         [ 0.2942,  0.2740]],\n",
       "\n",
       "        [[ 0.2942,  0.2740],\n",
       "         [ 0.2942,  0.2740],\n",
       "         [ 0.6612, -0.0670]],\n",
       "\n",
       "        [[ 0.2942,  0.2740],\n",
       "         [ 0.6612, -0.0670],\n",
       "         [-0.0979,  0.9512]],\n",
       "\n",
       "        [[ 0.6612, -0.0670],\n",
       "         [-0.0979,  0.9512],\n",
       "         [ 0.2644, -1.6462]],\n",
       "\n",
       "        [[-0.0979,  0.9512],\n",
       "         [ 0.2644, -1.6462],\n",
       "         [-0.7768, -0.1596]],\n",
       "\n",
       "        [[ 0.2644, -1.6462],\n",
       "         [-0.7768, -0.1596],\n",
       "         [ 0.3096,  0.0227]],\n",
       "\n",
       "        [[-0.7768, -0.1596],\n",
       "         [ 0.3096,  0.0227],\n",
       "         [-0.7829, -1.5884]],\n",
       "\n",
       "        [[ 0.3096,  0.0227],\n",
       "         [-0.7829, -1.5884],\n",
       "         [-0.7829, -1.5884]],\n",
       "\n",
       "        [[-0.7829, -1.5884],\n",
       "         [-0.7829, -1.5884],\n",
       "         [ 0.2644, -1.6462]],\n",
       "\n",
       "        [[ 0.2942,  0.2740],\n",
       "         [ 0.2942,  0.2740],\n",
       "         [ 0.2942,  0.2740]],\n",
       "\n",
       "        [[ 0.2942,  0.2740],\n",
       "         [ 0.2942,  0.2740],\n",
       "         [-0.0979,  0.9512]],\n",
       "\n",
       "        [[ 0.2942,  0.2740],\n",
       "         [-0.0979,  0.9512],\n",
       "         [-0.3984, -0.9173]],\n",
       "\n",
       "        [[-0.0979,  0.9512],\n",
       "         [-0.3984, -0.9173],\n",
       "         [ 0.4635, -0.0327]],\n",
       "\n",
       "        [[-0.3984, -0.9173],\n",
       "         [ 0.4635, -0.0327],\n",
       "         [-2.1710,  0.4784]],\n",
       "\n",
       "        [[ 0.4635, -0.0327],\n",
       "         [-2.1710,  0.4784],\n",
       "         [ 0.6612, -0.0670]],\n",
       "\n",
       "        [[-2.1710,  0.4784],\n",
       "         [ 0.6612, -0.0670],\n",
       "         [ 0.2644, -1.6462]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:44.426044Z",
     "start_time": "2025-08-26T14:03:44.421098Z"
    }
   },
   "cell_type": "code",
   "source": "embedding.shape",
   "id": "fdfda423e7a1a6e3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "now our input is embeeded",
   "id": "b540d145d62b6000"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:44.490639Z",
     "start_time": "2025-08-26T14:03:44.482602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# so to convert the input vector to 2 dimensional space there are manu ways to represent it\n",
    "# but one of the effective and easiest way pass dotview  .view(x,6)\n",
    "\n",
    "embedding.view(-1, 6)"
   ],
   "id": "458f836736cf59cf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2942,  0.2740,  0.2942,  0.2740,  0.2942,  0.2740],\n",
       "        [ 0.2942,  0.2740,  0.2942,  0.2740,  0.3096,  0.0227],\n",
       "        [ 0.2942,  0.2740,  0.3096,  0.0227, -1.1424,  0.3848],\n",
       "        [ 0.3096,  0.0227, -1.1424,  0.3848, -1.1424,  0.3848],\n",
       "        [-1.1424,  0.3848, -1.1424,  0.3848,  0.2644, -1.6462],\n",
       "        [ 0.2942,  0.2740,  0.2942,  0.2740,  0.2942,  0.2740],\n",
       "        [ 0.2942,  0.2740,  0.2942,  0.2740, -0.3984, -0.9173],\n",
       "        [ 0.2942,  0.2740, -0.3984, -0.9173, -0.7829, -1.5884],\n",
       "        [-0.3984, -0.9173, -0.7829, -1.5884,  0.6612, -0.0670],\n",
       "        [-0.7829, -1.5884,  0.6612, -0.0670, -2.1613, -0.3094],\n",
       "        [ 0.6612, -0.0670, -2.1613, -0.3094,  0.6612, -0.0670],\n",
       "        [-2.1613, -0.3094,  0.6612, -0.0670,  0.2644, -1.6462],\n",
       "        [ 0.2942,  0.2740,  0.2942,  0.2740,  0.2942,  0.2740],\n",
       "        [ 0.2942,  0.2740,  0.2942,  0.2740,  0.2644, -1.6462],\n",
       "        [ 0.2942,  0.2740,  0.2644, -1.6462, -2.1613, -0.3094],\n",
       "        [ 0.2644, -1.6462, -2.1613, -0.3094,  0.2644, -1.6462],\n",
       "        [ 0.2942,  0.2740,  0.2942,  0.2740,  0.2942,  0.2740],\n",
       "        [ 0.2942,  0.2740,  0.2942,  0.2740,  0.6612, -0.0670],\n",
       "        [ 0.2942,  0.2740,  0.6612, -0.0670, -0.0979,  0.9512],\n",
       "        [ 0.6612, -0.0670, -0.0979,  0.9512,  0.2644, -1.6462],\n",
       "        [-0.0979,  0.9512,  0.2644, -1.6462, -0.7768, -0.1596],\n",
       "        [ 0.2644, -1.6462, -0.7768, -0.1596,  0.3096,  0.0227],\n",
       "        [-0.7768, -0.1596,  0.3096,  0.0227, -0.7829, -1.5884],\n",
       "        [ 0.3096,  0.0227, -0.7829, -1.5884, -0.7829, -1.5884],\n",
       "        [-0.7829, -1.5884, -0.7829, -1.5884,  0.2644, -1.6462],\n",
       "        [ 0.2942,  0.2740,  0.2942,  0.2740,  0.2942,  0.2740],\n",
       "        [ 0.2942,  0.2740,  0.2942,  0.2740, -0.0979,  0.9512],\n",
       "        [ 0.2942,  0.2740, -0.0979,  0.9512, -0.3984, -0.9173],\n",
       "        [-0.0979,  0.9512, -0.3984, -0.9173,  0.4635, -0.0327],\n",
       "        [-0.3984, -0.9173,  0.4635, -0.0327, -2.1710,  0.4784],\n",
       "        [ 0.4635, -0.0327, -2.1710,  0.4784,  0.6612, -0.0670],\n",
       "        [-2.1710,  0.4784,  0.6612, -0.0670,  0.2644, -1.6462]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hidden Layer\n",
    "\n",
    "## Passing to the Second Layer from embedding layer\n",
    "\n",
    "- The **second layer** consists of a weight matrix \\( W_1 \\) and a bias vector \\( b_1 \\).\n",
    "- The output of the second layer is computed as:\n",
    "\n",
    "$$\n",
    "h = X \\cdot W_1 + b_1\n",
    "$$\n",
    "\n",
    "- If we apply the non-linearity (**tanh**), we get:\n",
    "\n",
    "$$\n",
    "h = \\tanh(W_1 \\cdot X + b_1)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "- Suppose the second layer has **100 weights** → \\( W_1 \\) has shape:\n",
    "\n",
    "$$\n",
    "[\\text{input\\_dim} \\times 100]\n",
    "$$\n",
    "\n",
    "- Then there are **100 biases** → \\( b_1 \\) has shape:\n",
    "\n",
    "$$\n",
    "[1 \\times 100]\n",
    "$$\n",
    "\n",
    "- The final hidden representation is:\n",
    "\n",
    "$$\n",
    "h \\in \\mathbb{R}^{1 \\times 100}\n",
    "$$\n"
   ],
   "id": "bb6e91bcb1efc137"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:44.584409Z",
     "start_time": "2025-08-26T14:03:44.580523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## now if we want to pass it to the second layer\n",
    "\n",
    "# consider second layer as weight w1 and bias b1\n",
    "# then o/p of second layer = i/p @w1 + b1\n",
    "\n",
    "#lets consider we have 100 weights in second layer such that we have 100 biases in second layer too h = tanh(w1*x1 + b1)\n",
    "\n",
    "W1 = torch.rand((6,100))\n",
    "B1 = torch.rand(100)"
   ],
   "id": "1199d474f81bf1fd",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:44.959029Z",
     "start_time": "2025-08-26T14:03:44.947404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "h = torch.tanh(embedding.view(-1, 6) @ W1 + B1)\n",
    "h"
   ],
   "id": "826ec8ef8103ae02",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8210,  0.7669,  0.7081,  ...,  0.6870,  0.8840,  0.8089],\n",
       "        [ 0.7252,  0.6706,  0.5843,  ...,  0.5487,  0.8595,  0.8079],\n",
       "        [ 0.6842,  0.0850,  0.3786,  ...,  0.5109,  0.3272,  0.6239],\n",
       "        ...,\n",
       "        [-0.6113, -0.7283, -0.3036,  ...,  0.1433, -0.9669, -0.3426],\n",
       "        [-0.7693, -0.7197, -0.9238,  ..., -0.7348,  0.7725, -0.0166],\n",
       "        [-0.9884, -0.7829, -0.8191,  ..., -0.9434, -0.8579, -0.4164]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:45.111980Z",
     "start_time": "2025-08-26T14:03:45.107276Z"
    }
   },
   "cell_type": "code",
   "source": "h.shape",
   "id": "b341240b34bf1f18",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Output Layer\n",
    "\n",
    "The **output layer** is computed as:\n",
    "\n",
    "$$\n",
    "\\text{logits} = h \\cdot W_2 + b_2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- \\( h \\) = hidden layer (e.g., size $$(1 \\times 100))$$\n",
    "- \\( W_2 \\) = weight matrix of shape $$([100 \\times 27])$$\n",
    "- \\( b_2 \\) = bias vector of shape $$([1 \\times 27])$$\n",
    "\n",
    "---\n",
    "\n",
    "### Key Idea\n",
    "- We only have **27 possible characters** in our vocabulary.\n",
    "- Therefore, the output layer must map from **100 hidden inputs** → **27 outputs**.\n",
    "- Each of the 27 outputs corresponds to the score (logit) for a particular character.\n"
   ],
   "id": "32cfed29a52fdd4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:45.472094Z",
     "start_time": "2025-08-26T14:03:45.468352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "-> We are creating the output layer here -> output layer = weight * hidden layer + Bias\n",
    "\n",
    "-> but we have only 27 character, so the layer needs to be 27 weights + 27 weights i.e., we need to take 100 inputs and give only 27 o/p\n",
    "\n",
    "->\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "W2 = torch.randn((100,27))\n",
    "B2 = torch.randn(27)"
   ],
   "id": "bffc94ac57b68e38",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:45.938482Z",
     "start_time": "2025-08-26T14:03:45.720237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logits = h @ W2+ B2\n",
    "logits"
   ],
   "id": "381af8ca36b3c856",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.4828e+00, -4.7235e+00,  6.0769e+00, -1.0728e+01,  5.2908e+00,\n",
       "         -3.8109e+00,  1.4558e+00, -8.6665e+00,  2.4420e+01,  1.6117e+01,\n",
       "          1.1084e+01, -4.2240e+00,  1.8020e+00, -7.7547e+00, -7.4275e+00,\n",
       "         -3.9043e+00, -4.0252e+00, -3.4028e+00, -5.7785e+00,  5.9162e+00,\n",
       "          2.0683e-01,  7.6149e+00, -7.9877e+00,  9.3866e+00,  8.0706e+00,\n",
       "          7.3596e-01, -5.3341e+00],\n",
       "        [-4.3453e+00, -4.6267e+00,  5.6571e+00, -1.0220e+01,  5.3992e+00,\n",
       "         -3.4473e+00,  1.2725e+00, -7.8682e+00,  2.4316e+01,  1.6184e+01,\n",
       "          1.0457e+01, -3.9366e+00,  2.2501e+00, -7.9159e+00, -7.2621e+00,\n",
       "         -3.8775e+00, -3.7157e+00, -3.7376e+00, -6.0313e+00,  5.7162e+00,\n",
       "          3.8249e-01,  6.7774e+00, -8.0717e+00,  9.4023e+00,  7.5804e+00,\n",
       "          5.3987e-01, -5.0560e+00],\n",
       "        [-6.5015e+00, -7.9534e+00,  2.6593e+00, -6.8494e+00,  6.0315e+00,\n",
       "         -1.6389e+00,  2.2716e+00, -2.1125e+00,  1.0993e+01,  1.2318e+01,\n",
       "          6.3369e+00, -1.1326e+00,  3.9612e+00, -6.9933e+00, -2.7182e+00,\n",
       "         -2.4122e+00, -5.7859e+00, -1.7975e+00,  1.9334e+00, -7.9127e-01,\n",
       "         -1.1142e+00, -9.3387e-01, -9.8759e+00,  5.1405e+00, -2.1892e-01,\n",
       "          4.5402e+00, -3.9985e-01],\n",
       "        [-3.0602e+00, -1.1450e+01,  1.1658e+00, -2.8706e+00, -9.2255e-01,\n",
       "          5.2775e+00,  7.1273e+00,  3.0224e+00, -8.4720e+00,  1.2113e+01,\n",
       "          2.3207e+00,  8.3666e+00,  8.3517e+00, -8.6155e-01,  5.0340e+00,\n",
       "         -9.6218e-02, -3.2262e+00, -2.7216e+00,  6.0700e+00, -5.8709e-01,\n",
       "          2.5616e+00, -8.3845e+00, -6.0631e+00, -2.4766e+00, -6.8575e+00,\n",
       "          5.2721e+00,  5.0106e+00],\n",
       "        [ 2.8606e+00, -2.5624e+00,  2.7524e+00,  7.8360e+00, -3.4940e-01,\n",
       "          1.0235e+01,  1.7426e+00,  1.0761e+01, -1.2400e+01,  2.1896e+00,\n",
       "         -7.0364e+00,  9.9452e+00,  5.7033e+00,  2.4549e+00,  2.3784e+00,\n",
       "          3.3254e+00,  3.2331e+00, -5.2471e+00,  2.0282e+00, -1.5928e+00,\n",
       "          5.8906e-01, -9.5020e+00,  3.1839e+00, -8.4479e+00, -4.7075e+00,\n",
       "         -1.5028e+00,  1.6964e-01],\n",
       "        [-4.4828e+00, -4.7235e+00,  6.0769e+00, -1.0728e+01,  5.2908e+00,\n",
       "         -3.8109e+00,  1.4558e+00, -8.6665e+00,  2.4420e+01,  1.6117e+01,\n",
       "          1.1084e+01, -4.2240e+00,  1.8020e+00, -7.7547e+00, -7.4275e+00,\n",
       "         -3.9043e+00, -4.0252e+00, -3.4028e+00, -5.7785e+00,  5.9162e+00,\n",
       "          2.0683e-01,  7.6149e+00, -7.9877e+00,  9.3866e+00,  8.0706e+00,\n",
       "          7.3596e-01, -5.3341e+00],\n",
       "        [-4.9154e+00, -4.6505e+00, -1.0356e+00, -4.0029e+00,  5.3139e+00,\n",
       "          2.9165e-01, -2.5682e-01,  1.7186e+00,  1.4591e+01,  1.5852e+01,\n",
       "          3.4031e+00, -7.5795e-01,  6.9024e+00, -1.0610e+01, -4.6456e+00,\n",
       "         -4.6480e+00, -3.2918e+00, -5.5072e+00, -2.4689e+00, -9.2397e-01,\n",
       "          1.6278e+00, -2.9606e+00, -9.6824e+00,  6.2684e+00,  1.8370e-01,\n",
       "          1.0859e+00,  8.1790e-01],\n",
       "        [-2.7197e-01,  3.9628e-01, -3.8798e-01,  7.5817e+00,  3.3447e+00,\n",
       "          6.0396e+00, -4.2404e+00,  1.0286e+01, -1.4398e+01, -4.7992e+00,\n",
       "         -6.1912e+00,  5.9911e+00,  6.3066e+00, -3.8081e+00,  6.1033e+00,\n",
       "         -3.4033e+00,  8.0204e-01, -1.1243e+00,  3.0916e+00, -9.7867e+00,\n",
       "          9.7272e-01, -7.9118e+00,  2.1403e+00, -7.2005e+00, -6.5602e+00,\n",
       "          2.3350e+00,  2.4905e-01],\n",
       "        [ 1.1743e+00, -2.7765e+00,  1.6218e+00,  5.6974e+00,  2.8802e+00,\n",
       "          6.5700e+00,  1.1346e-01,  1.0331e+01, -1.5757e+01, -8.2766e+00,\n",
       "         -6.5440e+00,  5.7291e+00,  5.1263e+00,  5.5314e+00,  3.3378e+00,\n",
       "          3.5406e+00,  4.5002e+00, -3.2859e+00,  3.9872e+00, -2.1239e+00,\n",
       "         -5.2772e-01,  2.0704e-01,  8.2950e+00, -8.1735e+00, -6.5821e+00,\n",
       "         -1.9122e+00, -3.3225e+00],\n",
       "        [ 4.5711e+00,  1.8057e-01, -9.0129e-01,  8.9533e+00,  2.5631e+00,\n",
       "          5.6207e+00, -1.7135e+00,  1.1720e+01, -2.4258e+01, -7.8160e+00,\n",
       "         -6.0555e+00,  3.2944e+00,  1.4534e+00,  6.5627e+00,  4.6226e+00,\n",
       "          8.8794e-01,  2.8386e+00,  4.4988e-01,  9.8038e+00, -4.8445e+00,\n",
       "          6.3609e-01, -9.9835e+00,  4.8946e+00, -1.0820e+01, -1.2532e+01,\n",
       "          3.4615e+00,  3.6835e+00],\n",
       "        [-2.1351e+00, -5.4834e+00,  4.2450e+00, -1.2750e-01, -3.0848e+00,\n",
       "          8.2813e+00,  4.9423e+00,  4.9907e+00, -2.5628e+00,  9.5576e+00,\n",
       "         -1.0680e+00,  1.2789e+01,  1.1388e+01, -2.4879e+00,  1.6638e+00,\n",
       "         -1.2759e+00, -3.8436e+00, -9.3821e+00,  1.4381e+00,  5.0477e+00,\n",
       "          3.3723e+00,  6.6899e-01,  5.5393e+00, -8.1910e-01,  3.4745e+00,\n",
       "         -2.8758e+00,  1.1057e-01],\n",
       "        [ 5.6133e+00,  1.7857e+00,  2.7817e+00,  8.5858e+00,  5.9107e+00,\n",
       "          5.3952e+00, -2.1560e+00,  1.3657e+01, -1.2080e+01, -3.4595e+00,\n",
       "         -6.1609e+00,  4.6895e+00,  8.8357e-01,  2.6612e+00, -3.9997e-01,\n",
       "          2.8298e+00,  5.5167e+00, -1.9235e+00,  4.8350e+00, -5.1212e+00,\n",
       "         -1.7117e-01, -1.0503e+01,  4.1600e+00, -1.1917e+01, -1.0654e+01,\n",
       "         -1.4612e+00, -2.6924e+00],\n",
       "        [-4.4828e+00, -4.7235e+00,  6.0769e+00, -1.0728e+01,  5.2908e+00,\n",
       "         -3.8109e+00,  1.4558e+00, -8.6665e+00,  2.4420e+01,  1.6117e+01,\n",
       "          1.1084e+01, -4.2240e+00,  1.8020e+00, -7.7547e+00, -7.4275e+00,\n",
       "         -3.9043e+00, -4.0252e+00, -3.4028e+00, -5.7785e+00,  5.9162e+00,\n",
       "          2.0683e-01,  7.6149e+00, -7.9877e+00,  9.3866e+00,  8.0706e+00,\n",
       "          7.3596e-01, -5.3341e+00],\n",
       "        [-4.1047e+00, -5.7653e-01, -1.5181e+00, -1.6267e+00,  5.7849e+00,\n",
       "          1.9803e+00, -1.5058e+00,  4.1251e+00,  1.8447e+01,  1.5757e+01,\n",
       "          1.7256e+00, -4.7260e-01,  7.6030e+00, -1.2275e+01, -6.5487e+00,\n",
       "         -5.5602e+00, -2.5220e+00, -7.3086e+00, -5.6162e+00, -5.2755e-01,\n",
       "          2.7358e+00, -2.0586e+00, -7.0009e+00,  6.8320e+00,  1.5344e+00,\n",
       "         -2.2817e+00,  7.1123e-01],\n",
       "        [-6.9657e-01, -9.4090e-01, -2.1800e+00,  9.7006e+00,  3.4834e+00,\n",
       "          5.0991e+00, -5.6606e+00,  7.4817e+00, -2.0461e+01, -1.0017e+01,\n",
       "         -6.4828e+00,  1.9641e+00,  5.5861e+00, -1.1601e+00,  7.0091e+00,\n",
       "         -1.8383e+00,  4.2259e-01,  6.6740e+00,  6.1750e+00, -1.3148e+01,\n",
       "          2.3650e+00, -7.1266e+00,  8.8302e-01, -7.7284e+00, -9.3875e+00,\n",
       "          7.3696e+00,  1.4679e+00],\n",
       "        [ 3.9678e+00,  6.8217e-01, -1.6370e+00,  8.2389e+00, -4.8125e+00,\n",
       "          6.8313e+00, -1.0279e+00,  1.0041e+01, -2.5170e+01, -9.8672e+00,\n",
       "         -1.0056e+01,  7.1983e+00,  3.0465e+00,  6.5013e+00,  6.0080e+00,\n",
       "          1.5565e+00,  3.2031e+00, -1.9469e+00,  3.5765e+00, -3.2432e+00,\n",
       "         -1.3214e+00, -7.9546e+00,  1.1457e+01, -6.0500e+00, -6.7024e+00,\n",
       "          1.2292e+00,  2.1335e+00],\n",
       "        [-4.4828e+00, -4.7235e+00,  6.0769e+00, -1.0728e+01,  5.2908e+00,\n",
       "         -3.8109e+00,  1.4558e+00, -8.6665e+00,  2.4420e+01,  1.6117e+01,\n",
       "          1.1084e+01, -4.2240e+00,  1.8020e+00, -7.7547e+00, -7.4275e+00,\n",
       "         -3.9043e+00, -4.0252e+00, -3.4028e+00, -5.7785e+00,  5.9162e+00,\n",
       "          2.0683e-01,  7.6149e+00, -7.9877e+00,  9.3866e+00,  8.0706e+00,\n",
       "          7.3596e-01, -5.3341e+00],\n",
       "        [-4.1795e+00, -4.2243e+00,  6.0574e+00, -1.0565e+01,  5.4658e+00,\n",
       "         -3.7833e+00,  1.0759e+00, -8.5752e+00,  2.5293e+01,  1.6335e+01,\n",
       "          1.0956e+01, -4.3026e+00,  1.9479e+00, -7.7777e+00, -7.6079e+00,\n",
       "         -3.9588e+00, -3.6131e+00, -3.7076e+00, -6.7604e+00,  6.2405e+00,\n",
       "          4.3827e-01,  7.5383e+00, -7.7352e+00,  9.5886e+00,  8.1712e+00,\n",
       "          2.9725e-01, -5.4415e+00],\n",
       "        [-4.6091e+00, -4.6196e+00,  6.5675e+00, -1.1018e+01,  5.5057e+00,\n",
       "         -4.4250e+00,  1.3357e+00, -9.2686e+00,  2.3465e+01,  1.5244e+01,\n",
       "          1.1660e+01, -5.0257e+00,  1.0437e+00, -7.6142e+00, -7.3794e+00,\n",
       "         -4.0296e+00, -4.9024e+00, -2.4619e+00, -4.6083e+00,  5.4320e+00,\n",
       "         -2.1893e-01,  8.8972e+00, -7.7667e+00,  9.1101e+00,  8.3126e+00,\n",
       "          1.4000e+00, -5.6581e+00],\n",
       "        [-3.1981e+00, -3.4190e+00, -2.2799e+00, -5.0352e+00,  3.8526e+00,\n",
       "         -1.1023e+00,  1.1456e+00,  1.4080e+00,  2.1121e+01,  2.2015e+01,\n",
       "          4.3672e+00, -2.0184e-02,  7.2424e+00, -1.1882e+01, -6.0408e+00,\n",
       "         -5.9734e+00, -3.7353e+00, -9.4037e+00, -4.4791e+00,  3.3777e+00,\n",
       "          1.9677e+00, -1.9175e+00, -1.0180e+01,  7.6955e+00,  6.1007e-01,\n",
       "         -1.9090e+00,  1.5032e+00],\n",
       "        [-8.0642e+00, -2.4823e+00,  2.4351e+00,  5.8225e+00,  8.3459e+00,\n",
       "          4.8386e+00, -4.9716e+00,  6.7018e+00, -2.2729e+00, -4.7073e+00,\n",
       "         -2.7224e+00,  1.4839e+00,  8.1133e+00, -6.1073e+00,  2.1873e-01,\n",
       "         -2.0520e+00,  2.0181e-01,  1.7075e+00,  1.3229e+00, -1.1159e+01,\n",
       "          7.5376e-01, -1.0150e+00, -4.9102e-01, -4.6513e+00, -1.1201e+00,\n",
       "          4.0167e+00, -2.2210e+00],\n",
       "        [ 2.0587e+00, -4.8194e+00,  1.0443e+00, -1.2790e+00,  2.4266e+00,\n",
       "          4.6407e+00,  4.0936e+00,  8.1513e+00, -9.5443e+00,  2.7495e+00,\n",
       "         -7.2311e-01,  4.0651e+00,  2.2573e+00,  2.6965e+00, -7.0818e-01,\n",
       "          1.8041e+00, -3.2811e+00, -6.6167e+00,  2.7853e+00,  1.4478e+00,\n",
       "         -1.5520e+00, -2.7896e-01,  5.3969e+00, -3.9300e-01, -5.9013e+00,\n",
       "         -4.0841e-01,  4.5112e-01],\n",
       "        [ 1.9459e+00, -1.1481e-01, -2.9630e-01,  7.9823e+00,  5.4162e+00,\n",
       "          6.9883e+00, -1.8322e+00,  1.1685e+01, -1.3658e+01, -3.0046e+00,\n",
       "         -4.7777e+00,  4.8955e+00,  1.9922e+00, -7.6432e-01,  2.1482e+00,\n",
       "         -4.5740e-02,  2.2182e+00, -9.6119e-01,  4.8869e+00, -8.0594e+00,\n",
       "          5.8169e-01, -1.2471e+01,  1.3651e+00, -9.4581e+00, -1.0618e+01,\n",
       "          1.4859e+00,  1.4487e+00],\n",
       "        [ 2.6145e+00, -9.9791e-03,  6.4867e-01,  9.0261e+00,  5.4611e-01,\n",
       "          5.3676e+00, -4.3047e+00,  9.8794e+00, -2.2176e+01, -8.5351e+00,\n",
       "         -9.5689e+00,  6.4987e+00,  5.3966e+00,  2.3615e+00,  8.8754e+00,\n",
       "         -8.6655e-01,  3.0279e+00, -6.7695e-01,  3.8545e+00, -6.6556e+00,\n",
       "          4.6594e-01, -6.4630e+00,  6.9241e+00, -9.2376e+00, -7.4462e+00,\n",
       "          1.1271e+00, -1.1736e+00],\n",
       "        [ 6.0592e+00,  2.4937e-01, -9.0242e-01,  1.0741e+01, -2.6359e+00,\n",
       "          5.4625e+00, -3.3891e+00,  1.1041e+01, -2.6267e+01, -1.2457e+01,\n",
       "         -1.2225e+01,  6.1150e+00,  2.7081e+00,  8.8986e+00,  7.1109e+00,\n",
       "          2.6302e+00,  5.7264e+00,  2.3005e-01,  5.5597e+00, -3.9391e+00,\n",
       "         -1.3641e+00, -7.8313e+00,  1.1265e+01, -9.8674e+00, -8.5785e+00,\n",
       "          1.1882e+00, -3.8127e-01],\n",
       "        [-4.4828e+00, -4.7235e+00,  6.0769e+00, -1.0728e+01,  5.2908e+00,\n",
       "         -3.8109e+00,  1.4558e+00, -8.6665e+00,  2.4420e+01,  1.6117e+01,\n",
       "          1.1084e+01, -4.2240e+00,  1.8020e+00, -7.7547e+00, -7.4275e+00,\n",
       "         -3.9043e+00, -4.0252e+00, -3.4028e+00, -5.7785e+00,  5.9162e+00,\n",
       "          2.0683e-01,  7.6149e+00, -7.9877e+00,  9.3866e+00,  8.0706e+00,\n",
       "          7.3596e-01, -5.3341e+00],\n",
       "        [-4.8836e+00, -5.1066e+00,  6.5357e+00, -1.1064e+01,  4.9920e+00,\n",
       "         -4.1494e+00,  2.1311e+00, -9.2496e+00,  2.3594e+01,  1.5744e+01,\n",
       "          1.1707e+01, -4.4261e+00,  1.3060e+00, -7.5204e+00, -7.3745e+00,\n",
       "         -3.5474e+00, -4.8195e+00, -2.8365e+00, -4.6411e+00,  5.9997e+00,\n",
       "         -3.9193e-02,  8.3997e+00, -7.8406e+00,  9.1699e+00,  8.2108e+00,\n",
       "          1.3506e+00, -5.2733e+00],\n",
       "        [-3.6634e+00, -6.8025e+00, -4.9110e-01, -5.7189e+00,  2.6625e+00,\n",
       "         -1.4274e+00,  2.5174e+00, -6.2383e-02,  1.6642e+01,  2.1349e+01,\n",
       "          6.0685e+00,  1.2440e+00,  7.7118e+00, -1.0335e+01, -4.2449e+00,\n",
       "         -4.1416e+00, -2.4605e+00, -6.8253e+00, -2.5055e+00,  2.4240e+00,\n",
       "          2.2373e+00, -3.5743e+00, -1.1852e+01,  7.0366e+00, -3.6124e-01,\n",
       "          2.3112e-01,  7.9130e-01],\n",
       "        [-4.4291e+00, -2.8904e+00,  8.3148e+00,  2.6425e-01,  6.2331e+00,\n",
       "          3.7153e+00,  4.7313e-01,  1.3438e+00,  1.5326e+01,  7.2599e+00,\n",
       "          3.5392e+00,  3.7324e+00,  7.7565e+00, -6.7326e+00, -3.5450e+00,\n",
       "         -1.7401e+00, -3.8041e-01, -2.8220e+00, -5.4665e+00,  4.4288e-01,\n",
       "          3.0095e+00,  2.2892e+00, -1.4334e+00,  1.9191e+00,  5.8271e+00,\n",
       "         -5.7558e-01, -5.4142e+00],\n",
       "        [ 3.4909e-01, -6.1911e+00, -2.2919e+00,  2.8947e+00,  5.6098e+00,\n",
       "          5.9615e+00,  6.3034e-02,  8.5958e+00, -2.1593e+01, -1.4031e+00,\n",
       "         -2.2573e-01,  1.0926e+00,  2.5160e+00,  6.4425e+00,  4.4323e+00,\n",
       "          1.2573e+00,  1.9890e+00,  1.7378e+00,  1.2070e+01, -6.7536e+00,\n",
       "          1.0846e+00, -9.3001e+00, -2.0712e+00, -9.5406e+00, -1.3666e+01,\n",
       "          7.6332e+00,  4.2180e+00],\n",
       "        [-1.2022e+00, -6.2666e+00,  5.4202e+00, -3.4674e+00, -3.5762e+00,\n",
       "          5.4939e+00,  7.7828e+00,  2.3827e+00,  3.0379e+00,  1.6840e+01,\n",
       "          1.0381e+00,  1.2135e+01,  1.1239e+01, -3.5353e+00,  6.2321e-01,\n",
       "         -9.3405e-01, -3.5841e+00, -1.0130e+01, -1.3844e-02,  9.4823e+00,\n",
       "          4.2600e+00, -1.0487e+00,  1.4644e+00,  1.2649e+00,  1.9073e+00,\n",
       "         -3.2635e+00,  2.4238e-01],\n",
       "        [ 4.4314e+00,  1.1164e+00,  4.4464e+00,  8.2723e+00,  6.5140e+00,\n",
       "          6.3507e+00, -1.0511e+00,  1.2774e+01, -5.8127e+00, -5.6888e-01,\n",
       "         -2.8605e+00,  4.6059e+00,  3.7191e+00,  3.1251e-01, -1.2878e+00,\n",
       "          2.6737e+00,  5.9866e+00, -1.3935e+00,  2.4369e+00, -5.5616e+00,\n",
       "          4.3240e-01, -1.0256e+01,  9.1544e-01, -1.1417e+01, -8.7532e+00,\n",
       "         -3.9199e+00, -4.2425e+00]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:46.016232Z",
     "start_time": "2025-08-26T14:03:46.004016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "counts = logits.exp()\n",
    "counts"
   ],
   "id": "46cf1d9989e43dbb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1302e-02, 8.8838e-03, 4.3568e+02, 2.1915e-05, 1.9849e+02, 2.2129e-02,\n",
       "         4.2877e+00, 1.7227e-04, 4.0335e+10, 9.9924e+06, 6.5115e+04, 1.4640e-02,\n",
       "         6.0615e+00, 4.2870e-04, 5.9465e-04, 2.0155e-02, 1.7860e-02, 3.3279e-02,\n",
       "         3.0934e-03, 3.7099e+02, 1.2298e+00, 2.0283e+03, 3.3960e-04, 1.1927e+04,\n",
       "         3.1992e+03, 2.0875e+00, 4.8241e-03],\n",
       "        [1.2968e-02, 9.7874e-03, 2.8631e+02, 3.6434e-05, 2.2122e+02, 3.1830e-02,\n",
       "         3.5698e+00, 3.8271e-04, 3.6343e+10, 1.0680e+07, 3.4783e+04, 1.9514e-02,\n",
       "         9.4891e+00, 3.6491e-04, 7.0161e-04, 2.0702e-02, 2.4338e-02, 2.3812e-02,\n",
       "         2.4023e-03, 3.0376e+02, 1.4659e+00, 8.7778e+02, 3.1225e-04, 1.2116e+04,\n",
       "         1.9594e+03, 1.7158e+00, 6.3712e-03],\n",
       "        [1.5012e-03, 3.5147e-04, 1.4287e+01, 1.0601e-03, 4.1633e+02, 1.9419e-01,\n",
       "         9.6947e+00, 1.2093e-01, 5.9453e+04, 2.2376e+05, 5.6507e+02, 3.2218e-01,\n",
       "         5.2519e+01, 9.1797e-04, 6.5995e-02, 8.9614e-02, 3.0705e-03, 1.6571e-01,\n",
       "         6.9131e+00, 4.5327e-01, 3.2817e-01, 3.9303e-01, 5.1397e-05, 1.7080e+02,\n",
       "         8.0339e-01, 9.3712e+01, 6.7042e-01],\n",
       "        [4.6878e-02, 1.0649e-05, 3.2086e+00, 5.6668e-02, 3.9751e-01, 1.9588e+02,\n",
       "         1.2455e+03, 2.0540e+01, 2.0924e-04, 1.8228e+05, 1.0183e+01, 4.3011e+03,\n",
       "         4.2375e+03, 4.2251e-01, 1.5354e+02, 9.0827e-01, 3.9707e-02, 6.5767e-02,\n",
       "         4.3267e+02, 5.5594e-01, 1.2957e+01, 2.2839e-04, 2.3272e-03, 8.4025e-02,\n",
       "         1.0515e-03, 1.9483e+02, 1.4999e+02],\n",
       "        [1.7472e+01, 7.7116e-02, 1.5680e+01, 2.5301e+03, 7.0511e-01, 2.7869e+04,\n",
       "         5.7122e+00, 4.7166e+04, 4.1181e-06, 8.9317e+00, 8.7927e-04, 2.0852e+04,\n",
       "         2.9987e+02, 1.1645e+01, 1.0787e+01, 2.7809e+01, 2.5359e+01, 5.2630e-03,\n",
       "         7.6004e+00, 2.0336e-01, 1.8023e+00, 7.4699e-05, 2.4140e+01, 2.1434e-04,\n",
       "         9.0277e-03, 2.2251e-01, 1.1849e+00],\n",
       "        [1.1302e-02, 8.8838e-03, 4.3568e+02, 2.1915e-05, 1.9849e+02, 2.2129e-02,\n",
       "         4.2877e+00, 1.7227e-04, 4.0335e+10, 9.9924e+06, 6.5115e+04, 1.4640e-02,\n",
       "         6.0615e+00, 4.2870e-04, 5.9465e-04, 2.0155e-02, 1.7860e-02, 3.3279e-02,\n",
       "         3.0934e-03, 3.7099e+02, 1.2298e+00, 2.0283e+03, 3.3960e-04, 1.1927e+04,\n",
       "         3.1992e+03, 2.0875e+00, 4.8241e-03],\n",
       "        [7.3325e-03, 9.5569e-03, 3.5503e-01, 1.8262e-02, 2.0314e+02, 1.3386e+00,\n",
       "         7.7351e-01, 5.5766e+00, 2.1724e+06, 7.6662e+06, 3.0056e+01, 4.6862e-01,\n",
       "         9.9467e+02, 2.4675e-05, 9.6040e-03, 9.5810e-03, 3.7187e-02, 4.0575e-03,\n",
       "         8.4674e-02, 3.9694e-01, 5.0927e+00, 5.1789e-02, 6.2374e-05, 5.2762e+02,\n",
       "         1.2017e+00, 2.9621e+00, 2.2657e+00],\n",
       "        [7.6188e-01, 1.4863e+00, 6.7842e-01, 1.9619e+03, 2.8351e+01, 4.1971e+02,\n",
       "         1.4402e-02, 2.9320e+04, 5.5858e-07, 8.2360e-03, 2.0474e-03, 3.9984e+02,\n",
       "         5.4818e+02, 2.2190e-02, 4.4732e+02, 3.3264e-02, 2.2301e+00, 3.2487e-01,\n",
       "         2.2012e+01, 5.6197e-05, 2.6451e+00, 3.6639e-04, 8.5018e+00, 7.4623e-04,\n",
       "         1.4156e-03, 1.0330e+01, 1.2828e+00],\n",
       "        [3.2357e+00, 6.2259e-02, 5.0620e+00, 2.9810e+02, 1.7818e+01, 7.1338e+02,\n",
       "         1.1202e+00, 3.0682e+04, 1.4352e-07, 2.5440e-04, 1.4387e-03, 3.0768e+02,\n",
       "         1.6839e+02, 2.5250e+02, 2.8157e+01, 3.4488e+01, 9.0032e+01, 3.7406e-02,\n",
       "         5.3902e+01, 1.1957e-01, 5.8995e-01, 1.2300e+00, 4.0039e+03, 2.8202e-04,\n",
       "         1.3849e-03, 1.4775e-01, 3.6062e-02],\n",
       "        [9.6653e+01, 1.1979e+00, 4.0605e-01, 7.7333e+03, 1.2976e+01, 2.7607e+02,\n",
       "         1.8024e-01, 1.2299e+05, 2.9157e-11, 4.0324e-04, 2.3450e-03, 2.6961e+01,\n",
       "         4.2778e+00, 7.0815e+02, 1.0176e+02, 2.4301e+00, 1.7092e+01, 1.5681e+00,\n",
       "         1.8103e+04, 7.8713e-03, 1.8891e+00, 4.6153e-05, 1.3356e+02, 2.0005e-05,\n",
       "         3.6110e-06, 3.1866e+01, 3.9784e+01],\n",
       "        [1.1823e-01, 4.1551e-03, 6.9753e+01, 8.8029e-01, 4.5739e-02, 3.9493e+03,\n",
       "         1.4010e+02, 1.4704e+02, 7.7088e-02, 1.4151e+04, 3.4371e-01, 3.5811e+05,\n",
       "         8.8255e+04, 8.3081e-02, 5.2794e+00, 2.7918e-01, 2.1416e-02, 8.4214e-05,\n",
       "         4.2127e+00, 1.5566e+02, 2.9144e+01, 1.9523e+00, 2.5449e+02, 4.4083e-01,\n",
       "         3.2282e+01, 5.6372e-02, 1.1169e+00],\n",
       "        [2.7406e+02, 5.9637e+00, 1.6147e+01, 5.3552e+03, 3.6897e+02, 2.2034e+02,\n",
       "         1.1579e-01, 8.5301e+05, 5.6691e-06, 3.1444e-02, 2.1104e-03, 1.0880e+02,\n",
       "         2.4195e+00, 1.4313e+01, 6.7034e-01, 1.6943e+01, 2.4881e+02, 1.4609e-01,\n",
       "         1.2584e+02, 5.9686e-03, 8.4268e-01, 2.7442e-05, 6.4069e+01, 6.6752e-06,\n",
       "         2.3600e-05, 2.3195e-01, 6.7716e-02],\n",
       "        [1.1302e-02, 8.8838e-03, 4.3568e+02, 2.1915e-05, 1.9849e+02, 2.2129e-02,\n",
       "         4.2877e+00, 1.7227e-04, 4.0335e+10, 9.9924e+06, 6.5115e+04, 1.4640e-02,\n",
       "         6.0615e+00, 4.2870e-04, 5.9465e-04, 2.0155e-02, 1.7860e-02, 3.3279e-02,\n",
       "         3.0934e-03, 3.7099e+02, 1.2298e+00, 2.0283e+03, 3.3960e-04, 1.1927e+04,\n",
       "         3.1992e+03, 2.0875e+00, 4.8241e-03],\n",
       "        [1.6495e-02, 5.6185e-01, 2.1912e-01, 1.9658e-01, 3.2534e+02, 7.2450e+00,\n",
       "         2.2184e-01, 6.1874e+01, 1.0262e+08, 6.9675e+06, 5.6158e+00, 6.2338e-01,\n",
       "         2.0042e+03, 4.6651e-06, 1.4320e-03, 3.8482e-03, 8.0303e-02, 6.6978e-04,\n",
       "         3.6385e-03, 5.9005e-01, 1.5422e+01, 1.2763e-01, 9.1109e-04, 9.2702e+02,\n",
       "         4.6387e+00, 1.0211e-01, 2.0365e+00],\n",
       "        [4.9829e-01, 3.9028e-01, 1.1304e-01, 1.6327e+04, 3.2569e+01, 1.6387e+02,\n",
       "         3.4803e-03, 1.7753e+03, 1.3003e-09, 4.4619e-05, 1.5296e-03, 7.1287e+00,\n",
       "         2.6669e+02, 3.1346e-01, 1.1066e+03, 1.5909e-01, 1.5259e+00, 7.9152e+02,\n",
       "         4.8057e+02, 1.9502e-06, 1.0644e+01, 8.0344e-04, 2.4182e+00, 4.4015e-04,\n",
       "         8.3762e-05, 1.5870e+03, 4.3402e+00],\n",
       "        [5.2867e+01, 1.9782e+00, 1.9456e-01, 3.7852e+03, 8.1271e-03, 9.2638e+02,\n",
       "         3.5777e-01, 2.2959e+04, 1.1717e-11, 5.1846e-05, 4.2932e-05, 1.3371e+03,\n",
       "         2.1042e+01, 6.6598e+02, 4.0666e+02, 4.7422e+00, 2.4609e+01, 1.4272e-01,\n",
       "         3.5748e+01, 3.9040e-02, 2.6676e-01, 3.5105e-04, 9.4522e+04, 2.3578e-03,\n",
       "         1.2279e-03, 3.4186e+00, 8.4444e+00],\n",
       "        [1.1302e-02, 8.8838e-03, 4.3568e+02, 2.1915e-05, 1.9849e+02, 2.2129e-02,\n",
       "         4.2877e+00, 1.7227e-04, 4.0335e+10, 9.9924e+06, 6.5115e+04, 1.4640e-02,\n",
       "         6.0615e+00, 4.2870e-04, 5.9465e-04, 2.0155e-02, 1.7860e-02, 3.3279e-02,\n",
       "         3.0934e-03, 3.7099e+02, 1.2298e+00, 2.0283e+03, 3.3960e-04, 1.1927e+04,\n",
       "         3.1992e+03, 2.0875e+00, 4.8241e-03],\n",
       "        [1.5305e-02, 1.4635e-02, 4.2726e+02, 2.5814e-05, 2.3647e+02, 2.2747e-02,\n",
       "         2.9327e+00, 1.8874e-04, 9.6475e+10, 1.2419e+07, 5.7284e+04, 1.3533e-02,\n",
       "         7.0141e+00, 4.1898e-04, 4.9649e-04, 1.9086e-02, 2.6969e-02, 2.4536e-02,\n",
       "         1.1588e-03, 5.1313e+02, 1.5500e+00, 1.8787e+03, 4.3716e-04, 1.4598e+04,\n",
       "         3.5374e+03, 1.3461e+00, 4.3332e-03],\n",
       "        [9.9610e-03, 9.8570e-03, 7.1161e+02, 1.6408e-05, 2.4609e+02, 1.1974e-02,\n",
       "         3.8025e+00, 9.4338e-05, 1.5508e+10, 4.1709e+06, 1.1589e+05, 6.5673e-03,\n",
       "         2.8398e+00, 4.9338e-04, 6.2398e-04, 1.7782e-02, 7.4290e-03, 8.5273e-02,\n",
       "         9.9689e-03, 2.2861e+02, 8.0337e-01, 7.3112e+03, 4.2361e-04, 9.0462e+03,\n",
       "         4.0750e+03, 4.0550e+00, 3.4892e-03],\n",
       "        [4.0842e-02, 3.2746e-02, 1.0229e-01, 6.5050e-03, 4.7114e+01, 3.3209e-01,\n",
       "         3.1443e+00, 4.0876e+00, 1.4886e+09, 3.6402e+09, 7.8825e+01, 9.8002e-01,\n",
       "         1.3975e+03, 6.9129e-06, 2.3797e-03, 2.5456e-03, 2.3867e-02, 8.2423e-05,\n",
       "         1.1343e-02, 2.9303e+01, 7.1539e+00, 1.4698e-01, 3.7927e-05, 2.1985e+03,\n",
       "         1.8406e+00, 1.4822e-01, 4.4962e+00],\n",
       "        [3.1460e-04, 8.3549e-02, 1.1417e+01, 3.3782e+02, 4.2127e+03, 1.2629e+02,\n",
       "         6.9317e-03, 8.1383e+02, 1.0301e-01, 9.0288e-03, 6.5715e-02, 4.4103e+00,\n",
       "         3.3386e+03, 2.2267e-03, 1.2445e+00, 1.2848e-01, 1.2236e+00, 5.5150e+00,\n",
       "         3.7544e+00, 1.4242e-05, 2.1250e+00, 3.6240e-01, 6.1200e-01, 9.5490e-03,\n",
       "         3.2626e-01, 5.5519e+01, 1.0850e-01],\n",
       "        [7.8356e+00, 8.0720e-03, 2.8414e+00, 2.7833e-01, 1.1320e+01, 1.0362e+02,\n",
       "         5.9954e+01, 3.4679e+03, 7.1608e-05, 1.5636e+01, 4.8524e-01, 5.8271e+01,\n",
       "         9.5577e+00, 1.4828e+01, 4.9254e-01, 6.0744e+00, 3.7586e-02, 1.3379e-03,\n",
       "         1.6204e+01, 4.2538e+00, 2.1182e-01, 7.5657e-01, 2.2071e+02, 6.7503e-01,\n",
       "         2.7360e-03, 6.6471e-01, 1.5701e+00],\n",
       "        [6.9998e+00, 8.9154e-01, 7.4357e-01, 2.9286e+03, 2.2502e+02, 1.0839e+03,\n",
       "         1.6006e-01, 1.1880e+05, 1.1703e-06, 4.9557e-02, 8.4155e-03, 1.3369e+02,\n",
       "         7.3315e+00, 4.6565e-01, 8.5694e+00, 9.5529e-01, 9.1907e+00, 3.8244e-01,\n",
       "         1.3255e+02, 3.1611e-04, 1.7891e+00, 3.8370e-06, 3.9162e+00, 7.8055e-05,\n",
       "         2.4479e-05, 4.4188e+00, 4.2576e+00],\n",
       "        [1.3660e+01, 9.9007e-01, 1.9130e+00, 8.3173e+03, 1.7265e+00, 2.1434e+02,\n",
       "         1.3505e-02, 1.9524e+04, 2.3397e-10, 1.9644e-04, 6.9867e-05, 6.6426e+02,\n",
       "         2.2065e+02, 1.0607e+01, 7.1535e+03, 4.2040e-01, 2.0654e+01, 5.0817e-01,\n",
       "         4.7205e+01, 1.2868e-03, 1.5935e+00, 1.5601e-03, 1.0165e+03, 9.7311e-05,\n",
       "         5.8363e-04, 3.0867e+00, 3.0926e-01],\n",
       "        [4.2805e+02, 1.2832e+00, 4.0558e-01, 4.6197e+04, 7.1653e-02, 2.3568e+02,\n",
       "         3.3739e-02, 6.2367e+04, 3.9106e-12, 3.8891e-06, 4.9075e-06, 4.5259e+02,\n",
       "         1.5000e+01, 7.3218e+03, 1.2252e+03, 1.3877e+01, 3.0688e+02, 1.2587e+00,\n",
       "         2.5974e+02, 1.9465e-02, 2.5562e-01, 3.9712e-04, 7.8009e+04, 5.1838e-05,\n",
       "         1.8812e-04, 3.2812e+00, 6.8299e-01],\n",
       "        [1.1302e-02, 8.8838e-03, 4.3568e+02, 2.1915e-05, 1.9849e+02, 2.2129e-02,\n",
       "         4.2877e+00, 1.7227e-04, 4.0335e+10, 9.9924e+06, 6.5115e+04, 1.4640e-02,\n",
       "         6.0615e+00, 4.2870e-04, 5.9465e-04, 2.0155e-02, 1.7860e-02, 3.3279e-02,\n",
       "         3.0934e-03, 3.7099e+02, 1.2298e+00, 2.0283e+03, 3.3960e-04, 1.1927e+04,\n",
       "         3.1992e+03, 2.0875e+00, 4.8241e-03],\n",
       "        [7.5699e-03, 6.0566e-03, 6.8929e+02, 1.5674e-05, 1.4724e+02, 1.5773e-02,\n",
       "         8.4241e+00, 9.6152e-05, 1.7641e+10, 6.8819e+06, 1.2139e+05, 1.1962e-02,\n",
       "         3.6914e+00, 5.4193e-04, 6.2702e-04, 2.8799e-02, 8.0709e-03, 5.8629e-02,\n",
       "         9.6470e-03, 4.0330e+02, 9.6157e-01, 4.4455e+03, 3.9343e-04, 9.6040e+03,\n",
       "         3.6807e+03, 3.8597e+00, 5.1266e-03],\n",
       "        [2.5644e-02, 1.1110e-03, 6.1195e-01, 3.2832e-03, 1.4332e+01, 2.3993e-01,\n",
       "         1.2396e+01, 9.3952e-01, 1.6888e+07, 1.8703e+09, 4.3201e+02, 3.4696e+00,\n",
       "         2.2346e+03, 3.2489e-05, 1.4336e-02, 1.5898e-02, 8.5396e-02, 1.0859e-03,\n",
       "         8.1631e-02, 1.1291e+01, 9.3681e+00, 2.8034e-02, 7.1277e-06, 1.1375e+03,\n",
       "         6.9681e-01, 1.2600e+00, 2.2063e+00],\n",
       "        [1.1925e-02, 5.5555e-02, 4.0838e+03, 1.3024e+00, 5.0934e+02, 4.1069e+01,\n",
       "         1.6050e+00, 3.8335e+00, 4.5311e+06, 1.4221e+03, 3.4440e+01, 4.1780e+01,\n",
       "         2.3367e+03, 1.1915e-03, 2.8870e-02, 1.7551e-01, 6.8358e-01, 5.9488e-02,\n",
       "         4.2258e-03, 1.5572e+00, 2.0277e+01, 9.8675e+00, 2.3851e-01, 6.8149e+00,\n",
       "         3.3937e+02, 5.6238e-01, 4.4529e-03],\n",
       "        [1.4178e+00, 2.0476e-03, 1.0107e-01, 1.8078e+01, 2.7310e+02, 3.8820e+02,\n",
       "         1.0651e+00, 5.4090e+03, 4.1895e-10, 2.4583e-01, 7.9793e-01, 2.9819e+00,\n",
       "         1.2379e+01, 6.2795e+02, 8.4124e+01, 3.5159e+00, 7.3080e+00, 5.6850e+00,\n",
       "         1.7456e+05, 1.1667e-03, 2.9582e+00, 9.1411e-05, 1.2603e-01, 7.1874e-05,\n",
       "         1.1612e-06, 2.0657e+03, 6.7896e+01],\n",
       "        [3.0052e-01, 1.8987e-03, 2.2593e+02, 3.1197e-02, 2.7981e-02, 2.4320e+02,\n",
       "         2.3991e+03, 1.0834e+01, 2.0860e+01, 2.0590e+07, 2.8240e+00, 1.8631e+05,\n",
       "         7.6047e+04, 2.9150e-02, 1.8649e+00, 3.9296e-01, 2.7763e-02, 3.9858e-05,\n",
       "         9.8625e-01, 1.3126e+04, 7.0809e+01, 3.5041e-01, 4.3250e+00, 3.5427e+00,\n",
       "         6.7347e+00, 3.8254e-02, 1.2743e+00],\n",
       "        [8.4052e+01, 3.0539e+00, 8.5316e+01, 3.9140e+03, 6.7451e+02, 5.7290e+02,\n",
       "         3.4954e-01, 3.5275e+05, 2.9893e-03, 5.6616e-01, 5.7240e-02, 1.0007e+02,\n",
       "         4.1228e+01, 1.3669e+00, 2.7588e-01, 1.4493e+01, 3.9806e+02, 2.4820e-01,\n",
       "         1.1438e+01, 3.8428e-03, 1.5409e+00, 3.5145e-05, 2.4979e+00, 1.1011e-05,\n",
       "         1.5795e-04, 1.9844e-02, 1.4372e-02]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:46.477240Z",
     "start_time": "2025-08-26T14:03:46.463538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "probs  = counts/counts.sum(1, keepdims = True )\n",
    "probs"
   ],
   "id": "a52434184c6df994",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.8013e-13, 2.2019e-13, 1.0799e-08, 5.4319e-16, 4.9198e-09, 5.4850e-13,\n",
       "         1.0628e-10, 4.2699e-15, 9.9975e-01, 2.4767e-04, 1.6139e-06, 3.6287e-13,\n",
       "         1.5024e-10, 1.0626e-14, 1.4739e-14, 4.9955e-13, 4.4269e-13, 8.2485e-13,\n",
       "         7.6672e-14, 9.1954e-09, 3.0481e-11, 5.0273e-08, 8.4174e-15, 2.9563e-07,\n",
       "         7.9295e-08, 5.1741e-11, 1.1957e-13],\n",
       "        [3.5670e-13, 2.6923e-13, 7.8756e-09, 1.0022e-15, 6.0851e-09, 8.7556e-13,\n",
       "         9.8195e-11, 1.0527e-14, 9.9970e-01, 2.9377e-04, 9.5677e-07, 5.3678e-13,\n",
       "         2.6102e-10, 1.0038e-14, 1.9299e-14, 5.6945e-13, 6.6946e-13, 6.5500e-13,\n",
       "         6.6082e-14, 8.3556e-09, 4.0324e-11, 2.4145e-08, 8.5890e-15, 3.3329e-07,\n",
       "         5.3897e-08, 4.7196e-11, 1.7525e-13],\n",
       "        [5.2757e-09, 1.2352e-09, 5.0210e-05, 3.7257e-09, 1.4631e-03, 6.8246e-07,\n",
       "         3.4071e-05, 4.2500e-07, 2.0894e-01, 7.8637e-01, 1.9859e-03, 1.1323e-06,\n",
       "         1.8457e-04, 3.2261e-09, 2.3193e-07, 3.1494e-07, 1.0791e-08, 5.8239e-07,\n",
       "         2.4295e-05, 1.5930e-06, 1.1533e-06, 1.3813e-06, 1.8063e-10, 6.0027e-04,\n",
       "         2.8234e-06, 3.2934e-04, 2.3561e-06],\n",
       "        [2.4258e-07, 5.5106e-11, 1.6604e-05, 2.9324e-07, 2.0570e-06, 1.0136e-03,\n",
       "         6.4450e-03, 1.0629e-04, 1.0828e-09, 9.4328e-01, 5.2695e-05, 2.2258e-02,\n",
       "         2.1928e-02, 2.1864e-06, 7.9454e-04, 4.7001e-06, 2.0548e-07, 3.4033e-07,\n",
       "         2.2390e-03, 2.8769e-06, 6.7048e-05, 1.1819e-09, 1.2043e-08, 4.3482e-07,\n",
       "         5.4414e-09, 1.0082e-03, 7.7616e-04],\n",
       "        [1.7671e-04, 7.7992e-07, 1.5858e-04, 2.5589e-02, 7.1313e-06, 2.8185e-01,\n",
       "         5.7771e-05, 4.7702e-01, 4.1649e-11, 9.0332e-05, 8.8926e-09, 2.1089e-01,\n",
       "         3.0328e-03, 1.1777e-04, 1.0910e-04, 2.8125e-04, 2.5647e-04, 5.3228e-08,\n",
       "         7.6868e-05, 2.0567e-06, 1.8228e-05, 7.5548e-10, 2.4415e-04, 2.1678e-09,\n",
       "         9.1303e-08, 2.2504e-06, 1.1984e-05],\n",
       "        [2.8013e-13, 2.2019e-13, 1.0799e-08, 5.4319e-16, 4.9198e-09, 5.4850e-13,\n",
       "         1.0628e-10, 4.2699e-15, 9.9975e-01, 2.4767e-04, 1.6139e-06, 3.6287e-13,\n",
       "         1.5024e-10, 1.0626e-14, 1.4739e-14, 4.9955e-13, 4.4269e-13, 8.2485e-13,\n",
       "         7.6672e-14, 9.1954e-09, 3.0481e-11, 5.0273e-08, 8.4174e-15, 2.9563e-07,\n",
       "         7.9295e-08, 5.1741e-11, 1.1957e-13],\n",
       "        [7.4514e-10, 9.7120e-10, 3.6079e-08, 1.8558e-09, 2.0643e-05, 1.3603e-07,\n",
       "         7.8606e-08, 5.6670e-07, 2.2076e-01, 7.7906e-01, 3.0544e-06, 4.7623e-08,\n",
       "         1.0108e-04, 2.5075e-12, 9.7598e-10, 9.7364e-10, 3.7790e-09, 4.1233e-10,\n",
       "         8.6048e-09, 4.0338e-08, 5.1753e-07, 5.2629e-09, 6.3386e-12, 5.3618e-05,\n",
       "         1.2212e-07, 3.0102e-07, 2.3025e-07],\n",
       "        [2.2965e-05, 4.4801e-05, 2.0450e-05, 5.9137e-02, 8.5458e-04, 1.2651e-02,\n",
       "         4.3412e-07, 8.8378e-01, 1.6837e-11, 2.4826e-07, 6.1714e-08, 1.2052e-02,\n",
       "         1.6524e-02, 6.6887e-07, 1.3483e-02, 1.0027e-06, 6.7221e-05, 9.7926e-06,\n",
       "         6.6351e-04, 1.6939e-09, 7.9732e-05, 1.1044e-08, 2.5627e-04, 2.2494e-08,\n",
       "         4.2670e-08, 3.1137e-04, 3.8667e-05],\n",
       "        [8.8258e-05, 1.6982e-06, 1.3807e-04, 8.1309e-03, 4.8600e-04, 1.9458e-02,\n",
       "         3.0553e-05, 8.3689e-01, 3.9146e-12, 6.9391e-09, 3.9243e-08, 8.3924e-03,\n",
       "         4.5931e-03, 6.8873e-03, 7.6801e-04, 9.4071e-04, 2.4557e-03, 1.0203e-06,\n",
       "         1.4702e-03, 3.2613e-06, 1.6092e-05, 3.3551e-05, 1.0921e-01, 7.6924e-09,\n",
       "         3.7774e-08, 4.0300e-06, 9.8362e-07],\n",
       "        [6.4314e-04, 7.9710e-06, 2.7019e-06, 5.1458e-02, 8.6345e-05, 1.8370e-03,\n",
       "         1.1993e-06, 8.1839e-01, 1.9402e-16, 2.6832e-09, 1.5604e-08, 1.7940e-04,\n",
       "         2.8465e-05, 4.7121e-03, 6.7714e-04, 1.6170e-05, 1.1373e-04, 1.0434e-05,\n",
       "         1.2046e-01, 5.2376e-08, 1.2570e-05, 3.0711e-10, 8.8873e-04, 1.3312e-10,\n",
       "         2.4028e-11, 2.1204e-04, 2.6472e-04],\n",
       "        [2.5409e-07, 8.9298e-09, 1.4991e-04, 1.8918e-06, 9.8298e-08, 8.4875e-03,\n",
       "         3.0109e-04, 3.1600e-04, 1.6567e-07, 3.0413e-02, 7.3867e-07, 7.6961e-01,\n",
       "         1.8967e-01, 1.7855e-07, 1.1346e-05, 6.0000e-07, 4.6026e-08, 1.8099e-10,\n",
       "         9.0535e-06, 3.3454e-04, 6.2635e-05, 4.1956e-06, 5.4694e-04, 9.4740e-07,\n",
       "         6.9378e-05, 1.2115e-07, 2.4004e-06],\n",
       "        [3.1874e-04, 6.9360e-06, 1.8779e-05, 6.2283e-03, 4.2912e-04, 2.5626e-04,\n",
       "         1.3467e-07, 9.9206e-01, 6.5933e-12, 3.6570e-08, 2.4545e-09, 1.2654e-04,\n",
       "         2.8139e-06, 1.6647e-05, 7.7962e-07, 1.9705e-05, 2.8937e-04, 1.6991e-07,\n",
       "         1.4635e-04, 6.9416e-09, 9.8005e-07, 3.1916e-11, 7.4514e-05, 7.7634e-12,\n",
       "         2.7447e-11, 2.6977e-07, 7.8755e-08],\n",
       "        [2.8013e-13, 2.2019e-13, 1.0799e-08, 5.4319e-16, 4.9198e-09, 5.4850e-13,\n",
       "         1.0628e-10, 4.2699e-15, 9.9975e-01, 2.4767e-04, 1.6139e-06, 3.6287e-13,\n",
       "         1.5024e-10, 1.0626e-14, 1.4739e-14, 4.9955e-13, 4.4269e-13, 8.2485e-13,\n",
       "         7.6672e-14, 9.1954e-09, 3.0481e-11, 5.0273e-08, 8.4174e-15, 2.9563e-07,\n",
       "         7.9295e-08, 5.1741e-11, 1.1957e-13],\n",
       "        [1.5051e-10, 5.1268e-09, 1.9995e-09, 1.7937e-09, 2.9687e-06, 6.6110e-08,\n",
       "         2.0243e-09, 5.6460e-07, 9.3639e-01, 6.3578e-02, 5.1244e-08, 5.6883e-09,\n",
       "         1.8288e-05, 4.2569e-14, 1.3067e-11, 3.5114e-11, 7.3276e-10, 6.1117e-12,\n",
       "         3.3201e-11, 5.3842e-09, 1.4073e-07, 1.1647e-09, 8.3137e-12, 8.4590e-06,\n",
       "         4.2328e-08, 9.3179e-10, 1.8583e-08],\n",
       "        [2.2089e-05, 1.7301e-05, 5.0108e-06, 7.2376e-01, 1.4438e-03, 7.2643e-03,\n",
       "         1.5428e-07, 7.8697e-02, 5.7640e-14, 1.9779e-09, 6.7806e-08, 3.1601e-04,\n",
       "         1.1822e-02, 1.3895e-05, 4.9056e-02, 7.0523e-06, 6.7643e-05, 3.5087e-02,\n",
       "         2.1303e-02, 8.6451e-11, 4.7185e-04, 3.5616e-08, 1.0720e-04, 1.9511e-08,\n",
       "         3.7131e-09, 7.0349e-02, 1.9240e-04],\n",
       "        [4.2377e-04, 1.5856e-05, 1.5595e-06, 3.0341e-02, 6.5144e-08, 7.4255e-03,\n",
       "         2.8678e-06, 1.8403e-01, 9.3919e-17, 4.1558e-10, 3.4413e-10, 1.0718e-02,\n",
       "         1.6866e-04, 5.3382e-03, 3.2596e-03, 3.8012e-05, 1.9726e-04, 1.1440e-06,\n",
       "         2.8655e-04, 3.1293e-07, 2.1383e-06, 2.8139e-09, 7.5765e-01, 1.8899e-08,\n",
       "         9.8425e-09, 2.7402e-05, 6.7687e-05],\n",
       "        [2.8013e-13, 2.2019e-13, 1.0799e-08, 5.4319e-16, 4.9198e-09, 5.4850e-13,\n",
       "         1.0628e-10, 4.2699e-15, 9.9975e-01, 2.4767e-04, 1.6139e-06, 3.6287e-13,\n",
       "         1.5024e-10, 1.0626e-14, 1.4739e-14, 4.9955e-13, 4.4269e-13, 8.2485e-13,\n",
       "         7.6672e-14, 9.1954e-09, 3.0481e-11, 5.0273e-08, 8.4174e-15, 2.9563e-07,\n",
       "         7.9295e-08, 5.1741e-11, 1.1957e-13],\n",
       "        [1.5863e-13, 1.5168e-13, 4.4281e-09, 2.6753e-16, 2.4508e-09, 2.3576e-13,\n",
       "         3.0395e-11, 1.9561e-15, 9.9987e-01, 1.2871e-04, 5.9370e-07, 1.4025e-13,\n",
       "         7.2695e-11, 4.3423e-15, 5.1456e-15, 1.9781e-13, 2.7951e-13, 2.5429e-13,\n",
       "         1.2010e-14, 5.3181e-09, 1.6065e-11, 1.9471e-08, 4.5308e-15, 1.5129e-07,\n",
       "         3.6662e-08, 1.3952e-11, 4.4909e-14],\n",
       "        [6.4214e-13, 6.3544e-13, 4.5875e-08, 1.0578e-15, 1.5865e-08, 7.7189e-13,\n",
       "         2.4513e-10, 6.0816e-15, 9.9972e-01, 2.6888e-04, 7.4710e-06, 4.2337e-13,\n",
       "         1.8307e-10, 3.1806e-14, 4.0225e-14, 1.1463e-12, 4.7892e-13, 5.4972e-12,\n",
       "         6.4266e-13, 1.4737e-08, 5.1790e-11, 4.7132e-07, 2.7308e-14, 5.8317e-07,\n",
       "         2.6270e-07, 2.6141e-10, 2.2493e-13],\n",
       "        [7.9632e-12, 6.3847e-12, 1.9945e-11, 1.2683e-12, 9.1861e-09, 6.4751e-11,\n",
       "         6.1307e-10, 7.9699e-10, 2.9023e-01, 7.0976e-01, 1.5369e-08, 1.9108e-10,\n",
       "         2.7247e-07, 1.3479e-15, 4.6400e-13, 4.9633e-13, 4.6535e-12, 1.6071e-14,\n",
       "         2.2117e-12, 5.7135e-09, 1.3948e-09, 2.8657e-11, 7.3949e-15, 4.2865e-07,\n",
       "         3.5887e-10, 2.8900e-11, 8.7665e-10],\n",
       "        [3.5284e-08, 9.3703e-06, 1.2804e-03, 3.7888e-02, 4.7247e-01, 1.4164e-02,\n",
       "         7.7742e-07, 9.1274e-02, 1.1553e-05, 1.0126e-06, 7.3702e-06, 4.9463e-04,\n",
       "         3.7444e-01, 2.4973e-07, 1.3957e-04, 1.4409e-05, 1.3723e-04, 6.1852e-04,\n",
       "         4.2107e-04, 1.5973e-09, 2.3832e-04, 4.0644e-05, 6.8638e-05, 1.0710e-06,\n",
       "         3.6591e-05, 6.2266e-03, 1.2169e-05],\n",
       "        [1.9569e-03, 2.0159e-06, 7.0961e-04, 6.9510e-05, 2.8271e-03, 2.5878e-02,\n",
       "         1.4973e-02, 8.6607e-01, 1.7883e-08, 3.9048e-03, 1.2118e-04, 1.4553e-02,\n",
       "         2.3869e-03, 3.7032e-03, 1.2301e-04, 1.5170e-03, 9.3867e-06, 3.3413e-07,\n",
       "         4.0469e-03, 1.0623e-03, 5.2901e-05, 1.8895e-04, 5.5121e-02, 1.6858e-04,\n",
       "         6.8328e-07, 1.6600e-04, 3.9211e-04],\n",
       "        [5.6747e-05, 7.2276e-06, 6.0280e-06, 2.3742e-02, 1.8242e-03, 8.7870e-03,\n",
       "         1.2976e-06, 9.6308e-01, 9.4875e-12, 4.0175e-07, 6.8224e-08, 1.0838e-03,\n",
       "         5.9436e-05, 3.7750e-06, 6.9471e-05, 7.7444e-06, 7.4508e-05, 3.1004e-06,\n",
       "         1.0745e-03, 2.5626e-09, 1.4504e-05, 3.1106e-11, 3.1748e-05, 6.3278e-10,\n",
       "         1.9845e-10, 3.5822e-05, 3.4516e-05],\n",
       "        [3.6708e-04, 2.6606e-05, 5.1407e-05, 2.2351e-01, 4.6396e-05, 5.7598e-03,\n",
       "         3.6291e-07, 5.2465e-01, 6.2874e-15, 5.2788e-09, 1.8775e-09, 1.7850e-02,\n",
       "         5.9295e-03, 2.8503e-04, 1.9223e-01, 1.1297e-05, 5.5503e-04, 1.3656e-05,\n",
       "         1.2685e-03, 3.4581e-08, 4.2821e-05, 4.1923e-08, 2.7316e-02, 2.6150e-09,\n",
       "         1.5683e-08, 8.2946e-05, 8.3106e-06],\n",
       "        [2.1746e-03, 6.5191e-06, 2.0605e-06, 2.3469e-01, 3.6402e-07, 1.1973e-03,\n",
       "         1.7140e-07, 3.1684e-01, 1.9867e-17, 1.9758e-11, 2.4931e-11, 2.2993e-03,\n",
       "         7.6206e-05, 3.7197e-02, 6.2246e-03, 7.0498e-05, 1.5590e-03, 6.3943e-06,\n",
       "         1.3195e-03, 9.8890e-08, 1.2986e-06, 2.0175e-09, 3.9631e-01, 2.6335e-10,\n",
       "         9.5568e-10, 1.6670e-05, 3.4698e-06],\n",
       "        [2.8013e-13, 2.2019e-13, 1.0799e-08, 5.4319e-16, 4.9198e-09, 5.4850e-13,\n",
       "         1.0628e-10, 4.2699e-15, 9.9975e-01, 2.4767e-04, 1.6139e-06, 3.6287e-13,\n",
       "         1.5024e-10, 1.0626e-14, 1.4739e-14, 4.9955e-13, 4.4269e-13, 8.2485e-13,\n",
       "         7.6672e-14, 9.1954e-09, 3.0481e-11, 5.0273e-08, 8.4174e-15, 2.9563e-07,\n",
       "         7.9295e-08, 5.1741e-11, 1.1957e-13],\n",
       "        [4.2892e-13, 3.4318e-13, 3.9056e-08, 8.8812e-16, 8.3427e-09, 8.9374e-13,\n",
       "         4.7733e-10, 5.4481e-15, 9.9960e-01, 3.8994e-04, 6.8780e-06, 6.7776e-13,\n",
       "         2.0916e-10, 3.0707e-14, 3.5528e-14, 1.6318e-12, 4.5732e-13, 3.3220e-12,\n",
       "         5.4662e-13, 2.2852e-08, 5.4484e-11, 2.5189e-07, 2.2293e-14, 5.4418e-07,\n",
       "         2.0855e-07, 2.1870e-10, 2.9049e-13],\n",
       "        [1.3588e-11, 5.8871e-13, 3.2426e-10, 1.7397e-12, 7.5940e-09, 1.2714e-10,\n",
       "         6.5686e-09, 4.9784e-10, 8.9489e-03, 9.9105e-01, 2.2892e-07, 1.8385e-09,\n",
       "         1.1841e-06, 1.7216e-14, 7.5966e-12, 8.4240e-12, 4.5250e-11, 5.7540e-13,\n",
       "         4.3255e-11, 5.9827e-09, 4.9640e-09, 1.4855e-11, 3.7769e-15, 6.0276e-07,\n",
       "         3.6923e-10, 6.6766e-10, 1.1691e-09],\n",
       "        [2.6267e-09, 1.2237e-08, 8.9952e-04, 2.8689e-07, 1.1219e-04, 9.0462e-06,\n",
       "         3.5353e-07, 8.4440e-07, 9.9805e-01, 3.1325e-04, 7.5860e-06, 9.2028e-06,\n",
       "         5.1469e-04, 2.6244e-10, 6.3591e-09, 3.8658e-08, 1.5057e-07, 1.3103e-08,\n",
       "         9.3080e-10, 3.4300e-07, 4.4665e-06, 2.1735e-06, 5.2535e-08, 1.5011e-06,\n",
       "         7.4753e-05, 1.2387e-07, 9.8082e-10],\n",
       "        [7.7250e-06, 1.1157e-08, 5.5071e-07, 9.8503e-05, 1.4880e-03, 2.1152e-03,\n",
       "         5.8031e-06, 2.9471e-02, 2.2827e-15, 1.3394e-06, 4.3476e-06, 1.6247e-05,\n",
       "         6.7448e-05, 3.4215e-03, 4.5836e-04, 1.9157e-05, 3.9819e-05, 3.0976e-05,\n",
       "         9.5111e-01, 6.3569e-09, 1.6118e-05, 4.9806e-10, 6.8668e-07, 3.9162e-10,\n",
       "         6.3270e-12, 1.1255e-02, 3.6994e-04],\n",
       "        [1.4401e-08, 9.0984e-11, 1.0826e-05, 1.4950e-09, 1.3408e-09, 1.1654e-05,\n",
       "         1.1496e-04, 5.1914e-07, 9.9962e-07, 9.8666e-01, 1.3532e-07, 8.9278e-03,\n",
       "         3.6441e-03, 1.3968e-09, 8.9365e-08, 1.8830e-08, 1.3304e-09, 1.9100e-12,\n",
       "         4.7260e-08, 6.2896e-04, 3.3931e-06, 1.6791e-08, 2.0725e-07, 1.6976e-07,\n",
       "         3.2272e-07, 1.8331e-09, 6.1063e-08],\n",
       "        [2.3435e-04, 8.5148e-06, 2.3788e-04, 1.0913e-02, 1.8807e-03, 1.5973e-03,\n",
       "         9.7459e-07, 9.8353e-01, 8.3347e-09, 1.5785e-06, 1.5959e-07, 2.7902e-04,\n",
       "         1.1495e-04, 3.8110e-06, 7.6921e-07, 4.0409e-05, 1.1099e-03, 6.9204e-07,\n",
       "         3.1890e-05, 1.0714e-08, 4.2964e-06, 9.7990e-11, 6.9645e-06, 3.0701e-11,\n",
       "         4.4039e-10, 5.5329e-08, 4.0070e-08]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T13:54:47.790506Z",
     "start_time": "2025-08-26T13:54:47.785284Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Probability of the True Label\n",
    "\n",
    "- `probs` gives the probability distribution of what character comes next in the sequence.\n",
    "- **Higher probability = higher chance** of being the correct next character.\n",
    "\n",
    "Now, we want to check the probability assigned to the **true output label** (from training data).\n",
    "\n",
    "---\n",
    "\n",
    "### Example with \"emma\"\n",
    "- For output **1** → row = 1, column = true label (`e = 5`) → `probs[1, 5]`\n",
    "- For output **2** → row = 2, column = true label (`m = 13`) → `probs[2, 13]`\n",
    "- For output **3** → row = 3, column = true label (`m = 13`) → `probs[3, 13]`\n",
    "- For output **4** → row = 4, column = true label (`a = 1`) → `probs[4, 1]`\n",
    "\n",
    "So each training example corresponds to one row, and the **true label index** tells us which column to select.\n",
    "\n",
    "---\n",
    "\n",
    "### Shortcut in PyTorch\n",
    "Instead of indexing manually for each row, we can do this in one line:\n",
    "\n",
    "```python\n",
    "probs[torch.arange(batch_size), Y]\n"
   ],
   "id": "1174071a13b793c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:47.009354Z",
     "start_time": "2025-08-26T14:03:47.004974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Probs gives us the probability of what character comes next in the sequence, higher probability = high chance of coming next.\n",
    "# so now i want to see what is the predicted probability for the right label i.e., we know true o/p from training data\n",
    "# now we pluck out what model gave the probability for tru o/p\n",
    "# that can be achieved by for o/p 1 its row 1 and column is going to be the true o/p form emma its e i.r., 5\n",
    "# so in prob matrix to see what is the probability of predicting e is [1,5]\n",
    "# for second o/p its [2,11]\n",
    "# for third o/p its [3,5]\n",
    "\n",
    "#inshortcut this can be written as [torch.arange(32),Y] because y has its all labels"
   ],
   "id": "3a4b95f079b3088f",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:47.554345Z",
     "start_time": "2025-08-26T14:03:47.547930Z"
    }
   },
   "cell_type": "code",
   "source": "probs[torch.arange(32), Y] # this is given the predicted probabilities for true label",
   "id": "478c5de482d1d67e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.4850e-13, 1.0038e-14, 3.2261e-09, 5.5106e-11, 1.7671e-04, 4.9955e-13,\n",
       "        1.0108e-04, 2.4826e-07, 1.0921e-01, 2.6832e-09, 8.9298e-09, 3.1874e-04,\n",
       "        2.2019e-13, 8.3137e-12, 1.7301e-05, 4.2377e-04, 2.4767e-04, 5.3181e-09,\n",
       "        6.3544e-13, 1.9945e-11, 1.4164e-02, 2.3869e-03, 5.9436e-05, 2.6606e-05,\n",
       "        2.1746e-03, 9.1954e-09, 1.6318e-12, 4.5250e-11, 9.9805e-01, 1.3394e-06,\n",
       "        9.0984e-11, 2.3435e-04])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:47.705232Z",
     "start_time": "2025-08-26T14:03:47.699378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss = -probs[torch.arange(32), Y].log().mean()\n",
    "loss # this is the loss of correct prediction"
   ],
   "id": "9522075bb7c83bbc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.1929)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:47.774364Z",
     "start_time": "2025-08-26T14:03:47.768678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Summarizing everything nd writing together\n",
    "X#input"
   ],
   "id": "466dbbf976dfc201",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:47.900180Z",
     "start_time": "2025-08-26T14:03:47.894182Z"
    }
   },
   "cell_type": "code",
   "source": "Y #o/p",
   "id": "df832b8a3a12a150",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:48.252336Z",
     "start_time": "2025-08-26T14:03:48.245915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # this is used for generating same weights from the tutorial\n",
    "C = torch.rand((27,2), generator=g) #Embedding layer( we choosed 2 dimensional embedding for 27 charcters\n",
    "W1 = torch.randn((6,100), generator=g)\n",
    "B1 = torch.rand(100, generator = g)\n",
    "W2 = torch.rand((100,27), generator = g)\n",
    "B2 = torch.rand(27, generator = g)\n",
    "parameters = [C, W1, B1, W2, B2]\n",
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ],
   "id": "6411b32771043a1e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:48.359340Z",
     "start_time": "2025-08-26T14:03:48.353071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding = C[X] #(32,3,2)\n",
    "Hidden_Layer = torch.tanh(embedding.view(-1,6)@ W1 + B1) #(32, 100)\n",
    "logits = Hidden_Layer @ W2 + B2 # (32,27)\n",
    "counts = logits.exp()\n",
    "probability = counts/counts.sum(1, keepdims = True)\n",
    "loss = -probability[torch.arange(32),Y].log().mean()\n",
    "loss"
   ],
   "id": "45b83a4e5f5b88b2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.5415)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:48.412203Z",
     "start_time": "2025-08-26T14:03:48.405585Z"
    }
   },
   "cell_type": "code",
   "source": "F.cross_entropy(logits,Y)# this step is same step we do for calculating the loss",
   "id": "7cc0313d760c5b6e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.5415)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Manual Loss Calculation vs. `F.cross_entropy`\n",
    "\n",
    "We can manually compute the loss from the logits using the following steps:\n",
    "\n",
    "\n",
    "counts = logits.exp()                                  # convert logits to unnormalized counts\n",
    "probabilities = counts / counts.sum(1, keepdims=True)  # normalize to get probabilities\n",
    "loss = -probabilities[torch.arange(32), Y].log().mean()  # negative log-likelihood of true labels\n",
    "\n",
    "\n",
    "\n",
    "## Important Notes\n",
    "\n",
    " **`F.cross_entropy` is highly recommended because:**\n",
    "- It is **more memory efficient**.\n",
    "- It is **numerically stable**.\n",
    "- It performs the **forward and backward passes efficiently**.\n",
    "\n",
    " **Manual computation** is useful only for **learning purposes**.\n",
    "In real projects, **always use `F.cross_entropy`**.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use `F.cross_entropy`?\n",
    "\n",
    "- Combines **`log_softmax` + negative log likelihood** in one optimized step.\n",
    "- Prevents **numerical issues** that may arise in manual calculations.\n",
    "- Automatically handles **batched data and gradients** efficiently.\n",
    "\n",
    " **Always prefer:**\n",
    "\n",
    "F.cross_entropy(logits, labels)\n"
   ],
   "id": "4a251862c73eb864"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:48.447481Z",
     "start_time": "2025-08-26T14:03:48.442852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\"\n",
    "counts = logits.exp()\n",
    "probability = counts/counts.sum(1, keepdims = True)\n",
    "loss = -probability[torch.arange(32),Y].log().mean()\n",
    "\n",
    "=\n",
    "\n",
    "F.cross_entropy(logits,Y)# this step is same step we do for calculating the loss\n",
    "\n",
    "# but F.cross_entrophy is very effective the above stepo we did is very ineffective but only to understading we did that\n",
    "\n",
    "in future allway use F.cross_entrophy which is memory efficenta nd does all calculations correctly\n",
    "\n",
    "\n",
    "make sure you allways use cross_entrophy ( because in many scenarios our manual calculation of loss fails)\n",
    "\n",
    "Forward and  backward pass will be much efficient when we us ethe cross entrophy\n",
    "\n",
    "\"\"\""
   ],
   "id": "cd50caae0fb69eec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\ncounts = logits.exp()\\nprobability = counts/counts.sum(1, keepdims = True)\\nloss = -probability[torch.arange(32),Y].log().mean()\\n\\n=\\n\\nF.cross_entropy(logits,Y)# this step is same step we do for calculating the loss\\n\\n# but F.cross_entrophy is very effective the above stepo we did is very ineffective but only to understading we did that\\n\\nin future allway use F.cross_entrophy which is memory efficenta nd does all calculations correctly\\n\\n\\nmake sure you allways use cross_entrophy ( because in many scenarios our manual calculation of loss fails)\\n\\nForward and  backward pass will be much efficient when we us ethe cross entrophy\\n\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cleaner version of modeling",
   "id": "42860d664ea65822"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:48.485940Z",
     "start_time": "2025-08-26T14:03:48.479491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # this is used for generating same weights from the tutorial\n",
    "C = torch.rand((27,2), generator=g) #Embedding layer( we choosed 2 dimensional embedding for 27 charcters\n",
    "W1 = torch.randn((6,100), generator=g)\n",
    "B1 = torch.rand(100, generator = g)\n",
    "W2 = torch.rand((100,27), generator = g)\n",
    "B2 = torch.rand(27, generator = g)\n",
    "parameters = [C, W1, B1, W2, B2]\n",
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ],
   "id": "37e8329bbc0cf804",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:48.511621Z",
     "start_time": "2025-08-26T14:03:48.507725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ],
   "id": "32e0cefb203b231f",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:48.542546Z",
     "start_time": "2025-08-26T14:03:48.535183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Forward pass\n",
    "embedding = C[X] #(32,3,2)\n",
    "Hidden_Layer = torch.tanh(embedding.view(-1,6)@ W1 + B1) #(32, 100)\n",
    "logits = Hidden_Layer @ W2 + B2 # (32,27)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "print(loss.item())\n",
    "\n",
    "# backward pass\n",
    "\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "loss.backward()\n",
    "# update the weights\n",
    "for p in parameters:\n",
    "    p.data = p.data + (-0.1 * p.grad)\n"
   ],
   "id": "3eae73b043b745f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.54152774810791\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:49.301558Z",
     "start_time": "2025-08-26T14:03:48.565840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# so every time we run this the weights keeps on updating\n",
    "\n",
    "for _ in range(1000):\n",
    "    # Forward pass\n",
    "    embedding = C[X] #(32,3,2)\n",
    "    Hidden_Layer = torch.tanh(embedding.view(-1,6)@ W1 + B1) #(32, 100)\n",
    "    logits = Hidden_Layer @ W2 + B2 # (32,27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update the weights\n",
    "    for p in parameters:\n",
    "        p.data = p.data + (-0.1 * p.grad)"
   ],
   "id": "9c1a1f69f10e256f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.614640712738037\n",
      "3.0285861492156982\n",
      "2.6664645671844482\n",
      "2.460556745529175\n",
      "2.333811044692993\n",
      "2.2418532371520996\n",
      "2.168241500854492\n",
      "2.1061441898345947\n",
      "2.05218768119812\n",
      "2.004423141479492\n",
      "1.961562156677246\n",
      "1.9226782321929932\n",
      "1.8870705366134644\n",
      "1.854193925857544\n",
      "1.8236156702041626\n",
      "1.7949838638305664\n",
      "1.768011450767517\n",
      "1.7424613237380981\n",
      "1.7181344032287598\n",
      "1.694862961769104\n",
      "1.6725071668624878\n",
      "1.650946021080017\n",
      "1.6300790309906006\n",
      "1.609818458557129\n",
      "1.5900893211364746\n",
      "1.570827841758728\n",
      "1.5519806146621704\n",
      "1.5335006713867188\n",
      "1.5153485536575317\n",
      "1.4974911212921143\n",
      "1.4799002408981323\n",
      "1.4625539779663086\n",
      "1.4454317092895508\n",
      "1.4285194873809814\n",
      "1.4118046760559082\n",
      "1.395277738571167\n",
      "1.3789324760437012\n",
      "1.3627640008926392\n",
      "1.3467694520950317\n",
      "1.3309472799301147\n",
      "1.3152976036071777\n",
      "1.299822449684143\n",
      "1.2845224142074585\n",
      "1.2694005966186523\n",
      "1.254460096359253\n",
      "1.2397040128707886\n",
      "1.225135326385498\n",
      "1.2107577323913574\n",
      "1.1965742111206055\n",
      "1.1825870275497437\n",
      "1.1687997579574585\n",
      "1.1552135944366455\n",
      "1.1418308019638062\n",
      "1.1286523342132568\n",
      "1.1156785488128662\n",
      "1.1029105186462402\n",
      "1.0903472900390625\n",
      "1.0779892206192017\n",
      "1.0658338069915771\n",
      "1.053881049156189\n",
      "1.0421286821365356\n",
      "1.030574083328247\n",
      "1.019215703010559\n",
      "1.0080509185791016\n",
      "0.9970762729644775\n",
      "0.9862897396087646\n",
      "0.9756873846054077\n",
      "0.9652665853500366\n",
      "0.9550237655639648\n",
      "0.9449557662010193\n",
      "0.9350587129592896\n",
      "0.9253295063972473\n",
      "0.9157644510269165\n",
      "0.9063603281974792\n",
      "0.8971136808395386\n",
      "0.8880208134651184\n",
      "0.8790785074234009\n",
      "0.870283305644989\n",
      "0.8616324663162231\n",
      "0.8531222343444824\n",
      "0.844749391078949\n",
      "0.8365113139152527\n",
      "0.8284047245979309\n",
      "0.8204264640808105\n",
      "0.8125742077827454\n",
      "0.8048443794250488\n",
      "0.797235369682312\n",
      "0.7897434830665588\n",
      "0.7823666930198669\n",
      "0.775102436542511\n",
      "0.767948567867279\n",
      "0.7609024047851562\n",
      "0.7539620995521545\n",
      "0.7471249103546143\n",
      "0.7403892874717712\n",
      "0.7337531447410583\n",
      "0.7272139191627502\n",
      "0.7207702398300171\n",
      "0.7144201993942261\n",
      "0.7081620097160339\n",
      "0.7019942998886108\n",
      "0.6959143280982971\n",
      "0.6899217367172241\n",
      "0.684013843536377\n",
      "0.6781901717185974\n",
      "0.6724485158920288\n",
      "0.6667877435684204\n",
      "0.6612063646316528\n",
      "0.6557034850120544\n",
      "0.6502771377563477\n",
      "0.6449261903762817\n",
      "0.6396499276161194\n",
      "0.6344465017318726\n",
      "0.6293152570724487\n",
      "0.6242549419403076\n",
      "0.6192642450332642\n",
      "0.614342451095581\n",
      "0.6094884872436523\n",
      "0.6047009229660034\n",
      "0.5999795794487\n",
      "0.5953226685523987\n",
      "0.5907299518585205\n",
      "0.5862001180648804\n",
      "0.5817327499389648\n",
      "0.5773270130157471\n",
      "0.5729816555976868\n",
      "0.568696141242981\n",
      "0.5644700527191162\n",
      "0.5603023767471313\n",
      "0.5561925172805786\n",
      "0.5521397590637207\n",
      "0.5481438636779785\n",
      "0.544203519821167\n",
      "0.5403187274932861\n",
      "0.5364888310432434\n",
      "0.5327132344245911\n",
      "0.5289915204048157\n",
      "0.5253229737281799\n",
      "0.5217071771621704\n",
      "0.5181435346603394\n",
      "0.514631986618042\n",
      "0.5111715793609619\n",
      "0.5077622532844543\n",
      "0.5044032335281372\n",
      "0.5010944604873657\n",
      "0.4978351294994354\n",
      "0.49462494254112244\n",
      "0.49146345257759094\n",
      "0.4883502721786499\n",
      "0.48528480529785156\n",
      "0.48226672410964966\n",
      "0.47929561138153076\n",
      "0.47637078166007996\n",
      "0.47349196672439575\n",
      "0.47065871953964233\n",
      "0.4678702652454376\n",
      "0.46512651443481445\n",
      "0.46242672204971313\n",
      "0.4597702920436859\n",
      "0.45715683698654175\n",
      "0.4545859396457672\n",
      "0.45205676555633545\n",
      "0.44956907629966736\n",
      "0.44712215662002563\n",
      "0.4447155296802521\n",
      "0.44234853982925415\n",
      "0.4400208294391632\n",
      "0.4377315938472748\n",
      "0.4354802966117859\n",
      "0.43326666951179504\n",
      "0.4310897886753082\n",
      "0.428949236869812\n",
      "0.4268442988395691\n",
      "0.4247746467590332\n",
      "0.4227394759654999\n",
      "0.42073842883110046\n",
      "0.41877084970474243\n",
      "0.4168359935283661\n",
      "0.4149334728717804\n",
      "0.4130628705024719\n",
      "0.4112232029438019\n",
      "0.40941449999809265\n",
      "0.4076358675956726\n",
      "0.405886709690094\n",
      "0.4041665196418762\n",
      "0.4024749994277954\n",
      "0.40081146359443665\n",
      "0.3991753160953522\n",
      "0.39756613969802856\n",
      "0.39598348736763\n",
      "0.39442676305770874\n",
      "0.3928956389427185\n",
      "0.3913894593715668\n",
      "0.3899077773094177\n",
      "0.3884503245353699\n",
      "0.3870164155960083\n",
      "0.38560572266578674\n",
      "0.3842175602912903\n",
      "0.38285186886787415\n",
      "0.3815079927444458\n",
      "0.38018566370010376\n",
      "0.3788842558860779\n",
      "0.3776035010814667\n",
      "0.3763430714607239\n",
      "0.3751024603843689\n",
      "0.37388136982917786\n",
      "0.3726792335510254\n",
      "0.3714960217475891\n",
      "0.3703310787677765\n",
      "0.3691841661930084\n",
      "0.36805492639541626\n",
      "0.3669431209564209\n",
      "0.365848183631897\n",
      "0.36476996541023254\n",
      "0.3637081980705261\n",
      "0.3626623749732971\n",
      "0.36163225769996643\n",
      "0.3606177568435669\n",
      "0.35961830615997314\n",
      "0.35863372683525085\n",
      "0.3576638400554657\n",
      "0.35670822858810425\n",
      "0.3557665944099426\n",
      "0.3548387289047241\n",
      "0.3539244532585144\n",
      "0.35302332043647766\n",
      "0.3521353602409363\n",
      "0.35126006603240967\n",
      "0.35039740800857544\n",
      "0.34954696893692017\n",
      "0.3487086594104767\n",
      "0.3478822708129883\n",
      "0.3470674157142639\n",
      "0.34626391530036926\n",
      "0.3454718291759491\n",
      "0.3446907103061676\n",
      "0.3439203202724457\n",
      "0.3431605398654938\n",
      "0.3424112796783447\n",
      "0.3416721522808075\n",
      "0.34094318747520447\n",
      "0.3402240574359894\n",
      "0.3395145535469055\n",
      "0.3388146162033081\n",
      "0.33812403678894043\n",
      "0.33744266629219055\n",
      "0.33677029609680176\n",
      "0.3361068069934845\n",
      "0.33545202016830444\n",
      "0.33480584621429443\n",
      "0.334168016910553\n",
      "0.3335384130477905\n",
      "0.3329170346260071\n",
      "0.33230361342430115\n",
      "0.33169806003570557\n",
      "0.331100195646286\n",
      "0.33050984144210815\n",
      "0.3299269676208496\n",
      "0.3293514847755432\n",
      "0.32878315448760986\n",
      "0.3282218873500824\n",
      "0.32766762375831604\n",
      "0.32712018489837646\n",
      "0.3265795409679413\n",
      "0.3260454535484314\n",
      "0.325517863035202\n",
      "0.32499662041664124\n",
      "0.3244818449020386\n",
      "0.3239731788635254\n",
      "0.3234705328941345\n",
      "0.32297390699386597\n",
      "0.32248324155807495\n",
      "0.3219984173774719\n",
      "0.3215191662311554\n",
      "0.3210456073284149\n",
      "0.32057759165763855\n",
      "0.32011497020721436\n",
      "0.31965768337249756\n",
      "0.31920576095581055\n",
      "0.31875890493392944\n",
      "0.3183172643184662\n",
      "0.3178806006908417\n",
      "0.31744885444641113\n",
      "0.31702205538749695\n",
      "0.31659993529319763\n",
      "0.3161826729774475\n",
      "0.3157700002193451\n",
      "0.3153618276119232\n",
      "0.3149583041667938\n",
      "0.3145591616630554\n",
      "0.314164400100708\n",
      "0.31377384066581726\n",
      "0.3133876323699951\n",
      "0.31300562620162964\n",
      "0.3126276731491089\n",
      "0.31225377321243286\n",
      "0.3118838667869568\n",
      "0.3115179240703583\n",
      "0.3111558258533478\n",
      "0.3107976019382477\n",
      "0.3104431629180908\n",
      "0.31009241938591003\n",
      "0.30974525213241577\n",
      "0.3094017803668976\n",
      "0.30906176567077637\n",
      "0.3087253272533417\n",
      "0.30839231610298157\n",
      "0.3080627918243408\n",
      "0.30773651599884033\n",
      "0.30741357803344727\n",
      "0.30709388852119446\n",
      "0.3067775070667267\n",
      "0.30646422505378723\n",
      "0.3061540722846985\n",
      "0.3058469891548157\n",
      "0.3055429458618164\n",
      "0.3052418529987335\n",
      "0.3049437999725342\n",
      "0.30464863777160645\n",
      "0.30435627698898315\n",
      "0.30406689643859863\n",
      "0.3037802577018738\n",
      "0.303496390581131\n",
      "0.30321523547172546\n",
      "0.30293673276901245\n",
      "0.30266091227531433\n",
      "0.3023877739906311\n",
      "0.30211710929870605\n",
      "0.3018490672111511\n",
      "0.30158349871635437\n",
      "0.3013204038143158\n",
      "0.3010597825050354\n",
      "0.300801545381546\n",
      "0.30054572224617004\n",
      "0.3002922534942627\n",
      "0.30004119873046875\n",
      "0.2997923195362091\n",
      "0.2995457351207733\n",
      "0.299301415681839\n",
      "0.29905930161476135\n",
      "0.29881930351257324\n",
      "0.2985815107822418\n",
      "0.29834580421447754\n",
      "0.29811224341392517\n",
      "0.29788073897361755\n",
      "0.2976512908935547\n",
      "0.2974238693714142\n",
      "0.2971983850002289\n",
      "0.29697495698928833\n",
      "0.296753466129303\n",
      "0.2965337932109833\n",
      "0.2963161766529083\n",
      "0.29610028862953186\n",
      "0.2958863377571106\n",
      "0.29567426443099976\n",
      "0.2954639196395874\n",
      "0.2952554225921631\n",
      "0.29504862427711487\n",
      "0.2948436439037323\n",
      "0.29464033246040344\n",
      "0.2944387197494507\n",
      "0.2942388355731964\n",
      "0.29404059052467346\n",
      "0.29384398460388184\n",
      "0.29364898800849915\n",
      "0.29345566034317017\n",
      "0.29326391220092773\n",
      "0.2930736243724823\n",
      "0.2928850054740906\n",
      "0.2926979064941406\n",
      "0.29251229763031006\n",
      "0.29232820868492126\n",
      "0.2921455502510071\n",
      "0.2919643819332123\n",
      "0.29178470373153687\n",
      "0.29160642623901367\n",
      "0.2914295792579651\n",
      "0.29125410318374634\n",
      "0.2910799980163574\n",
      "0.2909073233604431\n",
      "0.2907359004020691\n",
      "0.2905658781528473\n",
      "0.2903972268104553\n",
      "0.290229856967926\n",
      "0.290063738822937\n",
      "0.28989893198013306\n",
      "0.2897353768348694\n",
      "0.2895731031894684\n",
      "0.2894120216369629\n",
      "0.28925222158432007\n",
      "0.28909361362457275\n",
      "0.28893619775772095\n",
      "0.2887799143791199\n",
      "0.2886248826980591\n",
      "0.2884710133075714\n",
      "0.2883182764053345\n",
      "0.2881666421890259\n",
      "0.2880161702632904\n",
      "0.2878668010234833\n",
      "0.2877185344696045\n",
      "0.28757137060165405\n",
      "0.2874252200126648\n",
      "0.28728023171424866\n",
      "0.2871362566947937\n",
      "0.2869933247566223\n",
      "0.2868514060974121\n",
      "0.2867105007171631\n",
      "0.28657066822052\n",
      "0.28643178939819336\n",
      "0.2862939238548279\n",
      "0.2861570417881012\n",
      "0.2860211730003357\n",
      "0.2858861982822418\n",
      "0.28575220704078674\n",
      "0.2856191396713257\n",
      "0.2854870557785034\n",
      "0.28535592555999756\n",
      "0.28522560000419617\n",
      "0.2850962281227112\n",
      "0.2849677503108978\n",
      "0.2848401367664337\n",
      "0.2847134470939636\n",
      "0.2845876216888428\n",
      "0.2844626307487488\n",
      "0.2843385338783264\n",
      "0.2842152416706085\n",
      "0.28409284353256226\n",
      "0.2839711904525757\n",
      "0.28385046124458313\n",
      "0.28373050689697266\n",
      "0.28361132740974426\n",
      "0.28349292278289795\n",
      "0.2833753824234009\n",
      "0.2832585871219635\n",
      "0.2831425964832306\n",
      "0.283027321100235\n",
      "0.28291285037994385\n",
      "0.28279909491539\n",
      "0.28268617391586304\n",
      "0.2825739085674286\n",
      "0.28246232867240906\n",
      "0.282351553440094\n",
      "0.28224149346351624\n",
      "0.2821321487426758\n",
      "0.28202348947525024\n",
      "0.2819156050682068\n",
      "0.2818082869052887\n",
      "0.2817017436027527\n",
      "0.2815958261489868\n",
      "0.28149059414863586\n",
      "0.2813860774040222\n",
      "0.2812821567058563\n",
      "0.28117895126342773\n",
      "0.2810763716697693\n",
      "0.280974417924881\n",
      "0.2808730900287628\n",
      "0.28077247738838196\n",
      "0.28067249059677124\n",
      "0.2805730402469635\n",
      "0.2804741859436035\n",
      "0.28037604689598083\n",
      "0.28027844429016113\n",
      "0.2801814675331116\n",
      "0.2800850570201874\n",
      "0.2799892723560333\n",
      "0.279894083738327\n",
      "0.2797994613647461\n",
      "0.27970534563064575\n",
      "0.27961188554763794\n",
      "0.27951890230178833\n",
      "0.27942654490470886\n",
      "0.27933475375175476\n",
      "0.279243528842926\n",
      "0.2791527509689331\n",
      "0.27906256914138794\n",
      "0.27897292375564575\n",
      "0.27888381481170654\n",
      "0.2787952125072479\n",
      "0.2787071764469147\n",
      "0.27861958742141724\n",
      "0.27853256464004517\n",
      "0.2784460484981537\n",
      "0.2783600389957428\n",
      "0.2782744765281677\n",
      "0.278189480304718\n",
      "0.2781049609184265\n",
      "0.2780208885669708\n",
      "0.2779373526573181\n",
      "0.27785423398017883\n",
      "0.27777165174484253\n",
      "0.27768954634666443\n",
      "0.27760791778564453\n",
      "0.2775266468524933\n",
      "0.2774459421634674\n",
      "0.27736565470695496\n",
      "0.2772858440876007\n",
      "0.2772064507007599\n",
      "0.2771275043487549\n",
      "0.2770490348339081\n",
      "0.2769710123538971\n",
      "0.27689334750175476\n",
      "0.276816189289093\n",
      "0.2767394483089447\n",
      "0.2766630947589874\n",
      "0.27658718824386597\n",
      "0.27651169896125793\n",
      "0.27643659710884094\n",
      "0.27636200189590454\n",
      "0.2762877345085144\n",
      "0.2762138545513153\n",
      "0.27614039182662964\n",
      "0.2760673463344574\n",
      "0.2759947180747986\n",
      "0.27592241764068604\n",
      "0.2758505642414093\n",
      "0.2757790982723236\n",
      "0.27570801973342896\n",
      "0.2756372392177582\n",
      "0.275566965341568\n",
      "0.2754969596862793\n",
      "0.2754274010658264\n",
      "0.2753581404685974\n",
      "0.27528926730155945\n",
      "0.2752207815647125\n",
      "0.27515265345573425\n",
      "0.27508488297462463\n",
      "0.27501749992370605\n",
      "0.27495041489601135\n",
      "0.2748836874961853\n",
      "0.2748173177242279\n",
      "0.2747512757778168\n",
      "0.2746855914592743\n",
      "0.27462029457092285\n",
      "0.2745552361011505\n",
      "0.2744905948638916\n",
      "0.27442625164985657\n",
      "0.2743622362613678\n",
      "0.2742985486984253\n",
      "0.27423518896102905\n",
      "0.2741721272468567\n",
      "0.27410945296287537\n",
      "0.27404704689979553\n",
      "0.27398499846458435\n",
      "0.27392321825027466\n",
      "0.27386176586151123\n",
      "0.2738006114959717\n",
      "0.273739755153656\n",
      "0.2736791968345642\n",
      "0.2736189663410187\n",
      "0.273559033870697\n",
      "0.27349942922592163\n",
      "0.27344006299972534\n",
      "0.2733810245990753\n",
      "0.27332228422164917\n",
      "0.2732638120651245\n",
      "0.27320563793182373\n",
      "0.27314770221710205\n",
      "0.27309009432792664\n",
      "0.2730327844619751\n",
      "0.27297574281692505\n",
      "0.2729189693927765\n",
      "0.2728624939918518\n",
      "0.27280616760253906\n",
      "0.27275025844573975\n",
      "0.27269455790519714\n",
      "0.2726391553878784\n",
      "0.2725840210914612\n",
      "0.27252912521362305\n",
      "0.2724744975566864\n",
      "0.27242016792297363\n",
      "0.27236607670783997\n",
      "0.2723122239112854\n",
      "0.27225857973098755\n",
      "0.27220529317855835\n",
      "0.2721521556377411\n",
      "0.2720993459224701\n",
      "0.2720467448234558\n",
      "0.27199438214302063\n",
      "0.2719423472881317\n",
      "0.27189043164253235\n",
      "0.27183884382247925\n",
      "0.27178746461868286\n",
      "0.27173638343811035\n",
      "0.2716854512691498\n",
      "0.2716347575187683\n",
      "0.2715843617916107\n",
      "0.2715342044830322\n",
      "0.2714841663837433\n",
      "0.2714344561100006\n",
      "0.27138495445251465\n",
      "0.271335631608963\n",
      "0.27128660678863525\n",
      "0.27123773097991943\n",
      "0.2711891233921051\n",
      "0.2711407542228699\n",
      "0.27109256386756897\n",
      "0.27104464173316956\n",
      "0.2709968388080597\n",
      "0.2709493339061737\n",
      "0.27090203762054443\n",
      "0.2708549201488495\n",
      "0.27080804109573364\n",
      "0.2707613408565521\n",
      "0.27071481943130493\n",
      "0.2706685960292816\n",
      "0.2706225514411926\n",
      "0.2705766558647156\n",
      "0.27053096890449524\n",
      "0.2704854905605316\n",
      "0.2704402804374695\n",
      "0.2703952193260193\n",
      "0.2703503668308258\n",
      "0.27030569314956665\n",
      "0.2702612578868866\n",
      "0.2702169120311737\n",
      "0.2701728641986847\n",
      "0.27012899518013\n",
      "0.27008524537086487\n",
      "0.2700417637825012\n",
      "0.2699984014034271\n",
      "0.26995524764060974\n",
      "0.26991233229637146\n",
      "0.26986950635910034\n",
      "0.2698269784450531\n",
      "0.2697845697402954\n",
      "0.26974236965179443\n",
      "0.2697002589702606\n",
      "0.2696584463119507\n",
      "0.2696167230606079\n",
      "0.2695751488208771\n",
      "0.2695339024066925\n",
      "0.2694927155971527\n",
      "0.26945167779922485\n",
      "0.2694108784198761\n",
      "0.26937025785446167\n",
      "0.2693297564983368\n",
      "0.269289493560791\n",
      "0.2692493200302124\n",
      "0.2692093253135681\n",
      "0.26916950941085815\n",
      "0.2691299021244049\n",
      "0.2690903842449188\n",
      "0.26905107498168945\n",
      "0.269011914730072\n",
      "0.26897290349006653\n",
      "0.26893413066864014\n",
      "0.2688954770565033\n",
      "0.268856942653656\n",
      "0.26881855726242065\n",
      "0.268780380487442\n",
      "0.2687423527240753\n",
      "0.2687044143676758\n",
      "0.26866668462753296\n",
      "0.26862913370132446\n",
      "0.26859167218208313\n",
      "0.2685543894767761\n",
      "0.26851725578308105\n",
      "0.2684802711009979\n",
      "0.26844340562820435\n",
      "0.2684067189693451\n",
      "0.26837021112442017\n",
      "0.2683338224887848\n",
      "0.26829755306243896\n",
      "0.2682614326477051\n",
      "0.2682254910469055\n",
      "0.2681896686553955\n",
      "0.26815396547317505\n",
      "0.2681184411048889\n",
      "0.26808300614356995\n",
      "0.2680477797985077\n",
      "0.2680126428604126\n",
      "0.26797768473625183\n",
      "0.2679428160190582\n",
      "0.26790809631347656\n",
      "0.26787352561950684\n",
      "0.26783910393714905\n",
      "0.2678048014640808\n",
      "0.26777058839797974\n",
      "0.2677365839481354\n",
      "0.2677026689052582\n",
      "0.26766887307167053\n",
      "0.2676352262496948\n",
      "0.2676016688346863\n",
      "0.26756832003593445\n",
      "0.2675350606441498\n",
      "0.26750192046165466\n",
      "0.2674688994884491\n",
      "0.2674359679222107\n",
      "0.2674032747745514\n",
      "0.26737064123153687\n",
      "0.2673380970954895\n",
      "0.26730573177337646\n",
      "0.2672734260559082\n",
      "0.26724129915237427\n",
      "0.2672092914581299\n",
      "0.2671773433685303\n",
      "0.2671456038951874\n",
      "0.26711392402648926\n",
      "0.2670823931694031\n",
      "0.26705098152160645\n",
      "0.267019659280777\n",
      "0.26698848605155945\n",
      "0.2669574022293091\n",
      "0.26692643761634827\n",
      "0.2668955624103546\n",
      "0.2668648362159729\n",
      "0.26683422923088074\n",
      "0.26680371165275574\n",
      "0.2667732834815979\n",
      "0.2667430341243744\n",
      "0.26671284437179565\n",
      "0.26668280363082886\n",
      "0.2666528820991516\n",
      "0.26662302017211914\n",
      "0.2665933072566986\n",
      "0.26656365394592285\n",
      "0.26653414964675903\n",
      "0.26650470495224\n",
      "0.2664753794670105\n",
      "0.26644620299339294\n",
      "0.26641711592674255\n",
      "0.2663881480693817\n",
      "0.26635923981666565\n",
      "0.26633042097091675\n",
      "0.2663017809391022\n",
      "0.26627323031425476\n",
      "0.26624467968940735\n",
      "0.26621633768081665\n",
      "0.2661880850791931\n",
      "0.26615989208221436\n",
      "0.26613181829452515\n",
      "0.2661038935184479\n",
      "0.2660759687423706\n",
      "0.26604822278022766\n",
      "0.2660205066204071\n",
      "0.2659929692745209\n",
      "0.2659654915332794\n",
      "0.2659381031990051\n",
      "0.265910804271698\n",
      "0.2658836543560028\n",
      "0.2658565044403076\n",
      "0.26582950353622437\n",
      "0.2658025920391083\n",
      "0.26577579975128174\n",
      "0.2657490372657776\n",
      "0.26572245359420776\n",
      "0.26569586992263794\n",
      "0.26566940546035767\n",
      "0.2656431496143341\n",
      "0.26561683416366577\n",
      "0.2655906081199646\n",
      "0.26556456089019775\n",
      "0.2655385732650757\n",
      "0.265512615442276\n",
      "0.26548686623573303\n",
      "0.2654610872268677\n",
      "0.26543551683425903\n",
      "0.265409916639328\n",
      "0.2653844356536865\n",
      "0.265359103679657\n",
      "0.26533377170562744\n",
      "0.26530858874320984\n",
      "0.26528340578079224\n",
      "0.26525840163230896\n",
      "0.26523345708847046\n",
      "0.2652086019515991\n",
      "0.26518380641937256\n",
      "0.2651590406894684\n",
      "0.26513445377349854\n",
      "0.2651098966598511\n",
      "0.26508545875549316\n",
      "0.26506105065345764\n",
      "0.26503679156303406\n",
      "0.26501256227493286\n",
      "0.26498842239379883\n",
      "0.26496437191963196\n",
      "0.26494041085243225\n",
      "0.2649165391921997\n",
      "0.26489269733428955\n",
      "0.26486897468566895\n",
      "0.26484525203704834\n",
      "0.26482170820236206\n",
      "0.2647981643676758\n",
      "0.26477476954460144\n",
      "0.26475146412849426\n",
      "0.2647281289100647\n",
      "0.26470494270324707\n",
      "0.26468175649642944\n",
      "0.26465877890586853\n",
      "0.26463577151298523\n",
      "0.2646128535270691\n",
      "0.2645900845527649\n",
      "0.2645673155784607\n",
      "0.26454460620880127\n",
      "0.2645220160484314\n",
      "0.2644994854927063\n",
      "0.264477014541626\n",
      "0.2644546627998352\n",
      "0.26443228125572205\n",
      "0.2644100487232208\n",
      "0.26438790559768677\n",
      "0.2643657624721527\n",
      "0.2643437683582306\n",
      "0.26432180404663086\n",
      "0.2642998993396759\n",
      "0.26427799463272095\n",
      "0.2642562985420227\n",
      "0.26423460245132446\n",
      "0.2642129957675934\n",
      "0.2641914486885071\n",
      "0.26416999101638794\n",
      "0.2641485631465912\n",
      "0.26412713527679443\n",
      "0.264105886220932\n",
      "0.26408469676971436\n",
      "0.2640635371208191\n",
      "0.264042466878891\n",
      "0.2640214264392853\n",
      "0.26400044560432434\n",
      "0.26397958397865295\n",
      "0.26395875215530396\n",
      "0.2639380097389221\n",
      "0.26391729712486267\n",
      "0.2638967037200928\n",
      "0.2638761103153229\n",
      "0.26385563611984253\n",
      "0.26383519172668457\n",
      "0.2638148367404938\n",
      "0.26379451155662537\n",
      "0.26377424597740173\n",
      "0.2637540400028229\n",
      "0.2637339234352112\n",
      "0.26371386647224426\n",
      "0.26369383931159973\n",
      "0.2636738717556\n",
      "0.26365402340888977\n",
      "0.26363417506217957\n",
      "0.2636144161224365\n",
      "0.26359468698501587\n",
      "0.26357507705688477\n",
      "0.26355546712875366\n",
      "0.2635359764099121\n",
      "0.26351648569107056\n",
      "0.26349708437919617\n",
      "0.26347771286964417\n",
      "0.2634584605693817\n",
      "0.26343920826911926\n",
      "0.2634199857711792\n",
      "0.2634009122848511\n",
      "0.26338180899620056\n",
      "0.2633628249168396\n",
      "0.26334384083747864\n",
      "0.26332494616508484\n",
      "0.2633061110973358\n",
      "0.26328733563423157\n",
      "0.2632686495780945\n",
      "0.2632499635219574\n",
      "0.2632313370704651\n",
      "0.26321277022361755\n",
      "0.2631942331790924\n",
      "0.2631758451461792\n",
      "0.2631574273109436\n",
      "0.2631390392780304\n",
      "0.26312077045440674\n",
      "0.26310256123542786\n",
      "0.2630843222141266\n",
      "0.26306623220443726\n",
      "0.26304811239242554\n",
      "0.26303014159202576\n",
      "0.2630121111869812\n",
      "0.2629942297935486\n",
      "0.26297637820243835\n",
      "0.2629585564136505\n",
      "0.26294079422950745\n",
      "0.2629230320453644\n",
      "0.2629053592681885\n",
      "0.26288774609565735\n",
      "0.2628701627254486\n",
      "0.2628527283668518\n",
      "0.26283520460128784\n",
      "0.2628178000450134\n",
      "0.2628004252910614\n",
      "0.26278311014175415\n",
      "0.26276588439941406\n",
      "0.2627486288547516\n",
      "0.2627314627170563\n",
      "0.26271435618400574\n",
      "0.2626972794532776\n",
      "0.2626802623271942\n",
      "0.2626633048057556\n",
      "0.2626463770866394\n",
      "0.26262950897216797\n",
      "0.26261264085769653\n",
      "0.26259586215019226\n",
      "0.26257914304733276\n",
      "0.26256245374679565\n",
      "0.26254576444625854\n",
      "0.262529194355011\n",
      "0.2625126540660858\n",
      "0.26249614357948303\n",
      "0.2624797224998474\n",
      "0.2624633312225342\n",
      "0.26244691014289856\n",
      "0.2624305486679077\n",
      "0.2624143362045288\n",
      "0.2623980939388275\n",
      "0.262381911277771\n",
      "0.26236578822135925\n",
      "0.2623496949672699\n",
      "0.26233363151550293\n",
      "0.2623176574707031\n",
      "0.2623016834259033\n",
      "0.2622857689857483\n",
      "0.26226985454559326\n",
      "0.26225408911705017\n",
      "0.2622382640838623\n",
      "0.2622225284576416\n",
      "0.2622067928314209\n",
      "0.26219114661216736\n",
      "0.2621755301952362\n",
      "0.26215994358062744\n",
      "0.26214441657066345\n",
      "0.26212894916534424\n",
      "0.26211345195770264\n",
      "0.2620980739593506\n",
      "0.26208269596099854\n",
      "0.26206737756729126\n",
      "0.26205211877822876\n",
      "0.26203688979148865\n",
      "0.26202163100242615\n",
      "0.2620064914226532\n",
      "0.26199138164520264\n",
      "0.26197633147239685\n",
      "0.26196128129959106\n",
      "0.26194626092910767\n",
      "0.26193132996559143\n",
      "0.2619164288043976\n",
      "0.26190149784088135\n",
      "0.2618866562843323\n",
      "0.26187190413475037\n",
      "0.26185712218284607\n",
      "0.26184239983558655\n",
      "0.2618277370929718\n",
      "0.26181307435035706\n",
      "0.2617984712123871\n",
      "0.2617838680744171\n",
      "0.2617693543434143\n",
      "0.26175493001937866\n",
      "0.26174044609069824\n",
      "0.2617260217666626\n",
      "0.26171165704727173\n",
      "0.26169732213020325\n",
      "0.26168304681777954\n",
      "0.26166877150535583\n",
      "0.2616545557975769\n",
      "0.2616403102874756\n",
      "0.2616261839866638\n",
      "0.26161208748817444\n",
      "0.26159802079200745\n",
      "0.26158398389816284\n",
      "0.2615699768066406\n",
      "0.2615560293197632\n",
      "0.26154208183288574\n",
      "0.2615281939506531\n",
      "0.261514276266098\n",
      "0.2615004777908325\n",
      "0.2614867091178894\n",
      "0.2614729404449463\n",
      "0.26145923137664795\n",
      "0.26144546270370483\n",
      "0.26143184304237366\n",
      "0.26141825318336487\n",
      "0.2614046633243561\n",
      "0.2613911032676697\n",
      "0.26137763261795044\n",
      "0.2613641619682312\n",
      "0.26135069131851196\n",
      "0.2613373100757599\n",
      "0.2613239288330078\n",
      "0.2613105773925781\n",
      "0.2612972557544708\n",
      "0.2612839639186859\n",
      "0.26127076148986816\n",
      "0.2612575888633728\n",
      "0.26124441623687744\n",
      "0.26123127341270447\n",
      "0.2612181305885315\n",
      "0.26120510697364807\n",
      "0.26119208335876465\n",
      "0.2611790597438812\n",
      "0.2611660361289978\n",
      "0.26115313172340393\n",
      "0.26114019751548767\n",
      "0.2611273229122162\n",
      "0.2611145079135895\n",
      "0.2611016631126404\n",
      "0.26108887791633606\n",
      "0.2610761523246765\n",
      "0.26106342673301697\n",
      "0.2610507607460022\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Why Loss is Not Zero in the Overfitting Case?\n",
    "\n",
    "Even though our neural network has **3k parameters** and we only trained it on **32 inputs and 32 outputs**, the loss does not go to zero.\n",
    "Normally, with such overparameterization, the network should perfectly memorize the training data (overfit).\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation\n",
    "- For input `\"...\"`, the training data actually has **multiple valid outputs**:\n",
    "  - `e`, `o`, `a`, `u`, `s`\n",
    "- This happens because many names start with different characters, but all begin with the same context `\"...\"`.\n",
    "\n",
    "As a result:\n",
    "- The **same input has multiple outputs**.\n",
    "- The model cannot assign probability **1.0** to all of them at once.\n",
    "- Therefore, even in the overfitting scenario, the loss remains > 0.\n",
    "\n",
    "---\n",
    "\n",
    "💡 **Key Takeaway:**\n",
    "The loss doesn’t reach zero because of **label ambiguity** — multiple correct outputs exist for the same input context.\n"
   ],
   "id": "1c1559b8cf03ebf0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:03:49.314441Z",
     "start_time": "2025-08-26T14:03:49.309164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## our neural n/w has low loss because it overfits because we only gave 32 i/p and 32 o/ps and trained dour model for that we 3k parameters for 32 examples the network will overfit single bath and getting low loss\n",
    "\n",
    "'''\n",
    "why loss is not becoming is zero for over fitting case with so many parameters ?\n",
    "\n",
    "ans) in our training data for  i/p ... we have e, o, a, u, s as o/p, because all the starting character has ... in front of their names\n",
    "due to sAME I/P HAS MULTIPLE O/P OVER MODEL HAS STILL LOSS IN CASE OF OVER FITTING T0O\n",
    "\n",
    "    '''"
   ],
   "id": "270e32ad181c5461",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwhy loss is not becoming is zero for over fitting case with so many parameters ?\\n\\nans) in our training data for  i/p ... we have e, o, a, u, s as o/p, because all the starting character has ... in front of their names\\ndue to sAME I/P HAS MULTIPLE O/P OVER MODEL HAS STILL LOSS IN CASE OF OVER FITTING T0O\\n\\n    '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creating Dataset for all words",
   "id": "b7a013732385e992"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:08:03.717615Z",
     "start_time": "2025-08-26T14:08:03.106351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## data set for neural network\n",
    "\n",
    "block_size = 3 #Context_length: how many characters do we take to predict the next one?\n",
    "X,Y =[],[] # X are the input to neural network and Y are  the labels of the neural network\n",
    "\n",
    "for w in words:\n",
    "    context = [0] * block_size\n",
    "\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix]\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "X.shape, Y.shape"
   ],
   "id": "edc73b3a07465fa3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:08:05.339613Z",
     "start_time": "2025-08-26T14:08:05.334722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # this is used for generating same weights from the tutorial\n",
    "C = torch.randn((27,2), generator=g) #Embedding layer( we choosed 2 dimensional embedding for 27 charcters\n",
    "W1 = torch.randn((6,100), generator=g)\n",
    "B1 = torch.randn(100, generator = g)\n",
    "W2 = torch.randn((100,27), generator = g)\n",
    "B2 = torch.randn(27, generator = g)\n",
    "parameters = [C, W1, B1, W2, B2]"
   ],
   "id": "caf31249410bcab",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:08:09.289090Z",
     "start_time": "2025-08-26T14:08:09.284355Z"
    }
   },
   "cell_type": "code",
   "source": "sum(p.nelement() for p in parameters) # number of parameters in total",
   "id": "b11196a77d54c7e5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:08:09.750243Z",
     "start_time": "2025-08-26T14:08:09.747069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ],
   "id": "530c82fb78fd7f2",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:07:05.865126Z",
     "start_time": "2025-08-26T14:03:50.044036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# so every time we run this the weights keeps on updating\n",
    "for _ in range(1000):\n",
    "    # Forward pass\n",
    "    embedding = C[X] #(32,3,2)\n",
    "    Hidden_Layer = torch.tanh(embedding.view(-1,6)@ W1 + B1) #(32, 100)\n",
    "    logits = Hidden_Layer @ W2 + B2 # (32,27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update the weights\n",
    "    for p in parameters:\n",
    "        p.data = p.data + (-0.1 * p.grad)"
   ],
   "id": "2d7c3a08addca82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.505229949951172\n",
      "17.08448600769043\n",
      "15.776532173156738\n",
      "14.833340644836426\n",
      "14.002604484558105\n",
      "13.253260612487793\n",
      "12.579917907714844\n",
      "11.983101844787598\n",
      "11.47049331665039\n",
      "11.05185604095459\n",
      "10.709586143493652\n",
      "10.407631874084473\n",
      "10.127808570861816\n",
      "9.864364624023438\n",
      "9.614501953125\n",
      "9.37643814086914\n",
      "9.148943901062012\n",
      "8.931110382080078\n",
      "8.722230911254883\n",
      "8.521748542785645\n",
      "8.32922649383545\n",
      "8.144325256347656\n",
      "7.966790676116943\n",
      "7.796450138092041\n",
      "7.633184432983398\n",
      "7.476907730102539\n",
      "7.32751989364624\n",
      "7.184884548187256\n",
      "7.04879093170166\n",
      "6.918951988220215\n",
      "6.795018196105957\n",
      "6.676602840423584\n",
      "6.563317775726318\n",
      "6.454789161682129\n",
      "6.350668907165527\n",
      "6.250642776489258\n",
      "6.154431343078613\n",
      "6.061785697937012\n",
      "5.972482204437256\n",
      "5.886327743530273\n",
      "5.803146839141846\n",
      "5.722784042358398\n",
      "5.645094394683838\n",
      "5.569945335388184\n",
      "5.497212886810303\n",
      "5.4267802238464355\n",
      "5.358535289764404\n",
      "5.292375087738037\n",
      "5.228203296661377\n",
      "5.165928363800049\n",
      "5.105468273162842\n",
      "5.04674768447876\n",
      "4.98969841003418\n",
      "4.934260845184326\n",
      "4.880379676818848\n",
      "4.828005790710449\n",
      "4.777095794677734\n",
      "4.727609157562256\n",
      "4.679513454437256\n",
      "4.632777690887451\n",
      "4.587378025054932\n",
      "4.543288230895996\n",
      "4.500490665435791\n",
      "4.4589667320251465\n",
      "4.418700218200684\n",
      "4.379675388336182\n",
      "4.3418779373168945\n",
      "4.305292129516602\n",
      "4.269898891448975\n",
      "4.235680103302002\n",
      "4.202611923217773\n",
      "4.170670032501221\n",
      "4.139824867248535\n",
      "4.110044002532959\n",
      "4.081292629241943\n",
      "4.053531169891357\n",
      "4.02672004699707\n",
      "4.00081729888916\n",
      "3.9757797718048096\n",
      "3.951563596725464\n",
      "3.9281277656555176\n",
      "3.905430793762207\n",
      "3.8834331035614014\n",
      "3.8620986938476562\n",
      "3.8413913249969482\n",
      "3.821279764175415\n",
      "3.80173397064209\n",
      "3.782726764678955\n",
      "3.764232873916626\n",
      "3.746229410171509\n",
      "3.7286953926086426\n",
      "3.711611270904541\n",
      "3.6949589252471924\n",
      "3.678723096847534\n",
      "3.662886619567871\n",
      "3.647437334060669\n",
      "3.6323606967926025\n",
      "3.617645025253296\n",
      "3.603278160095215\n",
      "3.589249610900879\n",
      "3.575546979904175\n",
      "3.562162160873413\n",
      "3.549084424972534\n",
      "3.5363049507141113\n",
      "3.5238139629364014\n",
      "3.511603593826294\n",
      "3.499664783477783\n",
      "3.4879894256591797\n",
      "3.4765701293945312\n",
      "3.4653983116149902\n",
      "3.454468011856079\n",
      "3.4437708854675293\n",
      "3.433300495147705\n",
      "3.4230499267578125\n",
      "3.4130120277404785\n",
      "3.403181552886963\n",
      "3.39355206489563\n",
      "3.3841166496276855\n",
      "3.3748714923858643\n",
      "3.365809679031372\n",
      "3.356926679611206\n",
      "3.3482167720794678\n",
      "3.3396759033203125\n",
      "3.331298589706421\n",
      "3.3230810165405273\n",
      "3.3150179386138916\n",
      "3.3071062564849854\n",
      "3.2993409633636475\n",
      "3.291718006134033\n",
      "3.284233808517456\n",
      "3.276885509490967\n",
      "3.2696683406829834\n",
      "3.2625787258148193\n",
      "3.2556138038635254\n",
      "3.2487704753875732\n",
      "3.2420454025268555\n",
      "3.2354350090026855\n",
      "3.2289369106292725\n",
      "3.2225475311279297\n",
      "3.216264247894287\n",
      "3.2100844383239746\n",
      "3.204005002975464\n",
      "3.198023796081543\n",
      "3.1921374797821045\n",
      "3.1863441467285156\n",
      "3.1806414127349854\n",
      "3.1750266551971436\n",
      "3.169497013092041\n",
      "3.1640517711639404\n",
      "3.158687114715576\n",
      "3.1534016132354736\n",
      "3.148193597793579\n",
      "3.1430602073669434\n",
      "3.13800048828125\n",
      "3.13301157951355\n",
      "3.1280927658081055\n",
      "3.123241901397705\n",
      "3.1184568405151367\n",
      "3.113736629486084\n",
      "3.1090786457061768\n",
      "3.1044833660125732\n",
      "3.099947452545166\n",
      "3.095470428466797\n",
      "3.0910511016845703\n",
      "3.0866875648498535\n",
      "3.082379102706909\n",
      "3.078124761581421\n",
      "3.073923110961914\n",
      "3.0697731971740723\n",
      "3.065673589706421\n",
      "3.06162428855896\n",
      "3.0576233863830566\n",
      "3.053670883178711\n",
      "3.0497653484344482\n",
      "3.0459060668945312\n",
      "3.04209303855896\n",
      "3.038325071334839\n",
      "3.0346009731292725\n",
      "3.03092098236084\n",
      "3.0272841453552246\n",
      "3.0236899852752686\n",
      "3.0201377868652344\n",
      "3.016627788543701\n",
      "3.0131585597991943\n",
      "3.009730339050293\n",
      "3.0063421726226807\n",
      "3.0029942989349365\n",
      "2.9996860027313232\n",
      "2.9964168071746826\n",
      "2.9931869506835938\n",
      "2.9899954795837402\n",
      "2.986842393875122\n",
      "2.983726978302002\n",
      "2.980649471282959\n",
      "2.977609157562256\n",
      "2.9746062755584717\n",
      "2.971640110015869\n",
      "2.968709945678711\n",
      "2.9658164978027344\n",
      "2.962958574295044\n",
      "2.960136651992798\n",
      "2.9573493003845215\n",
      "2.954597234725952\n",
      "2.9518795013427734\n",
      "2.9491961002349854\n",
      "2.946547031402588\n",
      "2.9439311027526855\n",
      "2.9413483142852783\n",
      "2.938798427581787\n",
      "2.936281204223633\n",
      "2.933795928955078\n",
      "2.9313416481018066\n",
      "2.9289188385009766\n",
      "2.9265267848968506\n",
      "2.9241647720336914\n",
      "2.921832323074341\n",
      "2.919529676437378\n",
      "2.917254686355591\n",
      "2.915009021759033\n",
      "2.912790536880493\n",
      "2.9105992317199707\n",
      "2.9084346294403076\n",
      "2.9062960147857666\n",
      "2.9041836261749268\n",
      "2.9020957946777344\n",
      "2.9000332355499268\n",
      "2.89799427986145\n",
      "2.895979404449463\n",
      "2.8939876556396484\n",
      "2.8920180797576904\n",
      "2.890070676803589\n",
      "2.8881452083587646\n",
      "2.8862407207489014\n",
      "2.884357213973999\n",
      "2.8824942111968994\n",
      "2.880650758743286\n",
      "2.8788270950317383\n",
      "2.8770222663879395\n",
      "2.8752357959747314\n",
      "2.8734681606292725\n",
      "2.871718168258667\n",
      "2.8699862957000732\n",
      "2.8682713508605957\n",
      "2.8665735721588135\n",
      "2.8648927211761475\n",
      "2.863227605819702\n",
      "2.8615784645080566\n",
      "2.859945774078369\n",
      "2.858328342437744\n",
      "2.8567261695861816\n",
      "2.8551390171051025\n",
      "2.853566884994507\n",
      "2.852008581161499\n",
      "2.8504655361175537\n",
      "2.848935842514038\n",
      "2.847421169281006\n",
      "2.845919609069824\n",
      "2.844430923461914\n",
      "2.842956304550171\n",
      "2.8414947986602783\n",
      "2.8400464057922363\n",
      "2.8386106491088867\n",
      "2.8371877670288086\n",
      "2.835777521133423\n",
      "2.8343799114227295\n",
      "2.832993984222412\n",
      "2.831620216369629\n",
      "2.83025860786438\n",
      "2.828908920288086\n",
      "2.827571153640747\n",
      "2.826244592666626\n",
      "2.8249294757843018\n",
      "2.8236260414123535\n",
      "2.822333335876465\n",
      "2.821052074432373\n",
      "2.81978178024292\n",
      "2.8185222148895264\n",
      "2.8172736167907715\n",
      "2.816035509109497\n",
      "2.814807891845703\n",
      "2.813591241836548\n",
      "2.8123841285705566\n",
      "2.811187267303467\n",
      "2.8100011348724365\n",
      "2.8088245391845703\n",
      "2.8076579570770264\n",
      "2.8065009117126465\n",
      "2.805354118347168\n",
      "2.8042163848876953\n",
      "2.8030881881713867\n",
      "2.8019697666168213\n",
      "2.8008599281311035\n",
      "2.799760103225708\n",
      "2.798668622970581\n",
      "2.797586679458618\n",
      "2.796513319015503\n",
      "2.7954487800598145\n",
      "2.7943928241729736\n",
      "2.7933452129364014\n",
      "2.792307138442993\n",
      "2.791276454925537\n",
      "2.7902541160583496\n",
      "2.789240837097168\n",
      "2.7882344722747803\n",
      "2.7872369289398193\n",
      "2.7862470149993896\n",
      "2.7852654457092285\n",
      "2.7842907905578613\n",
      "2.7833240032196045\n",
      "2.782365560531616\n",
      "2.7814137935638428\n",
      "2.7804694175720215\n",
      "2.7795326709747314\n",
      "2.7786028385162354\n",
      "2.7776801586151123\n",
      "2.7767646312713623\n",
      "2.7758562564849854\n",
      "2.774954319000244\n",
      "2.7740590572357178\n",
      "2.7731707096099854\n",
      "2.772289276123047\n",
      "2.771413564682007\n",
      "2.77054500579834\n",
      "2.7696828842163086\n",
      "2.768826961517334\n",
      "2.767976999282837\n",
      "2.7671332359313965\n",
      "2.7662956714630127\n",
      "2.7654638290405273\n",
      "2.7646384239196777\n",
      "2.7638187408447266\n",
      "2.7630043029785156\n",
      "2.7621958255767822\n",
      "2.761392831802368\n",
      "2.7605960369110107\n",
      "2.7598042488098145\n",
      "2.7590177059173584\n",
      "2.758237361907959\n",
      "2.7574617862701416\n",
      "2.7566916942596436\n",
      "2.7559261322021484\n",
      "2.7551658153533936\n",
      "2.754411458969116\n",
      "2.7536606788635254\n",
      "2.752915859222412\n",
      "2.752176284790039\n",
      "2.7514405250549316\n",
      "2.7507097721099854\n",
      "2.749983549118042\n",
      "2.7492623329162598\n",
      "2.7485456466674805\n",
      "2.747833728790283\n",
      "2.7471251487731934\n",
      "2.746422290802002\n",
      "2.745723247528076\n",
      "2.745028495788574\n",
      "2.744338035583496\n",
      "2.743652105331421\n",
      "2.7429699897766113\n",
      "2.7422916889190674\n",
      "2.7416176795959473\n",
      "2.74094820022583\n",
      "2.7402822971343994\n",
      "2.7396202087402344\n",
      "2.738961935043335\n",
      "2.738307476043701\n",
      "2.737656831741333\n",
      "2.7370100021362305\n",
      "2.7363667488098145\n",
      "2.735726833343506\n",
      "2.735090970993042\n",
      "2.7344584465026855\n",
      "2.7338294982910156\n",
      "2.7332043647766113\n",
      "2.7325820922851562\n",
      "2.7319629192352295\n",
      "2.7313477993011475\n",
      "2.7307357788085938\n",
      "2.73012638092041\n",
      "2.729520797729492\n",
      "2.7289185523986816\n",
      "2.728318929672241\n",
      "2.727722644805908\n",
      "2.7271292209625244\n",
      "2.726538896560669\n",
      "2.7259511947631836\n",
      "2.7253668308258057\n",
      "2.724785327911377\n",
      "2.7242066860198975\n",
      "2.723630905151367\n",
      "2.723057746887207\n",
      "2.722487688064575\n",
      "2.721919536590576\n",
      "2.7213544845581055\n",
      "2.720792055130005\n",
      "2.7202320098876953\n",
      "2.719674825668335\n",
      "2.7191200256347656\n",
      "2.7185676097869873\n",
      "2.718017578125\n",
      "2.717470169067383\n",
      "2.7169249057769775\n",
      "2.7163820266723633\n",
      "2.715841770172119\n",
      "2.715303421020508\n",
      "2.7147674560546875\n",
      "2.7142333984375\n",
      "2.7137019634246826\n",
      "2.713172435760498\n",
      "2.7126450538635254\n",
      "2.7121193408966064\n",
      "2.7115964889526367\n",
      "2.7110750675201416\n",
      "2.7105557918548584\n",
      "2.710038185119629\n",
      "2.7095229625701904\n",
      "2.7090091705322266\n",
      "2.7084977626800537\n",
      "2.7079882621765137\n",
      "2.70747971534729\n",
      "2.7069737911224365\n",
      "2.7064692974090576\n",
      "2.705965995788574\n",
      "2.705465078353882\n",
      "2.704965591430664\n",
      "2.704467535018921\n",
      "2.7039716243743896\n",
      "2.703477144241333\n",
      "2.702983856201172\n",
      "2.7024919986724854\n",
      "2.7020022869110107\n",
      "2.7015132904052734\n",
      "2.701026201248169\n",
      "2.700540781021118\n",
      "2.700056314468384\n",
      "2.699573516845703\n",
      "2.699091911315918\n",
      "2.6986114978790283\n",
      "2.6981327533721924\n",
      "2.697655439376831\n",
      "2.697179079055786\n",
      "2.696704149246216\n",
      "2.696229934692383\n",
      "2.6957573890686035\n",
      "2.6952860355377197\n",
      "2.6948153972625732\n",
      "2.6943461894989014\n",
      "2.693878650665283\n",
      "2.693411350250244\n",
      "2.6929454803466797\n",
      "2.6924805641174316\n",
      "2.692017078399658\n",
      "2.691553831100464\n",
      "2.6910924911499023\n",
      "2.69063138961792\n",
      "2.6901721954345703\n",
      "2.6897132396698\n",
      "2.689255714416504\n",
      "2.688798666000366\n",
      "2.688343048095703\n",
      "2.6878879070281982\n",
      "2.6874337196350098\n",
      "2.6869800090789795\n",
      "2.686527967453003\n",
      "2.6860764026641846\n",
      "2.6856255531311035\n",
      "2.685175657272339\n",
      "2.6847267150878906\n",
      "2.6842780113220215\n",
      "2.683830976486206\n",
      "2.6833841800689697\n",
      "2.68293833732605\n",
      "2.682492971420288\n",
      "2.6820485591888428\n",
      "2.6816048622131348\n",
      "2.681161880493164\n",
      "2.6807198524475098\n",
      "2.6802785396575928\n",
      "2.679837703704834\n",
      "2.6793973445892334\n",
      "2.678957939147949\n",
      "2.6785194873809814\n",
      "2.6780810356140137\n",
      "2.6776435375213623\n",
      "2.6772072315216064\n",
      "2.6767711639404297\n",
      "2.676335573196411\n",
      "2.67590069770813\n",
      "2.675466537475586\n",
      "2.6750328540802\n",
      "2.67460036277771\n",
      "2.674168109893799\n",
      "2.673736095428467\n",
      "2.673305034637451\n",
      "2.672874927520752\n",
      "2.672445058822632\n",
      "2.672015905380249\n",
      "2.6715874671936035\n",
      "2.671159267425537\n",
      "2.670732259750366\n",
      "2.6703054904937744\n",
      "2.66987943649292\n",
      "2.6694533824920654\n",
      "2.6690289974212646\n",
      "2.6686043739318848\n",
      "2.6681809425354004\n",
      "2.667757987976074\n",
      "2.6673355102539062\n",
      "2.6669139862060547\n",
      "2.6664927005767822\n",
      "2.666071891784668\n",
      "2.665651798248291\n",
      "2.6652326583862305\n",
      "2.664813756942749\n",
      "2.664395570755005\n",
      "2.663978099822998\n",
      "2.6635611057281494\n",
      "2.663144826889038\n",
      "2.662729024887085\n",
      "2.6623144149780273\n",
      "2.6618998050689697\n",
      "2.6614859104156494\n",
      "2.6610732078552246\n",
      "2.6606605052948\n",
      "2.6602485179901123\n",
      "2.6598377227783203\n",
      "2.6594271659851074\n",
      "2.659017324447632\n",
      "2.6586084365844727\n",
      "2.6581997871398926\n",
      "2.657792329788208\n",
      "2.6573848724365234\n",
      "2.6569786071777344\n",
      "2.6565730571746826\n",
      "2.656167984008789\n",
      "2.655764102935791\n",
      "2.655360460281372\n",
      "2.6549580097198486\n",
      "2.6545557975769043\n",
      "2.6541545391082764\n",
      "2.653754234313965\n",
      "2.6533544063568115\n",
      "2.6529552936553955\n",
      "2.652557134628296\n",
      "2.6521596908569336\n",
      "2.6517629623413086\n",
      "2.6513671875\n",
      "2.6509721279144287\n",
      "2.6505775451660156\n",
      "2.650184154510498\n",
      "2.649791955947876\n",
      "2.649399995803833\n",
      "2.6490089893341064\n",
      "2.6486189365386963\n",
      "2.6482295989990234\n",
      "2.647841215133667\n",
      "2.647453546524048\n",
      "2.647067070007324\n",
      "2.646681308746338\n",
      "2.646296501159668\n",
      "2.6459124088287354\n",
      "2.645529270172119\n",
      "2.6451468467712402\n",
      "2.644766092300415\n",
      "2.644386053085327\n",
      "2.6440064907073975\n",
      "2.6436281204223633\n",
      "2.6432504653930664\n",
      "2.642874002456665\n",
      "2.642498731613159\n",
      "2.6421239376068115\n",
      "2.6417505741119385\n",
      "2.6413779258728027\n",
      "2.6410062313079834\n",
      "2.6406357288360596\n",
      "2.640265941619873\n",
      "2.639897584915161\n",
      "2.6395301818847656\n",
      "2.6391632556915283\n",
      "2.638798236846924\n",
      "2.6384339332580566\n",
      "2.6380701065063477\n",
      "2.6377079486846924\n",
      "2.6373469829559326\n",
      "2.636986494064331\n",
      "2.636627674102783\n",
      "2.6362695693969727\n",
      "2.6359126567840576\n",
      "2.635557174682617\n",
      "2.635202407836914\n",
      "2.6348490715026855\n",
      "2.6344964504241943\n",
      "2.634145498275757\n",
      "2.6337950229644775\n",
      "2.633445978164673\n",
      "2.6330981254577637\n",
      "2.632751226425171\n",
      "2.6324055194854736\n",
      "2.632061004638672\n",
      "2.6317176818847656\n",
      "2.631376028060913\n",
      "2.6310348510742188\n",
      "2.630695104598999\n",
      "2.630356550216675\n",
      "2.630019187927246\n",
      "2.6296825408935547\n",
      "2.629347562789917\n",
      "2.6290135383605957\n",
      "2.628680944442749\n",
      "2.628349542617798\n",
      "2.628019094467163\n",
      "2.627690076828003\n",
      "2.6273622512817383\n",
      "2.627035617828369\n",
      "2.6267104148864746\n",
      "2.6263864040374756\n",
      "2.626063346862793\n",
      "2.625741958618164\n",
      "2.6254210472106934\n",
      "2.6251015663146973\n",
      "2.624783515930176\n",
      "2.624466896057129\n",
      "2.6241512298583984\n",
      "2.6238369941711426\n",
      "2.6235239505767822\n",
      "2.6232123374938965\n",
      "2.622901201248169\n",
      "2.622591972351074\n",
      "2.622283458709717\n",
      "2.621976613998413\n",
      "2.621670961380005\n",
      "2.621366262435913\n",
      "2.621063709259033\n",
      "2.6207613945007324\n",
      "2.6204607486724854\n",
      "2.620161533355713\n",
      "2.619863271713257\n",
      "2.6195662021636963\n",
      "2.6192703247070312\n",
      "2.618975877761841\n",
      "2.618682861328125\n",
      "2.6183910369873047\n",
      "2.618100166320801\n",
      "2.6178102493286133\n",
      "2.6175220012664795\n",
      "2.617234945297241\n",
      "2.6169493198394775\n",
      "2.6166648864746094\n",
      "2.6163814067840576\n",
      "2.6160991191864014\n",
      "2.6158182621002197\n",
      "2.6155383586883545\n",
      "2.615260124206543\n",
      "2.614982843399048\n",
      "2.614706516265869\n",
      "2.614431858062744\n",
      "2.6141586303710938\n",
      "2.6138858795166016\n",
      "2.613614797592163\n",
      "2.61334490776062\n",
      "2.6130759716033936\n",
      "2.6128084659576416\n",
      "2.612541913986206\n",
      "2.612277030944824\n",
      "2.6120126247406006\n",
      "2.6117498874664307\n",
      "2.611488103866577\n",
      "2.6112277507781982\n",
      "2.6109683513641357\n",
      "2.6107101440429688\n",
      "2.6104533672332764\n",
      "2.6101977825164795\n",
      "2.609943389892578\n",
      "2.609689712524414\n",
      "2.6094369888305664\n",
      "2.6091861724853516\n",
      "2.608935832977295\n",
      "2.608686685562134\n",
      "2.608438730239868\n",
      "2.608192205429077\n",
      "2.6079463958740234\n",
      "2.6077017784118652\n",
      "2.6074585914611816\n",
      "2.6072163581848145\n",
      "2.6069750785827637\n",
      "2.6067347526550293\n",
      "2.6064958572387695\n",
      "2.6062581539154053\n",
      "2.6060211658477783\n",
      "2.605785369873047\n",
      "2.605550527572632\n",
      "2.6053168773651123\n",
      "2.605084180831909\n",
      "2.6048524379730225\n",
      "2.6046218872070312\n",
      "2.6043922901153564\n",
      "2.604163885116577\n",
      "2.6039364337921143\n",
      "2.6037099361419678\n",
      "2.603484630584717\n",
      "2.603259801864624\n",
      "2.603036880493164\n",
      "2.602814197540283\n",
      "2.602592706680298\n",
      "2.602372169494629\n",
      "2.6021530628204346\n",
      "2.6019341945648193\n",
      "2.6017162799835205\n",
      "2.6015002727508545\n",
      "2.6012840270996094\n",
      "2.601069688796997\n",
      "2.600855588912964\n",
      "2.600642681121826\n",
      "2.600430965423584\n",
      "2.6002197265625\n",
      "2.6000096797943115\n",
      "2.5998005867004395\n",
      "2.5995919704437256\n",
      "2.5993845462799072\n",
      "2.5991780757904053\n",
      "2.5989725589752197\n",
      "2.5987679958343506\n",
      "2.5985639095306396\n",
      "2.598360776901245\n",
      "2.598158359527588\n",
      "2.597956895828247\n",
      "2.597756862640381\n",
      "2.5975568294525146\n",
      "2.597357988357544\n",
      "2.5971598625183105\n",
      "2.5969626903533936\n",
      "2.596766233444214\n",
      "2.5965707302093506\n",
      "2.5963757038116455\n",
      "2.596181869506836\n",
      "2.5959887504577637\n",
      "2.5957961082458496\n",
      "2.595604658126831\n",
      "2.5954136848449707\n",
      "2.5952231884002686\n",
      "2.595033884048462\n",
      "2.5948452949523926\n",
      "2.5946576595306396\n",
      "2.594470262527466\n",
      "2.5942838191986084\n",
      "2.5940988063812256\n",
      "2.5939135551452637\n",
      "2.593729257583618\n",
      "2.59354567527771\n",
      "2.59336256980896\n",
      "2.5931804180145264\n",
      "2.592999219894409\n",
      "2.59281849861145\n",
      "2.5926382541656494\n",
      "2.592458963394165\n",
      "2.592280387878418\n",
      "2.59210205078125\n",
      "2.5919244289398193\n",
      "2.591747760772705\n",
      "2.591571569442749\n",
      "2.591395854949951\n",
      "2.591221570968628\n",
      "2.591047525405884\n",
      "2.5908737182617188\n",
      "2.590700626373291\n",
      "2.590528726577759\n",
      "2.5903568267822266\n",
      "2.5901856422424316\n",
      "2.590015172958374\n",
      "2.5898451805114746\n",
      "2.5896761417388916\n",
      "2.5895073413848877\n",
      "2.589339017868042\n",
      "2.5891714096069336\n",
      "2.5890047550201416\n",
      "2.5888383388519287\n",
      "2.588672637939453\n",
      "2.5885071754455566\n",
      "2.5883426666259766\n",
      "2.5881783962249756\n",
      "2.588014841079712\n",
      "2.5878515243530273\n",
      "2.5876893997192383\n",
      "2.587527275085449\n",
      "2.5873658657073975\n",
      "2.587205171585083\n",
      "2.5870444774627686\n",
      "2.5868849754333496\n",
      "2.5867257118225098\n",
      "2.58656644821167\n",
      "2.5864081382751465\n",
      "2.5862503051757812\n",
      "2.5860931873321533\n",
      "2.5859363079071045\n",
      "2.585780143737793\n",
      "2.5856244564056396\n",
      "2.5854690074920654\n",
      "2.5853142738342285\n",
      "2.58516001701355\n",
      "2.5850062370300293\n",
      "2.584852695465088\n",
      "2.584699869155884\n",
      "2.584547281265259\n",
      "2.584395408630371\n",
      "2.5842440128326416\n",
      "2.584092855453491\n",
      "2.583942174911499\n",
      "2.583791971206665\n",
      "2.5836422443389893\n",
      "2.5834927558898926\n",
      "2.5833442211151123\n",
      "2.583195686340332\n",
      "2.58304762840271\n",
      "2.582900047302246\n",
      "2.5827529430389404\n",
      "2.582606315612793\n",
      "2.5824601650238037\n",
      "2.5823137760162354\n",
      "2.5821688175201416\n",
      "2.5820236206054688\n",
      "2.581879138946533\n",
      "2.581735134124756\n",
      "2.5815913677215576\n",
      "2.5814478397369385\n",
      "2.5813047885894775\n",
      "2.581162691116333\n",
      "2.5810201168060303\n",
      "2.580878257751465\n",
      "2.5807368755340576\n",
      "2.5805962085723877\n",
      "2.5804555416107178\n",
      "2.580315351486206\n",
      "2.5801753997802734\n",
      "2.580036163330078\n",
      "2.579897403717041\n",
      "2.579758405685425\n",
      "2.579620122909546\n",
      "2.579482078552246\n",
      "2.5793445110321045\n",
      "2.579207420349121\n",
      "2.579070568084717\n",
      "2.5789341926574707\n",
      "2.5787975788116455\n",
      "2.578662157058716\n",
      "2.578526735305786\n",
      "2.5783917903900146\n",
      "2.5782570838928223\n",
      "2.578122615814209\n",
      "2.577988624572754\n",
      "2.577855110168457\n",
      "2.5777218341827393\n",
      "2.5775890350341797\n",
      "2.57745623588562\n",
      "2.577324151992798\n",
      "2.5771920680999756\n",
      "2.5770604610443115\n",
      "2.5769293308258057\n",
      "2.576798439025879\n",
      "2.5766677856445312\n",
      "2.576537609100342\n",
      "2.5764076709747314\n",
      "2.5762782096862793\n",
      "2.5761489868164062\n",
      "2.5760202407836914\n",
      "2.5758912563323975\n",
      "2.57576322555542\n",
      "2.5756351947784424\n",
      "2.575507640838623\n",
      "2.575380563735962\n",
      "2.5752532482147217\n",
      "2.575126886367798\n",
      "2.575000286102295\n",
      "2.57487416267395\n",
      "2.5747485160827637\n",
      "2.5746233463287354\n",
      "2.574497699737549\n",
      "2.5743727684020996\n",
      "2.5742485523223877\n",
      "2.574124336242676\n",
      "2.574000358581543\n",
      "2.5738768577575684\n",
      "2.5737531185150146\n",
      "2.5736300945281982\n",
      "2.57350754737854\n",
      "2.573385000228882\n",
      "2.5732624530792236\n",
      "2.573140859603882\n",
      "2.57301926612854\n",
      "2.5728981494903564\n",
      "2.5727767944335938\n",
      "2.5726561546325684\n",
      "2.572535753250122\n",
      "2.572415828704834\n",
      "2.572295904159546\n",
      "2.572175979614258\n",
      "2.5720572471618652\n",
      "2.5719377994537354\n",
      "2.5718190670013428\n",
      "2.5717008113861084\n",
      "2.571582555770874\n",
      "2.5714645385742188\n",
      "2.5713467597961426\n",
      "2.5712292194366455\n",
      "2.5711123943328857\n",
      "2.570995330810547\n",
      "2.5708789825439453\n",
      "2.5707626342773438\n",
      "2.5706467628479004\n",
      "2.570530652999878\n",
      "2.5704152584075928\n",
      "2.5702998638153076\n",
      "2.5701847076416016\n",
      "2.5700700283050537\n",
      "2.569955825805664\n",
      "2.569841146469116\n",
      "2.569727659225464\n",
      "2.5696139335632324\n",
      "2.56950044631958\n",
      "2.5693869590759277\n",
      "2.5692741870880127\n",
      "2.5691616535186768\n",
      "2.5690488815307617\n",
      "2.568936586380005\n",
      "2.5688250064849854\n",
      "2.5687129497528076\n",
      "2.568601608276367\n",
      "2.568490505218506\n",
      "2.5683794021606445\n",
      "2.5682687759399414\n",
      "2.5681583881378174\n",
      "2.5680482387542725\n",
      "2.5679380893707275\n",
      "2.5678281784057617\n",
      "2.567718982696533\n",
      "2.5676095485687256\n",
      "2.567500591278076\n",
      "2.5673916339874268\n",
      "2.5672831535339355\n",
      "2.5671746730804443\n",
      "2.5670666694641113\n",
      "2.5669586658477783\n",
      "2.5668511390686035\n",
      "2.5667436122894287\n",
      "2.566636562347412\n",
      "2.5665295124053955\n",
      "2.566422700881958\n",
      "2.5663161277770996\n",
      "2.5662100315093994\n",
      "2.56610369682312\n",
      "2.565997838973999\n",
      "2.565892219543457\n",
      "2.5657870769500732\n",
      "2.5656819343566895\n",
      "2.5655767917633057\n",
      "2.56547212600708\n",
      "2.5653676986694336\n",
      "2.565263032913208\n",
      "2.565159320831299\n",
      "2.5650551319122314\n",
      "2.5649516582489014\n",
      "2.5648481845855713\n",
      "2.5647449493408203\n",
      "2.5646417140960693\n",
      "2.5645389556884766\n",
      "2.564436197280884\n",
      "2.564333915710449\n",
      "2.5642316341400146\n",
      "2.5641298294067383\n",
      "2.564028024673462\n",
      "2.5639262199401855\n",
      "2.5638251304626465\n",
      "2.5637240409851074\n",
      "2.5636229515075684\n",
      "2.5635221004486084\n",
      "2.5634214878082275\n",
      "2.563321352005005\n",
      "2.563220977783203\n",
      "2.5631213188171387\n",
      "2.563020944595337\n",
      "2.5629217624664307\n",
      "2.5628223419189453\n",
      "2.562723159790039\n",
      "2.562624454498291\n",
      "2.562525749206543\n",
      "2.562426805496216\n",
      "2.562328338623047\n",
      "2.562230110168457\n",
      "2.5621321201324463\n",
      "2.5620343685150146\n",
      "2.561936378479004\n",
      "2.5618393421173096\n",
      "2.561741828918457\n",
      "2.5616445541381836\n",
      "2.5615475177764893\n",
      "2.561450958251953\n",
      "2.561354398727417\n",
      "2.561257839202881\n",
      "2.561161756515503\n",
      "2.561065912246704\n",
      "2.5609700679779053\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:08:53.147263Z",
     "start_time": "2025-08-26T14:08:53.141260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## So training taking long time so in poytorch generally people will consider lot of mini batches and process them in to multiple small batche s\n",
    "\n",
    "## to get mini batch we will do this\n",
    "torch.randint(0, X.shape[0], (32,))"
   ],
   "id": "87c6646e5116f8dc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([118809, 131466, 142978, 116205,  99844,  81206,  40385,  11601, 212580,\n",
       "        194611,  69417, 220753,  85972, 195724, 122620,  95672, 134246, 199022,\n",
       "         19965,  82163,  90983,  66995,  73478, 210010,  66414,  80576, 203972,\n",
       "         78668,  50288, 218552, 136920, 207809])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:08:53.574154Z",
     "start_time": "2025-08-26T14:08:53.570079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ],
   "id": "d4770bb5ba9cf473",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:08:54.284541Z",
     "start_time": "2025-08-26T14:08:54.124400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# so every time we run this the weights keeps on updating\n",
    "\n",
    "for _ in range(200):\n",
    "\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    # Forward pass\n",
    "    embedding = C[X[ix]] #(32,3,2)\n",
    "    Hidden_Layer = torch.tanh(embedding.view(-1,6)@ W1 + B1) #(32, 100)\n",
    "    logits = Hidden_Layer @ W2 + B2 # (32,27)\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update the weights\n",
    "    for p in parameters:\n",
    "        p.data = p.data + (-0.1 * p.grad)\n"
   ],
   "id": "263ab752c89bffa2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.34398078918457\n",
      "17.964786529541016\n",
      "16.328784942626953\n",
      "18.694889068603516\n",
      "13.0975923538208\n",
      "14.475982666015625\n",
      "9.109421730041504\n",
      "13.121930122375488\n",
      "14.217618942260742\n",
      "9.671998977661133\n",
      "12.852156639099121\n",
      "11.131784439086914\n",
      "12.090120315551758\n",
      "10.389299392700195\n",
      "10.917837142944336\n",
      "10.190481185913086\n",
      "10.285539627075195\n",
      "7.289907455444336\n",
      "9.178886413574219\n",
      "10.240134239196777\n",
      "7.553822040557861\n",
      "8.215204238891602\n",
      "8.586637496948242\n",
      "6.6284661293029785\n",
      "7.821995258331299\n",
      "7.593568325042725\n",
      "10.05458927154541\n",
      "8.569539070129395\n",
      "6.398820877075195\n",
      "7.125687599182129\n",
      "6.995219707489014\n",
      "6.965597152709961\n",
      "6.547895431518555\n",
      "6.689959526062012\n",
      "7.625567436218262\n",
      "5.503087043762207\n",
      "6.205079078674316\n",
      "7.662099838256836\n",
      "6.149379253387451\n",
      "5.340783596038818\n",
      "7.977917671203613\n",
      "5.2957234382629395\n",
      "4.969181060791016\n",
      "5.186641216278076\n",
      "4.532034397125244\n",
      "4.603589057922363\n",
      "5.450834274291992\n",
      "7.379859924316406\n",
      "5.111886024475098\n",
      "4.571963310241699\n",
      "4.2693939208984375\n",
      "5.7333455085754395\n",
      "5.3730692863464355\n",
      "5.226903438568115\n",
      "3.6863789558410645\n",
      "5.065442085266113\n",
      "5.2442545890808105\n",
      "4.674819469451904\n",
      "4.521739482879639\n",
      "4.689345359802246\n",
      "5.206523895263672\n",
      "4.166417121887207\n",
      "4.055726528167725\n",
      "4.381852149963379\n",
      "4.87408447265625\n",
      "4.190678119659424\n",
      "3.903541326522827\n",
      "4.17098331451416\n",
      "4.5202436447143555\n",
      "4.561328887939453\n",
      "5.238063812255859\n",
      "4.182966709136963\n",
      "3.443025588989258\n",
      "3.5620102882385254\n",
      "3.9487528800964355\n",
      "6.228381633758545\n",
      "5.017297267913818\n",
      "4.759073734283447\n",
      "4.006087779998779\n",
      "3.598118305206299\n",
      "4.036763668060303\n",
      "4.029749870300293\n",
      "4.1989898681640625\n",
      "2.5182180404663086\n",
      "3.7528154850006104\n",
      "3.631039619445801\n",
      "3.345097303390503\n",
      "3.5593576431274414\n",
      "3.4035520553588867\n",
      "5.161469459533691\n",
      "3.7970449924468994\n",
      "5.045498847961426\n",
      "3.664668083190918\n",
      "4.1656975746154785\n",
      "3.837082624435425\n",
      "3.5166804790496826\n",
      "3.9123728275299072\n",
      "3.189908266067505\n",
      "4.2215657234191895\n",
      "4.351047039031982\n",
      "2.965588092803955\n",
      "3.5528199672698975\n",
      "3.5439438819885254\n",
      "4.1594438552856445\n",
      "3.234767436981201\n",
      "3.95832896232605\n",
      "3.8330304622650146\n",
      "3.485069990158081\n",
      "3.6075921058654785\n",
      "4.375840187072754\n",
      "3.092015027999878\n",
      "3.610548257827759\n",
      "3.6062963008880615\n",
      "4.863180160522461\n",
      "4.294458866119385\n",
      "3.0756421089172363\n",
      "3.494994878768921\n",
      "3.906907081604004\n",
      "3.5921201705932617\n",
      "3.5103840827941895\n",
      "3.642282247543335\n",
      "2.918332576751709\n",
      "3.3653173446655273\n",
      "2.7811012268066406\n",
      "3.662604331970215\n",
      "3.017451763153076\n",
      "3.2915284633636475\n",
      "3.7802889347076416\n",
      "2.9540047645568848\n",
      "3.3019652366638184\n",
      "3.0171899795532227\n",
      "3.1331069469451904\n",
      "3.1722264289855957\n",
      "2.9501876831054688\n",
      "3.0287342071533203\n",
      "4.311925411224365\n",
      "3.9900286197662354\n",
      "3.3848557472229004\n",
      "3.3511123657226562\n",
      "3.2969303131103516\n",
      "3.9179983139038086\n",
      "2.952469825744629\n",
      "3.402163028717041\n",
      "3.403862237930298\n",
      "2.2508435249328613\n",
      "3.3665812015533447\n",
      "3.1830828189849854\n",
      "2.80462646484375\n",
      "3.9023396968841553\n",
      "4.160982608795166\n",
      "2.6742358207702637\n",
      "3.2166850566864014\n",
      "4.057038307189941\n",
      "4.067079544067383\n",
      "3.21905255317688\n",
      "2.933583974838257\n",
      "3.4204211235046387\n",
      "2.9959535598754883\n",
      "3.0617966651916504\n",
      "2.566103458404541\n",
      "2.6588947772979736\n",
      "2.979609251022339\n",
      "2.951653003692627\n",
      "3.409679651260376\n",
      "3.703709840774536\n",
      "3.992678165435791\n",
      "2.8641302585601807\n",
      "3.0652260780334473\n",
      "2.9400148391723633\n",
      "3.7610292434692383\n",
      "3.721278667449951\n",
      "3.091501235961914\n",
      "3.4485809803009033\n",
      "3.501934051513672\n",
      "3.02258563041687\n",
      "2.391263961791992\n",
      "3.688568592071533\n",
      "2.6171321868896484\n",
      "3.551053524017334\n",
      "2.814971923828125\n",
      "3.186774969100952\n",
      "3.5309500694274902\n",
      "3.736992120742798\n",
      "2.928997278213501\n",
      "3.5386366844177246\n",
      "2.954390525817871\n",
      "3.324803590774536\n",
      "3.5147814750671387\n",
      "3.243548631668091\n",
      "3.0827832221984863\n",
      "3.3782358169555664\n",
      "3.279817819595337\n",
      "3.250383138656616\n",
      "2.992722272872925\n",
      "3.4582505226135254\n",
      "3.3734450340270996\n",
      "3.460383653640747\n",
      "3.166069746017456\n",
      "3.3010141849517822\n",
      "2.3311972618103027\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:09:02.327526Z",
     "start_time": "2025-08-26T14:09:02.235690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#loss of entire model or loss of whole training data set\n",
    "embedding = C[X] #(32,3,2)\n",
    "Hidden_Layer = torch.tanh(embedding.view(-1,6)@ W1 + B1) #(32, 100)\n",
    "logits = Hidden_Layer @ W2 + B2 # (32,27)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ],
   "id": "51ff2cb33b21c2ee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0443, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:09:02.901342Z",
     "start_time": "2025-08-26T14:09:02.889291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Finding the optimal Learning rate\n",
    "learning_rates = 10**(torch.linspace(-3,0,1000))\n",
    "learning_rates"
   ],
   "id": "5f82b3ed088012e9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n",
       "        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n",
       "        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
       "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
       "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
       "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n",
       "        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n",
       "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n",
       "        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n",
       "        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
       "        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n",
       "        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n",
       "        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
       "        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n",
       "        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
       "        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n",
       "        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n",
       "        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n",
       "        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n",
       "        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n",
       "        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n",
       "        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n",
       "        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n",
       "        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n",
       "        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n",
       "        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n",
       "        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n",
       "        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n",
       "        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n",
       "        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n",
       "        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n",
       "        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n",
       "        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n",
       "        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n",
       "        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n",
       "        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n",
       "        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n",
       "        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n",
       "        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n",
       "        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n",
       "        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n",
       "        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n",
       "        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n",
       "        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n",
       "        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n",
       "        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n",
       "        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n",
       "        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n",
       "        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n",
       "        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n",
       "        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n",
       "        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n",
       "        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n",
       "        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n",
       "        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n",
       "        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n",
       "        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n",
       "        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n",
       "        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n",
       "        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n",
       "        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n",
       "        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n",
       "        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n",
       "        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n",
       "        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n",
       "        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n",
       "        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n",
       "        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n",
       "        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n",
       "        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n",
       "        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n",
       "        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n",
       "        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n",
       "        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n",
       "        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n",
       "        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n",
       "        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n",
       "        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n",
       "        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n",
       "        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n",
       "        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n",
       "        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n",
       "        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n",
       "        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n",
       "        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n",
       "        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n",
       "        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n",
       "        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n",
       "        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n",
       "        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n",
       "        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n",
       "        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n",
       "        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n",
       "        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n",
       "        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n",
       "        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n",
       "        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n",
       "        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n",
       "        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n",
       "        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n",
       "        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n",
       "        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n",
       "        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n",
       "        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n",
       "        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n",
       "        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:09:04.245913Z",
     "start_time": "2025-08-26T14:09:04.170894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# so every time we run this the weights keeps on updating\n",
    "lri = []\n",
    "lossi = []\n",
    "\n",
    "for i in range(100):\n",
    "    #mini batch\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "\n",
    "    # Forward pass\n",
    "    embedding = C[X[ix]] #(32,3,2)\n",
    "    Hidden_Layer = torch.tanh(embedding.view(-1,6)@ W1 + B1) #(32, 100)\n",
    "    logits = Hidden_Layer @ W2 + B2 # (32,27)\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update the weights\n",
    "    lr = learning_rates[i]\n",
    "    for p in parameters:\n",
    "        p.data = p.data + (-lr * p.grad)\n",
    "    lri.append(lr)\n",
    "    lossi.append(loss.item())\n"
   ],
   "id": "1e2f84f56608c445",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.927851438522339\n",
      "3.3382904529571533\n",
      "2.636394739151001\n",
      "3.6531147956848145\n",
      "3.331305980682373\n",
      "3.125523328781128\n",
      "3.890500068664551\n",
      "3.4903457164764404\n",
      "2.903484582901001\n",
      "3.79121470451355\n",
      "3.593137741088867\n",
      "3.19089937210083\n",
      "2.6625466346740723\n",
      "2.660254955291748\n",
      "3.3850786685943604\n",
      "3.116486072540283\n",
      "3.2435691356658936\n",
      "2.93933367729187\n",
      "2.921928882598877\n",
      "3.1994268894195557\n",
      "2.3581323623657227\n",
      "3.3626620769500732\n",
      "2.854327917098999\n",
      "2.90183687210083\n",
      "2.844917058944702\n",
      "3.084699869155884\n",
      "3.051894426345825\n",
      "3.3879318237304688\n",
      "3.1668529510498047\n",
      "3.273350715637207\n",
      "2.948725938796997\n",
      "2.891364336013794\n",
      "3.5089242458343506\n",
      "2.650442361831665\n",
      "3.7256767749786377\n",
      "2.6915605068206787\n",
      "3.3862202167510986\n",
      "2.7049989700317383\n",
      "2.93994402885437\n",
      "3.156604528427124\n",
      "2.9215140342712402\n",
      "2.865410566329956\n",
      "2.65971302986145\n",
      "3.352051258087158\n",
      "3.386953353881836\n",
      "3.0387279987335205\n",
      "2.7468276023864746\n",
      "3.0976877212524414\n",
      "3.3479743003845215\n",
      "2.5291295051574707\n",
      "3.0160272121429443\n",
      "2.9575741291046143\n",
      "2.810072660446167\n",
      "3.2960474491119385\n",
      "2.5438954830169678\n",
      "2.75771427154541\n",
      "2.7998595237731934\n",
      "3.021228313446045\n",
      "3.0009379386901855\n",
      "2.7393829822540283\n",
      "2.973546028137207\n",
      "4.029396057128906\n",
      "2.7535550594329834\n",
      "3.090130090713501\n",
      "2.3824613094329834\n",
      "3.101033926010132\n",
      "3.2685539722442627\n",
      "2.6576690673828125\n",
      "3.099426507949829\n",
      "3.422434091567993\n",
      "3.3158857822418213\n",
      "3.1830387115478516\n",
      "3.115809917449951\n",
      "3.3220326900482178\n",
      "3.50785756111145\n",
      "3.165654182434082\n",
      "3.200552463531494\n",
      "2.723313093185425\n",
      "3.103822946548462\n",
      "3.131115198135376\n",
      "3.7600345611572266\n",
      "2.798485040664673\n",
      "2.448293924331665\n",
      "2.878411054611206\n",
      "2.6155285835266113\n",
      "3.158353328704834\n",
      "2.326446294784546\n",
      "2.9535129070281982\n",
      "2.9886903762817383\n",
      "2.689861297607422\n",
      "3.600252866744995\n",
      "2.444408655166626\n",
      "2.65507435798645\n",
      "3.127960443496704\n",
      "3.2712936401367188\n",
      "2.926226854324341\n",
      "2.8789994716644287\n",
      "3.7485175132751465\n",
      "3.058479070663452\n",
      "2.8142809867858887\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:09:05.234913Z",
     "start_time": "2025-08-26T14:09:05.117601Z"
    }
   },
   "cell_type": "code",
   "source": "plt.plot(lri, lossi)",
   "id": "5bf4410ec2ba0a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x220f6af3080>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:09:08.607210Z",
     "start_time": "2025-08-26T14:09:08.580794Z"
    }
   },
   "cell_type": "code",
   "source": "lri",
   "id": "12d960bd637ea755",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0010),\n",
       " tensor(0.0010),\n",
       " tensor(0.0010),\n",
       " tensor(0.0010),\n",
       " tensor(0.0010),\n",
       " tensor(0.0010),\n",
       " tensor(0.0010),\n",
       " tensor(0.0010),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0013),\n",
       " tensor(0.0013),\n",
       " tensor(0.0013),\n",
       " tensor(0.0013),\n",
       " tensor(0.0013),\n",
       " tensor(0.0013),\n",
       " tensor(0.0013),\n",
       " tensor(0.0013),\n",
       " tensor(0.0013),\n",
       " tensor(0.0013),\n",
       " tensor(0.0013),\n",
       " tensor(0.0014),\n",
       " tensor(0.0014),\n",
       " tensor(0.0014),\n",
       " tensor(0.0014),\n",
       " tensor(0.0014),\n",
       " tensor(0.0014),\n",
       " tensor(0.0014),\n",
       " tensor(0.0014),\n",
       " tensor(0.0014),\n",
       " tensor(0.0014),\n",
       " tensor(0.0015),\n",
       " tensor(0.0015),\n",
       " tensor(0.0015),\n",
       " tensor(0.0015),\n",
       " tensor(0.0015),\n",
       " tensor(0.0015),\n",
       " tensor(0.0015),\n",
       " tensor(0.0015),\n",
       " tensor(0.0015),\n",
       " tensor(0.0015),\n",
       " tensor(0.0016),\n",
       " tensor(0.0016),\n",
       " tensor(0.0016),\n",
       " tensor(0.0016),\n",
       " tensor(0.0016),\n",
       " tensor(0.0016),\n",
       " tensor(0.0016),\n",
       " tensor(0.0016),\n",
       " tensor(0.0016),\n",
       " tensor(0.0017),\n",
       " tensor(0.0017),\n",
       " tensor(0.0017),\n",
       " tensor(0.0017),\n",
       " tensor(0.0017),\n",
       " tensor(0.0017),\n",
       " tensor(0.0017),\n",
       " tensor(0.0017),\n",
       " tensor(0.0018),\n",
       " tensor(0.0018),\n",
       " tensor(0.0018),\n",
       " tensor(0.0018),\n",
       " tensor(0.0018),\n",
       " tensor(0.0018),\n",
       " tensor(0.0018),\n",
       " tensor(0.0018),\n",
       " tensor(0.0019),\n",
       " tensor(0.0019),\n",
       " tensor(0.0019),\n",
       " tensor(0.0019),\n",
       " tensor(0.0019),\n",
       " tensor(0.0019),\n",
       " tensor(0.0019),\n",
       " tensor(0.0019),\n",
       " tensor(0.0020),\n",
       " tensor(0.0020),\n",
       " tensor(0.0020)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:09:09.286438Z",
     "start_time": "2025-08-26T14:09:09.230222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(50):\n",
    "\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # ------------\n",
    "      # forward pass:\n",
    "      # Embedding\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + B1\n",
    "\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + B2 # (N, vocab_size)\n",
    "      # ------------\n",
    "      # Sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ],
   "id": "b4f0ee6335a1a163",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cartah.\n",
      "aallijerh.\n",
      "jmrzi.\n",
      "thiy.\n",
      "snaasan.\n",
      "jmzhuen.\n",
      "amerrhc.\n",
      "aaeei.\n",
      "nernnra.\n",
      "ceziiv.\n",
      "arle.\n",
      "ndh.\n",
      "amahdin.\n",
      "edinn.\n",
      "srlin.\n",
      "ania.\n",
      "bqyllztro.\n",
      "djaryni.\n",
      "jkqe.\n",
      "ciusai.\n",
      "edde.\n",
      "oia.\n",
      "etleahysrleaa.\n",
      "aeyih.\n",
      "jtvsya.\n",
      "ambbzaoo.\n",
      "rjioon.\n",
      "iala.\n",
      "rren.\n",
      "ke.\n",
      "sadlnagynne.\n",
      "srloraa.\n",
      "rynh.\n",
      "aonkllibraa.\n",
      "kaein.\n",
      "oqe.\n",
      "ryy.\n",
      "bda.\n",
      "adye.\n",
      "uitegbnfmirdegjerniyla.\n",
      "kemlesaa.\n",
      "nnljadrk.\n",
      "kallsusaimevmbtevl.\n",
      "ky.\n",
      "adyloo.\n",
      "roao.\n",
      "kton.\n",
      "omarin.\n",
      "ca.\n",
      "losnszns.\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "- The **MLP-based character-level model** significantly improves upon bigram models by capturing **longer dependencies** between characters.\n",
    "- We observed that:\n",
    "  - **Embeddings** provide compact representations of characters.\n",
    "  - The **hidden layer** learns non-linear transformations, enabling the model to represent complex patterns.\n",
    "  - Using **cross-entropy loss** ensures stable training compared to manual negative log-likelihood calculations.\n",
    "  - **Mini-batches** and **learning rate tuning** are essential for efficient training on larger datasets.\n",
    "\n",
    "- However, the model still faces challenges:\n",
    "  - Explosive starting loss, which highlights the need for **smaller activation weights** at initialization.\n",
    "  - Difficulty in **controlling gradients**, requiring careful tuning of optimization strategies.\n",
    "  - Potential instability without techniques like **batch normalization**.\n"
   ],
   "id": "619a100db08cc0ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bd9e43955a4718aa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
