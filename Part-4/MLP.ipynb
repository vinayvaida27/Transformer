{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:09.565817Z",
     "start_time": "2025-07-16T14:07:09.561845Z"
    }
   },
   "source": "## Multi Layer Neural network",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:12.191078Z",
     "start_time": "2025-07-16T14:07:12.187419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## in previous Notebook we have seen used single layer perceptron neural network and  Bi gram stats model both of them cannot capture the context better, they dont sound the names better\n",
    "# so in this notebook we use the multilayer perceptron which are useful to generate the meaning full name\n",
    "\n",
    "# so the problem with bi-gram model we face is we are able to capture the context of 2 characters at a time.(that gives us 27 character * 27 characters = 729 possibilities combination comes in context)\n",
    "# if we have the 3 characters as context  we would get (27*27*27 =19,623 combinations context) generating count vector normalizing and everything becomes difficult now.\n",
    "# the modeling approach we gonna follow  is from this research paper bengio et al. 2003 (https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n"
   ],
   "id": "871b15627454a0ab",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:15.821804Z",
     "start_time": "2025-07-16T14:07:12.443055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "id": "a2f91463e9020a1c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:15.905547Z",
     "start_time": "2025-07-16T14:07:15.836327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:5]"
   ],
   "id": "cdab35aafdd2ac69",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:15.951617Z",
     "start_time": "2025-07-16T14:07:15.946519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {\n",
    "    s: i+1 for i,s in enumerate(chars)\n",
    "}\n",
    "stoi['.'] = 0\n",
    "itos = { i:s for s,i in stoi.items() }\n"
   ],
   "id": "20a56c1d5b400d4e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating dataset for Neural Networks from Text File to Input Tensor and Output Tensor",
   "id": "67379305a8347179"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:15.975504Z",
     "start_time": "2025-07-16T14:07:15.968706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## data set for neural network\n",
    "\n",
    "block_size = 3 #Context_length: how many characters do we take to predict the next one?\n",
    "X,Y =[],[] # X are the input to neural network and Y are  the labels of the neural network\n",
    "\n",
    "for w in words[:1]:\n",
    "\n",
    "    print(w)\n",
    "    context = [0] * block_size\n",
    "\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "\n",
    "        print(\"input data : \",''.join(itos[i] for i in context), \", Output :\", '=', itos[ix])\n",
    "        print('context :', context)\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print(\"input tensor :\", X)\n",
    "\n",
    "print(\"output tensor :\", Y)"
   ],
   "id": "df30e971691caba6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "input data :  ... , Output : = e\n",
      "context : [0, 0, 0]\n",
      "input data :  ..e , Output : = m\n",
      "context : [0, 0, 5]\n",
      "input data :  .em , Output : = m\n",
      "context : [0, 5, 13]\n",
      "input data :  emm , Output : = a\n",
      "context : [5, 13, 13]\n",
      "input data :  mma , Output : = .\n",
      "context : [13, 13, 1]\n",
      "\n",
      "\n",
      "input tensor : tensor([[ 0,  0,  0],\n",
      "        [ 0,  0,  5],\n",
      "        [ 0,  5, 13],\n",
      "        [ 5, 13, 13],\n",
      "        [13, 13,  1]])\n",
      "output tensor : tensor([ 5, 13, 13,  1,  0])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:16.018547Z",
     "start_time": "2025-07-16T14:07:16.015039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Above output explanation\n",
    "\n",
    "you can notes how the input data is store in the input tensor it is simple\n",
    "\n",
    "input data :  ... , Output : = e\n",
    "context : [0, 0, 0]\n",
    "\n",
    "when we get input '...' we have the o/p as 'e'\n",
    "similary i/p = '..e' => o/p = 'm'\n",
    "\n",
    "-> we are trying to take context of 3 character at same time and trying to predict the o/p of what character come next and run in the loop\n",
    "\n",
    "1)so we are able to use above block and change the parameters like block size to get different context\n",
    "2) if we consider all the input and one cblock size we are able to get the whole i/p and o/p data\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ],
   "id": "9aafb3d72ea457d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAbove output explanation\\n\\nyou can notes how the input data is store in the input tensor it is simple\\n\\ninput data :  ... , Output : = e\\ncontext : [0, 0, 0]\\n\\nwhen we get input '...' we have the o/p as 'e'\\nsimilary i/p = '..e' => o/p = 'm'\\n\\n-> we are trying to take context of 3 character at same time and trying to predict the o/p of what character come next and run in the loop\\n\\n1)so we are able to use above block and change the parameters like block size to get different context\\n2) if we consider all the input and one cblock size we are able to get the whole i/p and o/p data\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Block size = 5",
   "id": "86a2e1a58372fa34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:16.047113Z",
     "start_time": "2025-07-16T14:07:16.042103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_size = 5 #Context_length: how many characters do we take to predict the next one?\n",
    "X,Y =[],[] # x are inputs and Y are labels\n",
    "\n",
    "for w in words[:1]:\n",
    "\n",
    "    print( 'word is :',w)\n",
    "\n",
    "    print('\\n')\n",
    "    print('input -->  output')\n",
    "    context = [0] * block_size\n",
    "\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "\n",
    "        print(''.join(itos[i] for i in context), \"--> \", itos[ix])\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print(\"input tensor :\", X)\n",
    "\n",
    "print(\"output tensor :\", Y)"
   ],
   "id": "bbf8520f0eb7ca04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word is : emma\n",
      "\n",
      "\n",
      "input -->  output\n",
      "..... -->  e\n",
      "....e -->  m\n",
      "...em -->  m\n",
      "..emm -->  a\n",
      ".emma -->  .\n",
      "\n",
      "\n",
      "input tensor : tensor([[ 0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  5],\n",
      "        [ 0,  0,  0,  5, 13],\n",
      "        [ 0,  0,  5, 13, 13],\n",
      "        [ 0,  5, 13, 13,  1]])\n",
      "output tensor : tensor([ 5, 13, 13,  1,  0])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:16.080622Z",
     "start_time": "2025-07-16T14:07:16.076187Z"
    }
   },
   "cell_type": "code",
   "source": "X",
   "id": "45abbded1ac2a39a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  5],\n",
       "        [ 0,  0,  0,  5, 13],\n",
       "        [ 0,  0,  5, 13, 13],\n",
       "        [ 0,  5, 13, 13,  1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:23.039861Z",
     "start_time": "2025-07-16T14:07:23.033861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(Y)"
   ],
   "id": "6fe0f90d2162c136",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    0\n",
       "0   5\n",
       "1  13\n",
       "2  13\n",
       "3   1\n",
       "4   0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### above we are trying to take 5 character as input and predicting what comes next as o/p",
   "id": "9695ecaf3199aba3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:23.112469Z",
     "start_time": "2025-07-16T14:07:23.106280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_size = 3 #Context_length: how many characters do we take to predict the next one?\n",
    "X,Y =[],[] # x are inputs and Y are labels\n",
    "\n",
    "for w in words[:5]:\n",
    "\n",
    "    #print(w)\n",
    "    context = [0] * block_size\n",
    "\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "\n",
    "        #print(\"input data : \",''.join(itos[i] for i in context), \", Output :\", '=', itos[ix])\n",
    "        #print('context :', context)\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "X"
   ],
   "id": "ab013fef48297968",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:23.224949Z",
     "start_time": "2025-07-16T14:07:23.218408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(Y)"
   ],
   "id": "9fbc192d9e0b8e45",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     0\n",
       "0    5\n",
       "1   13\n",
       "2   13\n",
       "3    1\n",
       "4    0\n",
       "5   15\n",
       "6   12\n",
       "7    9\n",
       "8   22\n",
       "9    9\n",
       "10   1\n",
       "11   0\n",
       "12   1\n",
       "13  22\n",
       "14   1\n",
       "15   0\n",
       "16   9\n",
       "17  19\n",
       "18   1\n",
       "19   2\n",
       "20   5\n",
       "21  12\n",
       "22  12\n",
       "23   1\n",
       "24   0\n",
       "25  19\n",
       "26  15\n",
       "27  16\n",
       "28   8\n",
       "29   9\n",
       "30   1\n",
       "31   0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:23.534970Z",
     "start_time": "2025-07-16T14:07:23.530657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Training Data is ready\n",
    "1. now the TRain data with the block_size/context = 3 is ready and o/p is also ready fo the model to get trained on.\n",
    "\"\"\""
   ],
   "id": "db5fdadbdf89bd9f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTraining Data is ready\\n1. now the TRain data with the block_size/context = 3 is ready and o/p is also ready fo the model to get trained on.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:23.688667Z",
     "start_time": "2025-07-16T14:07:23.684544Z"
    }
   },
   "cell_type": "code",
   "source": "(X.shape, X.dtype), (Y.shape, Y.dtype)",
   "id": "f6abc7661359b0cd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((torch.Size([32, 3]), torch.int64), (torch.Size([32]), torch.int64))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:23.908205Z",
     "start_time": "2025-07-16T14:07:23.905205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## so our tensor matrix x has 228146 rows and 3 columns only both of there dtype is integer\n",
    "\n",
    "## In paper they have 17,000 words and they embedded in 30 dimensional\n",
    "\n",
    "## in our case we have only 27 possible characters so let's consider 2- dimensional space so we have 27 charcter with 2 dimenisonal space(27*2) tensor"
   ],
   "id": "a8badac9970dacd9",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:23.995985Z",
     "start_time": "2025-07-16T14:07:23.989566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# let embedded the 2 random numbers with 27*2\n",
    "\n",
    "C = torch.randn((27,2)) # this is used as an embedding layer\n",
    "C"
   ],
   "id": "90c4cf80a2bf2f3c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.6265e-01, -5.7562e-01],\n",
       "        [ 4.3103e-01, -1.4376e+00],\n",
       "        [ 8.2138e-01, -2.7323e-01],\n",
       "        [ 8.8447e-01,  1.4804e+00],\n",
       "        [-4.5491e-01, -1.7313e-01],\n",
       "        [-9.8686e-01,  5.4246e-04],\n",
       "        [-1.0636e-01, -5.8234e-01],\n",
       "        [ 4.4966e-01,  1.1518e+00],\n",
       "        [ 6.4602e-01, -1.2099e+00],\n",
       "        [-1.6913e-01, -1.6939e+00],\n",
       "        [ 3.7878e-01,  1.8156e-01],\n",
       "        [-1.1004e-01,  4.3457e-01],\n",
       "        [-1.0578e+00,  1.5091e-01],\n",
       "        [ 1.4987e+00,  1.0107e+00],\n",
       "        [-1.5612e+00, -6.8247e-01],\n",
       "        [ 3.7086e-03,  3.8279e-01],\n",
       "        [-1.2025e+00,  6.8690e-02],\n",
       "        [-6.4464e-01, -5.7007e-01],\n",
       "        [ 6.7176e-02,  9.2242e-01],\n",
       "        [ 2.1345e-01,  7.2505e-01],\n",
       "        [ 5.1182e-01, -5.8370e-01],\n",
       "        [ 9.5653e-01, -3.5498e-01],\n",
       "        [ 1.0434e+00, -1.5332e+00],\n",
       "        [-9.9752e-01, -5.3241e-01],\n",
       "        [-2.6609e-02, -1.0094e+00],\n",
       "        [-9.0189e-01, -2.2955e+00],\n",
       "        [ 8.8348e-01,  2.1801e-01]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:24.064462Z",
     "start_time": "2025-07-16T14:07:24.060161Z"
    }
   },
   "cell_type": "code",
   "source": "C[5]",
   "id": "8d1c6c7ebbdd3d61",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9.8686e-01,  5.4246e-04])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:24.131097Z",
     "start_time": "2025-07-16T14:07:24.125023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Example of encoding the number 5 with number of classes = 27(charac) and multiplying it with the weights matrix\n",
    "# [1*27] @ [27*2] = [1*2] tensor\n",
    "\n",
    "(F.one_hot(torch.tensor(5), num_classes=27).float()) @ C"
   ],
   "id": "f3a337bff25f2141",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9.8686e-01,  5.4246e-04])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:24.340097Z",
     "start_time": "2025-07-16T14:07:24.337599Z"
    }
   },
   "cell_type": "code",
   "source": " # if we multiply the embedding vector(C) * index_vector/one_hot encoding vector of the input(X) = we get the Result from the first layer",
   "id": "91bf9541d1ecd513",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:24.373152Z",
     "start_time": "2025-07-16T14:07:24.368151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "C = torch.randn((27,2))\n",
    "C ## c is noting buit one type of encoding"
   ],
   "id": "9f163d4210a00987",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4801, -0.5718],\n",
       "        [-0.3303,  0.4769],\n",
       "        [-0.1991,  0.0677],\n",
       "        [ 0.5129,  0.9135],\n",
       "        [-0.4071, -1.4313],\n",
       "        [ 0.4187,  1.1150],\n",
       "        [ 0.8956, -0.4083],\n",
       "        [ 0.3990, -0.2001],\n",
       "        [-0.1843,  2.0090],\n",
       "        [-0.3285,  0.0651],\n",
       "        [-0.6277,  1.2797],\n",
       "        [-0.1945,  2.2645],\n",
       "        [ 0.5152, -0.1842],\n",
       "        [ 1.0437, -0.2197],\n",
       "        [-0.5586,  1.5264],\n",
       "        [ 1.0341,  0.2543],\n",
       "        [ 0.9467, -1.4448],\n",
       "        [-0.5550,  0.5129],\n",
       "        [-1.6583, -0.6111],\n",
       "        [-2.5221, -0.1362],\n",
       "        [ 1.3103, -0.2016],\n",
       "        [ 0.8770, -0.6936],\n",
       "        [ 1.3031,  1.1813],\n",
       "        [ 0.3493,  2.4033],\n",
       "        [ 1.6857, -0.6433],\n",
       "        [-0.1681,  0.7351],\n",
       "        [-1.3295, -0.2232]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:24.423571Z",
     "start_time": "2025-07-16T14:07:24.418903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we want to encode one number 5 we can simply pass this number to the Embedding tensor C and get the output i.e.,\n",
    "\n",
    "C[5] #Give embedding of 5 in 2d Vector Space"
   ],
   "id": "655e7724225f6ce6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4187, 1.1150])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:24.483928Z",
     "start_time": "2025-07-16T14:07:24.480245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# what if the embedding is like list\n",
    "\n",
    "print(C[[2,3,4]]) # we got the embedding of 3 integer vector as below\n"
   ],
   "id": "7af03145d23088db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1991,  0.0677],\n",
      "        [ 0.5129,  0.9135],\n",
      "        [-0.4071, -1.4313]])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:24.659163Z",
     "start_time": "2025-07-16T14:07:24.655071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# what if we give the tensor input to get embedding\n",
    "\n",
    "print(C[torch.tensor([5,6,7,7,7,7,7,7,])]) # still we are able to get the embedding by giving tensor matrix as input"
   ],
   "id": "684794359b52802b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4187,  1.1150],\n",
      "        [ 0.8956, -0.4083],\n",
      "        [ 0.3990, -0.2001],\n",
      "        [ 0.3990, -0.2001],\n",
      "        [ 0.3990, -0.2001],\n",
      "        [ 0.3990, -0.2001],\n",
      "        [ 0.3990, -0.2001],\n",
      "        [ 0.3990, -0.2001]])\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:24.703118Z",
     "start_time": "2025-07-16T14:07:24.699023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Full explanation of C\n",
    "\n",
    " C is the embedding vector  when you pass the single integer -> we get the embedding of that single integer\n",
    "if we pass list to embedding vector c -> we goonna get embedding for that list passed\n",
    "if we pass the tensor to embedding vector -. we gonna get th emebedding for that tensor\n",
    "\n",
    "\n",
    "to get emebedding for anything we need to pass it to the C\n",
    "\n",
    "\"\"\""
   ],
   "id": "1e5bc35f693b87b6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFull explanation of C\\n\\n C is the embedding vector  when you pass the single integer -> we get the embedding of that single integer\\nif we pass list to embedding vector c -> we goonna get embedding for that list passed\\nif we pass the tensor to embedding vector -. we gonna get th emebedding for that tensor\\n\\n\\nto get emebedding for anything we need to pass it to the C\\n\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:24.739411Z",
     "start_time": "2025-07-16T14:07:24.732832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding = C[X]\n",
    "embedding"
   ],
   "id": "84e3f5464cbff43f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4801, -0.5718],\n",
       "         [-0.4801, -0.5718],\n",
       "         [-0.4801, -0.5718]],\n",
       "\n",
       "        [[-0.4801, -0.5718],\n",
       "         [-0.4801, -0.5718],\n",
       "         [ 0.4187,  1.1150]],\n",
       "\n",
       "        [[-0.4801, -0.5718],\n",
       "         [ 0.4187,  1.1150],\n",
       "         [ 1.0437, -0.2197]],\n",
       "\n",
       "        [[ 0.4187,  1.1150],\n",
       "         [ 1.0437, -0.2197],\n",
       "         [ 1.0437, -0.2197]],\n",
       "\n",
       "        [[ 1.0437, -0.2197],\n",
       "         [ 1.0437, -0.2197],\n",
       "         [-0.3303,  0.4769]],\n",
       "\n",
       "        [[-0.4801, -0.5718],\n",
       "         [-0.4801, -0.5718],\n",
       "         [-0.4801, -0.5718]],\n",
       "\n",
       "        [[-0.4801, -0.5718],\n",
       "         [-0.4801, -0.5718],\n",
       "         [ 1.0341,  0.2543]],\n",
       "\n",
       "        [[-0.4801, -0.5718],\n",
       "         [ 1.0341,  0.2543],\n",
       "         [ 0.5152, -0.1842]],\n",
       "\n",
       "        [[ 1.0341,  0.2543],\n",
       "         [ 0.5152, -0.1842],\n",
       "         [-0.3285,  0.0651]],\n",
       "\n",
       "        [[ 0.5152, -0.1842],\n",
       "         [-0.3285,  0.0651],\n",
       "         [ 1.3031,  1.1813]],\n",
       "\n",
       "        [[-0.3285,  0.0651],\n",
       "         [ 1.3031,  1.1813],\n",
       "         [-0.3285,  0.0651]],\n",
       "\n",
       "        [[ 1.3031,  1.1813],\n",
       "         [-0.3285,  0.0651],\n",
       "         [-0.3303,  0.4769]],\n",
       "\n",
       "        [[-0.4801, -0.5718],\n",
       "         [-0.4801, -0.5718],\n",
       "         [-0.4801, -0.5718]],\n",
       "\n",
       "        [[-0.4801, -0.5718],\n",
       "         [-0.4801, -0.5718],\n",
       "         [-0.3303,  0.4769]],\n",
       "\n",
       "        [[-0.4801, -0.5718],\n",
       "         [-0.3303,  0.4769],\n",
       "         [ 1.3031,  1.1813]],\n",
       "\n",
       "        [[-0.3303,  0.4769],\n",
       "         [ 1.3031,  1.1813],\n",
       "         [-0.3303,  0.4769]],\n",
       "\n",
       "        [[-0.4801, -0.5718],\n",
       "         [-0.4801, -0.5718],\n",
       "         [-0.4801, -0.5718]],\n",
       "\n",
       "        [[-0.4801, -0.5718],\n",
       "         [-0.4801, -0.5718],\n",
       "         [-0.3285,  0.0651]],\n",
       "\n",
       "        [[-0.4801, -0.5718],\n",
       "         [-0.3285,  0.0651],\n",
       "         [-2.5221, -0.1362]],\n",
       "\n",
       "        [[-0.3285,  0.0651],\n",
       "         [-2.5221, -0.1362],\n",
       "         [-0.3303,  0.4769]],\n",
       "\n",
       "        [[-2.5221, -0.1362],\n",
       "         [-0.3303,  0.4769],\n",
       "         [-0.1991,  0.0677]],\n",
       "\n",
       "        [[-0.3303,  0.4769],\n",
       "         [-0.1991,  0.0677],\n",
       "         [ 0.4187,  1.1150]],\n",
       "\n",
       "        [[-0.1991,  0.0677],\n",
       "         [ 0.4187,  1.1150],\n",
       "         [ 0.5152, -0.1842]],\n",
       "\n",
       "        [[ 0.4187,  1.1150],\n",
       "         [ 0.5152, -0.1842],\n",
       "         [ 0.5152, -0.1842]],\n",
       "\n",
       "        [[ 0.5152, -0.1842],\n",
       "         [ 0.5152, -0.1842],\n",
       "         [-0.3303,  0.4769]],\n",
       "\n",
       "        [[-0.4801, -0.5718],\n",
       "         [-0.4801, -0.5718],\n",
       "         [-0.4801, -0.5718]],\n",
       "\n",
       "        [[-0.4801, -0.5718],\n",
       "         [-0.4801, -0.5718],\n",
       "         [-2.5221, -0.1362]],\n",
       "\n",
       "        [[-0.4801, -0.5718],\n",
       "         [-2.5221, -0.1362],\n",
       "         [ 1.0341,  0.2543]],\n",
       "\n",
       "        [[-2.5221, -0.1362],\n",
       "         [ 1.0341,  0.2543],\n",
       "         [ 0.9467, -1.4448]],\n",
       "\n",
       "        [[ 1.0341,  0.2543],\n",
       "         [ 0.9467, -1.4448],\n",
       "         [-0.1843,  2.0090]],\n",
       "\n",
       "        [[ 0.9467, -1.4448],\n",
       "         [-0.1843,  2.0090],\n",
       "         [-0.3285,  0.0651]],\n",
       "\n",
       "        [[-0.1843,  2.0090],\n",
       "         [-0.3285,  0.0651],\n",
       "         [-0.3303,  0.4769]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:24.938810Z",
     "start_time": "2025-07-16T14:07:24.934809Z"
    }
   },
   "cell_type": "code",
   "source": "embedding.shape",
   "id": "fdfda423e7a1a6e3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "now our input is embeeded",
   "id": "b540d145d62b6000"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:24.996062Z",
     "start_time": "2025-07-16T14:07:24.987791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# so to convert the input vector to 2 dimensional space there are manu ways to represent it\n",
    "# but one of the effective and easiest way pass dotview  .view(x,6)\n",
    "\n",
    "embedding.view(-1, 6)"
   ],
   "id": "458f836736cf59cf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4801, -0.5718, -0.4801, -0.5718, -0.4801, -0.5718],\n",
       "        [-0.4801, -0.5718, -0.4801, -0.5718,  0.4187,  1.1150],\n",
       "        [-0.4801, -0.5718,  0.4187,  1.1150,  1.0437, -0.2197],\n",
       "        [ 0.4187,  1.1150,  1.0437, -0.2197,  1.0437, -0.2197],\n",
       "        [ 1.0437, -0.2197,  1.0437, -0.2197, -0.3303,  0.4769],\n",
       "        [-0.4801, -0.5718, -0.4801, -0.5718, -0.4801, -0.5718],\n",
       "        [-0.4801, -0.5718, -0.4801, -0.5718,  1.0341,  0.2543],\n",
       "        [-0.4801, -0.5718,  1.0341,  0.2543,  0.5152, -0.1842],\n",
       "        [ 1.0341,  0.2543,  0.5152, -0.1842, -0.3285,  0.0651],\n",
       "        [ 0.5152, -0.1842, -0.3285,  0.0651,  1.3031,  1.1813],\n",
       "        [-0.3285,  0.0651,  1.3031,  1.1813, -0.3285,  0.0651],\n",
       "        [ 1.3031,  1.1813, -0.3285,  0.0651, -0.3303,  0.4769],\n",
       "        [-0.4801, -0.5718, -0.4801, -0.5718, -0.4801, -0.5718],\n",
       "        [-0.4801, -0.5718, -0.4801, -0.5718, -0.3303,  0.4769],\n",
       "        [-0.4801, -0.5718, -0.3303,  0.4769,  1.3031,  1.1813],\n",
       "        [-0.3303,  0.4769,  1.3031,  1.1813, -0.3303,  0.4769],\n",
       "        [-0.4801, -0.5718, -0.4801, -0.5718, -0.4801, -0.5718],\n",
       "        [-0.4801, -0.5718, -0.4801, -0.5718, -0.3285,  0.0651],\n",
       "        [-0.4801, -0.5718, -0.3285,  0.0651, -2.5221, -0.1362],\n",
       "        [-0.3285,  0.0651, -2.5221, -0.1362, -0.3303,  0.4769],\n",
       "        [-2.5221, -0.1362, -0.3303,  0.4769, -0.1991,  0.0677],\n",
       "        [-0.3303,  0.4769, -0.1991,  0.0677,  0.4187,  1.1150],\n",
       "        [-0.1991,  0.0677,  0.4187,  1.1150,  0.5152, -0.1842],\n",
       "        [ 0.4187,  1.1150,  0.5152, -0.1842,  0.5152, -0.1842],\n",
       "        [ 0.5152, -0.1842,  0.5152, -0.1842, -0.3303,  0.4769],\n",
       "        [-0.4801, -0.5718, -0.4801, -0.5718, -0.4801, -0.5718],\n",
       "        [-0.4801, -0.5718, -0.4801, -0.5718, -2.5221, -0.1362],\n",
       "        [-0.4801, -0.5718, -2.5221, -0.1362,  1.0341,  0.2543],\n",
       "        [-2.5221, -0.1362,  1.0341,  0.2543,  0.9467, -1.4448],\n",
       "        [ 1.0341,  0.2543,  0.9467, -1.4448, -0.1843,  2.0090],\n",
       "        [ 0.9467, -1.4448, -0.1843,  2.0090, -0.3285,  0.0651],\n",
       "        [-0.1843,  2.0090, -0.3285,  0.0651, -0.3303,  0.4769]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hidden Layer",
   "id": "bb6e91bcb1efc137"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:25.054964Z",
     "start_time": "2025-07-16T14:07:25.051319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## now if we want to pass it to the second layer\n",
    "\n",
    "# consider second layer as weight w1 and bias b1\n",
    "# then o/p of second layer = i/p @w1 + b1\n",
    "\n",
    "#lets consider we have 100 weights in second layer such that we have 100 biases in second layer too h = tanh(w1*x1 + b1)\n",
    "\n",
    "W1 = torch.rand((6,100))\n",
    "B1 = torch.rand(100)"
   ],
   "id": "1199d474f81bf1fd",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:25.112967Z",
     "start_time": "2025-07-16T14:07:25.103245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "h = torch.tanh(embedding.view(-1, 6) @ W1 + B1)\n",
    "h"
   ],
   "id": "826ec8ef8103ae02",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6272, -0.8217, -0.9371,  ..., -0.7860, -0.8345, -0.0813],\n",
       "        [ 0.5613,  0.0244,  0.6126,  ..., -0.6147,  0.4454,  0.5109],\n",
       "        [ 0.4501,  0.6037,  0.6672,  ...,  0.6337,  0.7415,  0.9378],\n",
       "        ...,\n",
       "        [ 0.9994,  0.9849,  0.9585,  ..., -0.0398,  0.9972,  0.7091],\n",
       "        [ 0.5349, -0.0637, -0.0439,  ...,  0.7753,  0.6206,  0.8619],\n",
       "        [ 0.9818,  0.8723,  0.8892,  ...,  0.9315,  0.9734,  0.5931]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:25.265667Z",
     "start_time": "2025-07-16T14:07:25.259898Z"
    }
   },
   "cell_type": "code",
   "source": "h.shape",
   "id": "b341240b34bf1f18",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:25.308220Z",
     "start_time": "2025-07-16T14:07:25.304786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "-> We are creating the output layer here -> output layer = weight * hidden layer + Bias\n",
    "\n",
    "-> but we have only 27 character, so the layer needs to be 27 weights + 27 weights i.e., we need to take 100 inputs and give only 27 o/p\n",
    "\n",
    "->\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "W2 = torch.randn((100,27))\n",
    "B2 = torch.randn(27)"
   ],
   "id": "bffc94ac57b68e38",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:25.515067Z",
     "start_time": "2025-07-16T14:07:25.506576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logits = h @ W2+ B2\n",
    "logits"
   ],
   "id": "381af8ca36b3c856",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1.1739,  -4.6670,   7.8754,   7.1339,   0.7039,  -1.7191, -10.0511,\n",
       "          -4.6246,   5.6103,  -2.3767,  -1.7321,  -0.7345,  10.3900,   1.0577,\n",
       "           0.2727,   1.5749,   3.6362,  -0.3643,  -9.6544,  -3.3648,  -6.9878,\n",
       "          -2.0819,  -2.3028,  -6.2240,  -1.0850,  -1.2537,  -3.2235],\n",
       "        [  1.7919,  10.5477,  -6.6625,   3.7009,   0.0461,   4.8559,  -2.9863,\n",
       "           8.6996,  -0.7946,   3.3162,   2.7923,   0.5470,  -1.8288,   3.8888,\n",
       "           1.4029,   1.7446,   0.9335,  -7.2009,  -2.6945,  -4.7039,   0.9766,\n",
       "           0.7695,  -4.7156,   3.6152,  -3.1825,  -4.9716,   0.5876],\n",
       "        [  2.5625,  13.3951,  -6.5449,  -7.3383,   0.1051,   5.5425,   8.9715,\n",
       "           9.4398,  -6.6858,   8.8093,   9.9245,   1.3003,  -9.5754,  -0.2307,\n",
       "           1.1340,   4.4511,  -1.0242,  -1.5479,  10.0884,   5.7848,   4.3789,\n",
       "           4.1966,   2.7485,   6.6056,  -1.8036,   1.2418,   2.9285],\n",
       "        [ -1.0134,  12.6199,  -9.6470,  -8.6922,   3.4526,   4.9844,  12.1842,\n",
       "          11.1307, -10.0221,   7.4128,   5.7057,   4.5240,  -7.9771,  -1.0170,\n",
       "           2.1788,   1.9663,  -4.0371,   0.4108,  11.5296,   1.1264,   6.2745,\n",
       "           1.6454,  -1.5548,   6.2413,  -1.5404,   2.8794,   6.9519],\n",
       "        [ -0.4505,  11.7813,  -5.7451,  -7.3357,   0.9216,   3.7932,   7.9971,\n",
       "          12.4624,  -8.7788,   6.9196,   4.6120,   4.0661,  -6.9156,   0.3226,\n",
       "           1.7383,   3.5122,  -5.0558,  -0.1653,  12.1195,   0.9210,   4.0570,\n",
       "          -0.2895,  -0.7371,   4.7391,  -3.4795,   2.6783,   5.2836],\n",
       "        [  1.1739,  -4.6670,   7.8754,   7.1339,   0.7039,  -1.7191, -10.0511,\n",
       "          -4.6246,   5.6103,  -2.3767,  -1.7321,  -0.7345,  10.3900,   1.0577,\n",
       "           0.2727,   1.5749,   3.6362,  -0.3643,  -9.6544,  -3.3648,  -6.9878,\n",
       "          -2.0819,  -2.3028,  -6.2240,  -1.0850,  -1.2537,  -3.2235],\n",
       "        [  3.1255,  11.7041,  -5.4948,   2.5109,   3.2544,   1.7514,  -4.7998,\n",
       "           5.5041,   2.1521,   5.5873,   5.4892,   3.0257,   2.0733,   0.9158,\n",
       "          -0.7139,   1.6878,   1.1342,  -5.7549,  -2.0831,  -3.8549,  -0.7273,\n",
       "           2.9691,  -2.9370,   2.4867,  -0.6686,  -3.8098,   0.7260],\n",
       "        [  1.5433,  12.0501,  -3.2510,  -6.5410,   2.9749,   3.4821,   4.6338,\n",
       "           9.1298,  -5.5638,   6.6336,   9.9978,   1.3230,  -3.8172,   3.2950,\n",
       "          -1.1703,   5.6864,   1.6147,  -2.1447,   6.4490,   2.4099,   1.9020,\n",
       "          -1.5605,   1.2198,   3.9703,  -0.2961,   0.6279,   3.6590],\n",
       "        [ -1.0645,  11.1755,  -4.9142,  -6.8453,   0.8092,   3.8346,   8.2890,\n",
       "          11.7014,  -8.7776,   8.1851,   4.2703,   3.6247,  -6.4025,  -1.1378,\n",
       "           1.0874,   2.9314,  -4.4782,  -0.0967,  12.2568,  -1.0231,   3.6975,\n",
       "           0.4214,   0.0290,   3.2828,  -4.1843,   4.0813,   4.8082],\n",
       "        [  0.1207,  13.8819, -10.0251,  -7.5524,   0.3580,   5.9760,  10.6864,\n",
       "          11.6223,  -9.4702,   7.8037,   4.6342,   3.9595,  -9.4234,  -0.4513,\n",
       "           3.7235,   2.1393,  -3.5415,  -0.1019,  10.7167,   1.6843,   6.3246,\n",
       "           2.9463,  -2.0070,   7.4583,  -2.0915,   1.0342,   6.8982],\n",
       "        [ -1.3172,   9.5650,  -5.1972,  -8.0961,   2.4667,   6.3862,  12.2199,\n",
       "          11.7562,  -9.0258,   6.3167,   6.7131,   0.5403,  -9.4680,  -0.1147,\n",
       "           2.9432,   4.3965,  -1.1791,  -1.4721,  12.0531,   1.1074,   4.7808,\n",
       "           0.6903,   0.0915,   5.0811,  -1.1682,   5.2379,   4.9762],\n",
       "        [ -1.5438,  11.7323,  -8.8361,  -8.1346,   1.1159,   5.9055,  10.9340,\n",
       "          11.5025,  -9.9374,   7.3274,   4.5381,   3.0972,  -8.4298,  -2.0504,\n",
       "           2.2075,   1.7461,  -3.3955,  -0.1965,  12.3238,  -0.3310,   6.2491,\n",
       "           1.9373,  -1.1017,   5.2763,  -3.9452,   4.6938,   5.8656],\n",
       "        [  1.1739,  -4.6670,   7.8754,   7.1339,   0.7039,  -1.7191, -10.0511,\n",
       "          -4.6246,   5.6103,  -2.3767,  -1.7321,  -0.7345,  10.3900,   1.0577,\n",
       "           0.2727,   1.5749,   3.6362,  -0.3643,  -9.6544,  -3.3648,  -6.9878,\n",
       "          -2.0819,  -2.3028,  -6.2240,  -1.0850,  -1.2537,  -3.2235],\n",
       "        [  1.8896,   0.1336,   3.7346,   7.6102,  -0.2508,   1.7261,  -6.2435,\n",
       "           2.9893,   1.9229,  -0.5378,   0.5290,  -2.2056,   5.9095,   2.4271,\n",
       "           0.6968,   2.2576,   4.6671,  -2.5263,  -7.1714,  -5.5204,  -3.2951,\n",
       "          -1.1582,  -3.5131,  -3.3211,  -3.9846,  -0.7776,  -3.1117],\n",
       "        [  2.0987,  14.7385,  -9.6798,  -5.8991,  -1.4019,   5.9808,   7.4998,\n",
       "          11.0736,  -7.0884,   8.7099,   5.1287,   2.0733,  -9.9180,   1.0951,\n",
       "           2.9739,   2.4595,  -1.0247,  -3.6662,   6.9403,   3.4595,   5.9416,\n",
       "           3.5609,  -1.6949,   8.5111,  -1.3996,  -2.6692,   5.1409],\n",
       "        [ -1.3504,  10.7262,  -8.0361,  -9.2043,   2.3940,   6.4416,  13.2633,\n",
       "          12.4602, -10.2403,   6.7609,   5.5008,   2.3363, -10.2575,  -0.9187,\n",
       "           3.4354,   3.8048,  -1.9501,  -1.4730,  12.3360,   0.4115,   6.0629,\n",
       "           0.7051,  -1.1426,   6.2086,  -1.1215,   4.8874,   6.1360],\n",
       "        [  1.1739,  -4.6670,   7.8754,   7.1339,   0.7039,  -1.7191, -10.0511,\n",
       "          -4.6246,   5.6103,  -2.3767,  -1.7321,  -0.7345,  10.3900,   1.0577,\n",
       "           0.2727,   1.5749,   3.6362,  -0.3643,  -9.6544,  -3.3648,  -6.9878,\n",
       "          -2.0819,  -2.3028,  -6.2240,  -1.0850,  -1.2537,  -3.2235],\n",
       "        [  1.6501,  -1.7816,   5.6852,   7.8483,   0.6078,   0.1560,  -7.9098,\n",
       "          -0.0164,   3.4445,  -0.8805,   0.0229,  -1.3101,   8.2261,   1.4374,\n",
       "           0.7069,   2.0733,   4.5117,  -1.1602,  -7.9631,  -4.4379,  -5.1901,\n",
       "          -1.2651,  -3.1888,  -4.6496,  -3.0166,  -0.5634,  -3.1681],\n",
       "        [  2.1465,  -9.8241,   9.7693,   6.3302,  -4.3253,   0.2311,  -9.3427,\n",
       "          -5.0305,   4.6475,  -5.3413,  -4.3179,  -5.4139,   6.5928,   2.3575,\n",
       "          -1.2135,   1.4436,   5.5187,  -0.7250,  -9.4255,  -3.7929,  -4.9707,\n",
       "          -4.3756,  -2.7973,  -6.0105,  -1.8420,   0.6805,  -5.1120],\n",
       "        [  5.2850,  -6.7901,   0.3115,   7.2803,  -3.1495,   5.9211,  -2.3160,\n",
       "          -4.5072,   5.1763,  -2.9142,  -3.5973,  -3.6420,   3.1269,  -2.2277,\n",
       "          -1.5675,  -4.1434,   1.5015,  -1.3424,  -8.9515,  -6.7056,  -0.4084,\n",
       "           7.1264,  -3.3942,  -6.2559,  -5.4249,   0.9688,  -5.6661],\n",
       "        [  2.7609,  -6.2457,   4.3971,   6.7370,   2.0200,   2.6919,  -5.7087,\n",
       "          -3.8941,   4.1246,  -5.0790,   1.4937,  -3.4229,   6.0670,   2.2611,\n",
       "          -4.2073,   1.3294,   9.7854,  -0.2870, -12.1876,  -4.0528,  -5.0877,\n",
       "          -3.3978,   0.3233,   2.4267,   0.9028,   1.4001,  -4.5059],\n",
       "        [  0.2765,  11.8497, -10.9174,  -5.4872,   0.6534,   6.9150,   9.1252,\n",
       "          11.6282,  -8.6870,   8.0657,   3.9673,   3.3034,  -9.4366,   0.2304,\n",
       "           3.2555,   2.4968,  -0.0249,  -3.2916,   8.2561,  -1.6052,   4.8480,\n",
       "           1.9211,  -1.4008,   7.3042,  -1.7106,   1.2072,   6.8042],\n",
       "        [  0.0279,  11.8284,  -7.6510,  -7.8257,   1.1013,   6.2535,  11.1489,\n",
       "          10.1265,  -8.9363,   7.7751,   7.5281,   1.7470,  -9.8032,  -0.9997,\n",
       "           1.9309,   3.3649,  -2.1971,  -0.5330,  12.0690,   2.6359,   5.2981,\n",
       "           3.1287,   1.1252,   5.7153,  -1.7558,   3.7688,   4.9356],\n",
       "        [ -1.1822,  12.3516,  -9.0718,  -7.9293,   2.8383,   4.7705,  11.4239,\n",
       "          10.7313,  -8.8711,   7.8001,   5.4415,   4.1522,  -7.8199,  -1.3201,\n",
       "           1.7733,   1.6393,  -3.3031,  -0.3560,  11.6877,  -0.3424,   5.9057,\n",
       "           1.3724,  -1.1406,   5.4839,  -1.5634,   3.3844,   6.8098],\n",
       "        [ -0.5613,  10.7630,  -3.9977,  -5.1410,   0.3020,   4.6694,   6.0887,\n",
       "          11.8274,  -7.6491,   6.9511,   4.4942,   2.7317,  -6.0630,   1.1604,\n",
       "           0.7837,   3.2232,  -3.3037,  -1.2245,   9.8918,  -1.6208,   2.7374,\n",
       "          -0.2746,  -0.6905,   3.3720,  -4.3890,   2.1404,   3.7964],\n",
       "        [  1.1739,  -4.6670,   7.8754,   7.1339,   0.7039,  -1.7191, -10.0511,\n",
       "          -4.6246,   5.6103,  -2.3767,  -1.7321,  -0.7345,  10.3900,   1.0577,\n",
       "           0.2727,   1.5749,   3.6362,  -0.3643,  -9.6544,  -3.3648,  -6.9878,\n",
       "          -2.0819,  -2.3028,  -6.2240,  -1.0850,  -1.2537,  -3.2235],\n",
       "        [  2.5497,  -9.5642,   9.1506,   7.9327,  -3.1681,  -2.5760, -11.1648,\n",
       "          -6.7465,   7.1773,  -6.2378,  -4.8524,  -3.6891,   8.6338,   2.1366,\n",
       "          -1.8239,   0.9588,   4.7228,  -1.2552, -11.8659,  -5.5167,  -5.9946,\n",
       "          -3.9900,  -3.1346,  -6.9400,  -1.1288,  -1.0253,  -4.6193],\n",
       "        [  4.3629,  -0.1326,  -2.5715,   5.9977,  -1.0924,   2.2966,  -5.2819,\n",
       "          -1.6269,   6.8276,  -0.1728,   1.3154,   0.2318,   5.5568,  -2.3169,\n",
       "          -1.6245,  -1.3305,  -1.4113,  -2.3948,  -7.2297,  -5.4075,  -4.3546,\n",
       "          10.7638,  -2.5110,  -4.1606,  -6.3913,  -1.2879,  -4.5362],\n",
       "        [  1.7619,   2.0146,   6.4285,   2.2648,   8.7854,   1.7547,  -5.3889,\n",
       "          -4.6207,   2.4910,   2.3632,   4.1450,  -0.3997,   8.8943,   4.7638,\n",
       "          -4.1176,   6.6481,  10.0847,  -2.4875,  -4.2004,   0.0282,  -6.7701,\n",
       "          -8.1173,   1.1228,   4.4369,  10.6486,  -0.7540,  -1.0685],\n",
       "        [ -1.0586,  13.3441, -10.0611,  -5.5641,   2.3058,   4.1654,   7.4648,\n",
       "          13.4225,  -9.0296,   5.7819,   3.3512,   4.9066,  -7.9609,   2.8544,\n",
       "           2.6574,   2.0022,  -5.0517,  -0.3528,  10.9970,   0.0910,   6.3728,\n",
       "           0.2299,  -4.6115,   5.9329,  -3.9698,   0.1987,   6.0767],\n",
       "        [  0.8655,  11.1634,  -0.7401,  -6.8716,  -3.1974,   5.8753,   8.7361,\n",
       "          12.0764,  -6.7701,   9.2560,   4.3513,  -1.7086, -10.2427,   0.3219,\n",
       "           0.4374,   5.2336,  -6.2794,   3.8693,  11.5068,   4.5388,   1.7145,\n",
       "           7.0475,   6.1528,   5.2633,  -9.1111,   5.7255,  -5.6789],\n",
       "        [  0.3615,   9.3997, -11.4688,  -5.3834,   0.5001,   7.1210,  10.9561,\n",
       "          10.5941,  -8.2439,   7.9419,   3.8203,   3.2667, -10.7137,  -2.3161,\n",
       "           2.5383,   2.2251,   0.9718,  -3.9189,  10.2332,  -5.7363,   5.3194,\n",
       "           1.8328,  -0.6514,   5.2432,  -0.6444,   5.1751,   7.0761]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:25.584893Z",
     "start_time": "2025-07-16T14:07:25.573349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "counts = logits.exp()\n",
    "counts"
   ],
   "id": "46cf1d9989e43dbb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.2347e+00, 9.4002e-03, 2.6319e+03, 1.2537e+03, 2.0217e+00, 1.7923e-01,\n",
       "         4.3137e-05, 9.8072e-03, 2.7324e+02, 9.2853e-02, 1.7691e-01, 4.7976e-01,\n",
       "         3.2533e+04, 2.8797e+00, 1.3135e+00, 4.8301e+00, 3.7948e+01, 6.9470e-01,\n",
       "         6.4144e-05, 3.4568e-02, 9.2309e-04, 1.2470e-01, 9.9979e-02, 1.9813e-03,\n",
       "         3.3790e-01, 2.8544e-01, 3.9815e-02],\n",
       "        [6.0009e+00, 3.8090e+04, 1.2779e-03, 4.0484e+01, 1.0472e+00, 1.2850e+02,\n",
       "         5.0472e-02, 6.0006e+03, 4.5176e-01, 2.7555e+01, 1.6319e+01, 1.7280e+00,\n",
       "         1.6060e-01, 4.8851e+01, 4.0669e+00, 5.7236e+00, 2.5435e+00, 7.4591e-04,\n",
       "         6.7575e-02, 9.0601e-03, 2.6553e+00, 2.1587e+00, 8.9549e-03, 3.7158e+01,\n",
       "         4.1483e-02, 6.9321e-03, 1.7997e+00],\n",
       "        [1.2968e+01, 6.5679e+05, 1.4375e-03, 6.5017e-04, 1.1109e+00, 2.5533e+02,\n",
       "         7.8752e+03, 1.2579e+04, 1.2485e-03, 6.6960e+03, 2.0424e+04, 3.6705e+00,\n",
       "         6.9415e-05, 7.9402e-01, 3.1080e+00, 8.5722e+01, 3.5909e-01, 2.1269e-01,\n",
       "         2.4063e+04, 3.2532e+02, 7.9749e+01, 6.6457e+01, 1.5619e+01, 7.3923e+02,\n",
       "         1.6471e-01, 3.4619e+00, 1.8700e+01],\n",
       "        [3.6299e-01, 3.0253e+05, 6.4621e-05, 1.6789e-04, 3.1582e+01, 1.4612e+02,\n",
       "         1.9567e+05, 6.8232e+04, 4.4406e-05, 1.6570e+03, 3.0057e+02, 9.2204e+01,\n",
       "         3.4324e-04, 3.6167e-01, 8.8354e+00, 7.1439e+00, 1.7649e-02, 1.5080e+00,\n",
       "         1.0168e+05, 3.0846e+00, 5.3084e+02, 5.1830e+00, 2.1123e-01, 5.1352e+02,\n",
       "         2.1430e-01, 1.7804e+01, 1.0451e+03],\n",
       "        [6.3732e-01, 1.3078e+05, 3.1984e-03, 6.5188e-04, 2.5134e+00, 4.4399e+01,\n",
       "         2.9722e+03, 2.5844e+05, 1.5396e-04, 1.0119e+03, 1.0069e+02, 5.8328e+01,\n",
       "         9.9219e-04, 1.3807e+00, 5.6878e+00, 3.3520e+01, 6.3724e-03, 8.4760e-01,\n",
       "         1.8342e+05, 2.5119e+00, 5.7802e+01, 7.4861e-01, 4.7851e-01, 1.1434e+02,\n",
       "         3.0822e-02, 1.4560e+01, 1.9707e+02],\n",
       "        [3.2347e+00, 9.4002e-03, 2.6319e+03, 1.2537e+03, 2.0217e+00, 1.7923e-01,\n",
       "         4.3137e-05, 9.8072e-03, 2.7324e+02, 9.2853e-02, 1.7691e-01, 4.7976e-01,\n",
       "         3.2533e+04, 2.8797e+00, 1.3135e+00, 4.8301e+00, 3.7948e+01, 6.9470e-01,\n",
       "         6.4144e-05, 3.4568e-02, 9.2309e-04, 1.2470e-01, 9.9979e-02, 1.9813e-03,\n",
       "         3.3790e-01, 2.8544e-01, 3.9815e-02],\n",
       "        [2.2770e+01, 1.2106e+05, 4.1081e-03, 1.2316e+01, 2.5905e+01, 5.7626e+00,\n",
       "         8.2311e-03, 2.4569e+02, 8.6028e+00, 2.6700e+02, 2.4207e+02, 2.0607e+01,\n",
       "         7.9507e+00, 2.4988e+00, 4.8974e-01, 5.4078e+00, 3.1086e+00, 3.1671e-03,\n",
       "         1.2454e-01, 2.1177e-02, 4.8322e-01, 1.9474e+01, 5.3027e-02, 1.2021e+01,\n",
       "         5.1244e-01, 2.2153e-02, 2.0668e+00],\n",
       "        [4.6799e+00, 1.7112e+05, 3.8737e-02, 1.4431e-03, 1.9587e+01, 3.2529e+01,\n",
       "         1.0290e+02, 9.2264e+03, 3.8340e-03, 7.6019e+02, 2.1977e+04, 3.7547e+00,\n",
       "         2.1989e-02, 2.6977e+01, 3.1028e-01, 2.9484e+02, 5.0262e+00, 1.1711e-01,\n",
       "         6.3210e+02, 1.1132e+01, 6.6990e+00, 2.1002e-01, 3.3866e+00, 5.3002e+01,\n",
       "         7.4369e-01, 1.8737e+00, 3.8824e+01],\n",
       "        [3.4492e-01, 7.1363e+04, 7.3414e-03, 1.0645e-03, 2.2461e+00, 4.6274e+01,\n",
       "         3.9799e+03, 1.2074e+05, 1.5414e-04, 3.5870e+03, 7.1543e+01, 3.7514e+01,\n",
       "         1.6573e-03, 3.2051e-01, 2.9664e+00, 1.8754e+01, 1.1354e-02, 9.0784e-01,\n",
       "         2.1040e+05, 3.5946e-01, 4.0348e+01, 1.5241e+00, 1.0294e+00, 2.6651e+01,\n",
       "         1.5233e-02, 5.9221e+01, 1.2251e+02],\n",
       "        [1.1283e+00, 1.0686e+06, 4.4274e-05, 5.2483e-04, 1.4305e+00, 3.9386e+02,\n",
       "         4.3758e+04, 1.1155e+05, 7.7119e-05, 2.4495e+03, 1.0295e+02, 5.2433e+01,\n",
       "         8.0807e-05, 6.3679e-01, 4.1409e+01, 8.4938e+00, 2.8969e-02, 9.0308e-01,\n",
       "         4.5102e+04, 5.3887e+00, 5.5814e+02, 1.9035e+01, 1.3439e-01, 1.7341e+03,\n",
       "         1.2350e-01, 2.8128e+00, 9.9046e+02],\n",
       "        [2.6788e-01, 1.4257e+04, 5.5321e-03, 3.0473e-04, 1.1784e+01, 5.9359e+02,\n",
       "         2.0279e+05, 1.2754e+05, 1.2027e-04, 5.5374e+02, 8.2308e+02, 1.7165e+00,\n",
       "         7.7283e-05, 8.9161e-01, 1.8976e+01, 8.1165e+01, 3.0756e-01, 2.2945e-01,\n",
       "         1.7164e+05, 3.0265e+00, 1.1920e+02, 1.9943e+00, 1.0958e+00, 1.6095e+02,\n",
       "         3.1092e-01, 1.8827e+02, 1.4492e+02],\n",
       "        [2.1356e-01, 1.2453e+05, 1.4539e-04, 2.9322e-04, 3.0523e+00, 3.6706e+02,\n",
       "         5.6050e+04, 9.8963e+04, 4.8333e-05, 1.5215e+03, 9.3514e+01, 2.2135e+01,\n",
       "         2.1826e-04, 1.2868e-01, 9.0927e+00, 5.7323e+00, 3.3524e-02, 8.2157e-01,\n",
       "         2.2499e+05, 7.1818e-01, 5.1752e+02, 6.9402e+00, 3.3230e-01, 1.9565e+02,\n",
       "         1.9347e-02, 1.0927e+02, 3.5270e+02],\n",
       "        [3.2347e+00, 9.4002e-03, 2.6319e+03, 1.2537e+03, 2.0217e+00, 1.7923e-01,\n",
       "         4.3137e-05, 9.8072e-03, 2.7324e+02, 9.2853e-02, 1.7691e-01, 4.7976e-01,\n",
       "         3.2533e+04, 2.8797e+00, 1.3135e+00, 4.8301e+00, 3.7948e+01, 6.9470e-01,\n",
       "         6.4144e-05, 3.4568e-02, 9.2309e-04, 1.2470e-01, 9.9979e-02, 1.9813e-03,\n",
       "         3.3790e-01, 2.8544e-01, 3.9815e-02],\n",
       "        [6.6167e+00, 1.1429e+00, 4.1871e+01, 2.0186e+03, 7.7817e-01, 5.6185e+00,\n",
       "         1.9430e-03, 1.9871e+01, 6.8409e+00, 5.8402e-01, 1.6972e+00, 1.1018e-01,\n",
       "         3.6852e+02, 1.1326e+01, 2.0073e+00, 9.5601e+00, 1.0639e+02, 7.9955e-02,\n",
       "         7.6826e-04, 4.0044e-03, 3.7064e-02, 3.1406e-01, 2.9805e-02, 3.6111e-02,\n",
       "         1.8600e-02, 4.5952e-01, 4.4524e-02],\n",
       "        [8.1552e+00, 2.5169e+06, 6.2536e-05, 2.7420e-03, 2.4612e-01, 3.9577e+02,\n",
       "         1.8076e+03, 6.4446e+04, 8.3476e-04, 6.0624e+03, 1.6879e+02, 7.9511e+00,\n",
       "         4.9278e-05, 2.9895e+00, 1.9568e+01, 1.1699e+01, 3.5890e-01, 2.5573e-02,\n",
       "         1.0331e+03, 3.1801e+01, 3.8054e+02, 3.5194e+01, 1.8361e-01, 4.9695e+03,\n",
       "         2.4671e-01, 6.9308e-02, 1.7088e+02],\n",
       "        [2.5913e-01, 4.5534e+04, 3.2358e-04, 1.0060e-04, 1.0957e+01, 6.2743e+02,\n",
       "         5.7570e+05, 2.5786e+05, 3.5703e-05, 8.6340e+02, 2.4489e+02, 1.0343e+01,\n",
       "         3.5094e-05, 3.9903e-01, 3.1044e+01, 4.4915e+01, 1.4225e-01, 2.2924e-01,\n",
       "         2.2775e+05, 1.5090e+00, 4.2963e+02, 2.0240e+00, 3.1898e-01, 4.9700e+02,\n",
       "         3.2579e-01, 1.3261e+02, 4.6222e+02],\n",
       "        [3.2347e+00, 9.4002e-03, 2.6319e+03, 1.2537e+03, 2.0217e+00, 1.7923e-01,\n",
       "         4.3137e-05, 9.8072e-03, 2.7324e+02, 9.2853e-02, 1.7691e-01, 4.7976e-01,\n",
       "         3.2533e+04, 2.8797e+00, 1.3135e+00, 4.8301e+00, 3.7948e+01, 6.9470e-01,\n",
       "         6.4144e-05, 3.4568e-02, 9.2309e-04, 1.2470e-01, 9.9979e-02, 1.9813e-03,\n",
       "         3.3790e-01, 2.8544e-01, 3.9815e-02],\n",
       "        [5.2077e+00, 1.6837e-01, 2.9449e+02, 2.5614e+03, 1.8364e+00, 1.1688e+00,\n",
       "         3.6711e-04, 9.8377e-01, 3.1328e+01, 4.1457e-01, 1.0232e+00, 2.6979e-01,\n",
       "         3.7371e+03, 4.2097e+00, 2.0277e+00, 7.9510e+00, 9.1077e+01, 3.1343e-01,\n",
       "         3.4809e-04, 1.1820e-02, 5.5712e-03, 2.8221e-01, 4.1220e-02, 9.5655e-03,\n",
       "         4.8965e-02, 5.6928e-01, 4.2082e-02],\n",
       "        [8.5549e+00, 5.4133e-05, 1.7489e+04, 5.6126e+02, 1.3230e-02, 1.2600e+00,\n",
       "         8.7606e-05, 6.5358e-03, 1.0433e+02, 4.7896e-03, 1.3328e-02, 4.4543e-03,\n",
       "         7.2985e+02, 1.0564e+01, 2.9715e-01, 4.2360e+00, 2.4931e+02, 4.8434e-01,\n",
       "         8.0640e-05, 2.2531e-02, 6.9383e-03, 1.2581e-02, 6.0976e-02, 2.4529e-03,\n",
       "         1.5850e-01, 1.9748e+00, 6.0241e-03],\n",
       "        [1.9735e+02, 1.1249e-03, 1.3654e+00, 1.4514e+03, 4.2873e-02, 3.7281e+02,\n",
       "         9.8670e-02, 1.1030e-02, 1.7703e+02, 5.4249e-02, 2.7399e-02, 2.6199e-02,\n",
       "         2.2803e+01, 1.0778e-01, 2.0857e-01, 1.5869e-02, 4.4884e+00, 2.6122e-01,\n",
       "         1.2955e-04, 1.2240e-03, 6.6471e-01, 1.2444e+03, 3.3567e-02, 1.9191e-03,\n",
       "         4.4056e-03, 2.6349e+00, 3.4615e-03],\n",
       "        [1.5814e+01, 1.9388e-03, 8.1219e+01, 8.4301e+02, 7.5383e+00, 1.4759e+01,\n",
       "         3.3168e-03, 2.0362e-02, 6.1845e+01, 6.2262e-03, 4.4536e+00, 3.2616e-02,\n",
       "         4.3138e+02, 9.5933e+00, 1.4886e-02, 3.7786e+00, 1.7773e+04, 7.5052e-01,\n",
       "         5.0934e-06, 1.7374e-02, 6.1720e-03, 3.3445e-02, 1.3817e+00, 1.1321e+01,\n",
       "         2.4666e+00, 4.0556e+00, 1.1044e-02],\n",
       "        [1.3186e+00, 1.4004e+05, 1.8139e-05, 4.1394e-03, 1.9221e+00, 1.0073e+03,\n",
       "         9.1838e+03, 1.1222e+05, 1.6877e-04, 3.1835e+03, 5.2840e+01, 2.7204e+01,\n",
       "         7.9749e-05, 1.2591e+00, 2.5933e+01, 1.2143e+01, 9.7537e-01, 3.7195e-02,\n",
       "         3.8511e+03, 2.0085e-01, 1.2749e+02, 6.8282e+00, 2.4641e-01, 1.4865e+03,\n",
       "         1.8075e-01, 3.3440e+00, 9.0162e+02],\n",
       "        [1.0283e+00, 1.3709e+05, 4.7557e-04, 3.9933e-04, 3.0079e+00, 5.1982e+02,\n",
       "         6.9487e+04, 2.4997e+04, 1.3153e-04, 2.3806e+03, 1.8595e+03, 5.7374e+00,\n",
       "         5.5275e-05, 3.6798e-01, 6.8956e+00, 2.8930e+01, 1.1113e-01, 5.8684e-01,\n",
       "         1.7437e+05, 1.3956e+01, 1.9995e+02, 2.2844e+01, 3.0809e+00, 3.0348e+02,\n",
       "         1.7277e-01, 4.3329e+01, 1.3916e+02],\n",
       "        [3.0660e-01, 2.3134e+05, 1.1486e-04, 3.6002e-04, 1.7087e+01, 1.1798e+02,\n",
       "         9.1485e+04, 4.5766e+04, 1.4039e-04, 2.4408e+03, 2.3079e+02, 6.3573e+01,\n",
       "         4.0167e-04, 2.6712e-01, 5.8905e+00, 5.1514e+00, 3.6768e-02, 7.0047e-01,\n",
       "         1.1910e+05, 7.1006e-01, 3.6712e+02, 3.9447e+00, 3.1963e-01, 2.4078e+02,\n",
       "         2.0942e-01, 2.9501e+01, 9.0667e+02],\n",
       "        [5.7049e-01, 4.7240e+04, 1.8357e-02, 5.8520e-03, 1.3526e+00, 1.0663e+02,\n",
       "         4.4086e+02, 1.3696e+05, 4.7647e-04, 1.0443e+03, 8.9493e+01, 1.5359e+01,\n",
       "         2.3274e-03, 3.1911e+00, 2.1895e+00, 2.5109e+01, 3.6745e-02, 2.9391e-01,\n",
       "         1.9769e+04, 1.9774e-01, 1.5446e+01, 7.5990e-01, 5.0134e-01, 2.9135e+01,\n",
       "         1.2414e-02, 8.5025e+00, 4.4539e+01],\n",
       "        [3.2347e+00, 9.4002e-03, 2.6319e+03, 1.2537e+03, 2.0217e+00, 1.7923e-01,\n",
       "         4.3137e-05, 9.8072e-03, 2.7324e+02, 9.2853e-02, 1.7691e-01, 4.7976e-01,\n",
       "         3.2533e+04, 2.8797e+00, 1.3135e+00, 4.8301e+00, 3.7948e+01, 6.9470e-01,\n",
       "         6.4144e-05, 3.4568e-02, 9.2309e-04, 1.2470e-01, 9.9979e-02, 1.9813e-03,\n",
       "         3.3790e-01, 2.8544e-01, 3.9815e-02],\n",
       "        [1.2804e+01, 7.0197e-05, 9.4199e+03, 2.7871e+03, 4.2085e-02, 7.6077e-02,\n",
       "         1.4165e-05, 1.1750e-03, 1.3094e+03, 1.9541e-03, 7.8098e-03, 2.4994e-02,\n",
       "         5.6184e+03, 8.4706e+00, 1.6140e-01, 2.6086e+00, 1.1249e+02, 2.8503e-01,\n",
       "         7.0257e-06, 4.0192e-03, 2.4923e-03, 1.8499e-02, 4.3517e-02, 9.6827e-04,\n",
       "         3.2341e-01, 3.5868e-01, 9.8599e-03],\n",
       "        [7.8482e+01, 8.7583e-01, 7.6421e-02, 4.0251e+02, 3.3539e-01, 9.9403e+00,\n",
       "         5.0828e-03, 1.9653e-01, 9.2297e+02, 8.4135e-01, 3.7263e+00, 1.2609e+00,\n",
       "         2.5900e+02, 9.8574e-02, 1.9702e-01, 2.6434e-01, 2.4384e-01, 9.1186e-02,\n",
       "         7.2472e-04, 4.4829e-03, 1.2848e-02, 4.7279e+04, 8.1187e-02, 1.5598e-02,\n",
       "         1.6761e-03, 2.7585e-01, 1.0714e-02],\n",
       "        [5.8233e+00, 7.4980e+00, 6.1927e+02, 9.6293e+00, 6.5379e+03, 5.7820e+00,\n",
       "         4.5671e-03, 9.8462e-03, 1.2074e+01, 1.0625e+01, 6.3117e+01, 6.7054e-01,\n",
       "         7.2903e+03, 1.1719e+02, 1.6284e-02, 7.7130e+02, 2.3973e+04, 8.3117e-02,\n",
       "         1.4989e-02, 1.0286e+00, 1.1476e-03, 2.9833e-04, 3.0736e+00, 8.4516e+01,\n",
       "         4.2134e+04, 4.7049e-01, 3.4353e-01],\n",
       "        [3.4693e-01, 6.2413e+05, 4.2710e-05, 3.8330e-03, 1.0032e+01, 6.4418e+01,\n",
       "         1.7455e+03, 6.7499e+05, 1.1981e-04, 3.2438e+02, 2.8537e+01, 1.3518e+02,\n",
       "         3.4885e-04, 1.7364e+01, 1.4259e+01, 7.4050e+00, 6.3986e-03, 7.0270e-01,\n",
       "         5.9693e+04, 1.0952e+00, 5.8570e+02, 1.2585e+00, 9.9373e-03, 3.7723e+02,\n",
       "         1.8876e-02, 1.2198e+00, 4.3559e+02],\n",
       "        [2.3763e+00, 7.0504e+04, 4.7707e-01, 1.0368e-03, 4.0867e-02, 3.5611e+02,\n",
       "         6.2235e+03, 1.7568e+05, 1.1476e-03, 1.0467e+04, 7.7576e+01, 1.8111e-01,\n",
       "         3.5615e-05, 1.3797e+00, 1.5487e+00, 1.8747e+02, 1.8745e-03, 4.7909e+01,\n",
       "         9.9393e+04, 9.3581e+01, 5.5538e+00, 1.1500e+03, 4.7002e+02, 1.9311e+02,\n",
       "         1.1043e-04, 3.0658e+02, 3.4175e-03],\n",
       "        [1.4355e+00, 1.2084e+04, 1.0451e-05, 4.5924e-03, 1.6490e+00, 1.2377e+03,\n",
       "         5.7300e+04, 3.9899e+04, 2.6286e-04, 2.8127e+03, 4.5617e+01, 2.6224e+01,\n",
       "         2.2239e-05, 9.8663e-02, 1.2658e+01, 9.2541e+00, 2.6427e+00, 1.9864e-02,\n",
       "         2.7810e+04, 3.2265e-03, 2.0426e+02, 6.2516e+00, 5.2132e-01, 1.8927e+02,\n",
       "         5.2500e-01, 1.7682e+02, 1.1833e+03]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:25.742116Z",
     "start_time": "2025-07-16T14:07:25.732063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "probs  = counts/counts.sum(1, keepdims = True )\n",
    "probs"
   ],
   "id": "a52434184c6df994",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.8026e-05, 2.5581e-07, 7.1621e-02, 3.4118e-02, 5.5016e-05, 4.8775e-06,\n",
       "         1.1739e-09, 2.6688e-07, 7.4357e-03, 2.5268e-06, 4.8143e-06, 1.3056e-05,\n",
       "         8.8533e-01, 7.8366e-05, 3.5744e-05, 1.3144e-04, 1.0327e-03, 1.8905e-05,\n",
       "         1.7456e-09, 9.4071e-07, 2.5120e-08, 3.3934e-06, 2.7207e-06, 5.3918e-08,\n",
       "         9.1954e-06, 7.7676e-06, 1.0835e-06],\n",
       "        [1.3510e-04, 8.5753e-01, 2.8771e-08, 9.1144e-04, 2.3577e-05, 2.8930e-03,\n",
       "         1.1363e-06, 1.3510e-01, 1.0171e-05, 6.2037e-04, 3.6741e-04, 3.8903e-05,\n",
       "         3.6157e-06, 1.0998e-03, 9.1562e-05, 1.2886e-04, 5.7263e-05, 1.6793e-08,\n",
       "         1.5214e-06, 2.0398e-07, 5.9781e-05, 4.8600e-05, 2.0161e-07, 8.3656e-04,\n",
       "         9.3394e-07, 1.5607e-07, 4.0519e-05],\n",
       "        [1.7764e-05, 8.9966e-01, 1.9690e-09, 8.9059e-10, 1.5217e-06, 3.4974e-04,\n",
       "         1.0787e-02, 1.7230e-02, 1.7102e-09, 9.1721e-03, 2.7977e-02, 5.0278e-06,\n",
       "         9.5084e-11, 1.0876e-06, 4.2573e-06, 1.1742e-04, 4.9188e-07, 2.9133e-07,\n",
       "         3.2961e-02, 4.4562e-04, 1.0924e-04, 9.1032e-05, 2.1394e-05, 1.0126e-03,\n",
       "         2.2562e-07, 4.7421e-06, 2.5615e-05],\n",
       "        [5.3979e-07, 4.4988e-01, 9.6094e-11, 2.4966e-10, 4.6963e-05, 2.1729e-04,\n",
       "         2.9097e-01, 1.0147e-01, 6.6034e-11, 2.4640e-03, 4.4696e-04, 1.3711e-04,\n",
       "         5.1042e-10, 5.3782e-07, 1.3139e-05, 1.0623e-05, 2.6245e-08, 2.2424e-06,\n",
       "         1.5120e-01, 4.5869e-06, 7.8940e-04, 7.7074e-06, 3.1411e-07, 7.6363e-04,\n",
       "         3.1867e-07, 2.6475e-05, 1.5541e-03],\n",
       "        [1.1040e-06, 2.2655e-01, 5.5406e-09, 1.1293e-09, 4.3541e-06, 7.6914e-05,\n",
       "         5.1488e-03, 4.4771e-01, 2.6671e-10, 1.7530e-03, 1.7442e-04, 1.0104e-04,\n",
       "         1.7188e-09, 2.3919e-06, 9.8531e-06, 5.8068e-05, 1.1039e-08, 1.4683e-06,\n",
       "         3.1774e-01, 4.3514e-06, 1.0013e-04, 1.2968e-06, 8.2893e-07, 1.9807e-04,\n",
       "         5.3394e-08, 2.5222e-05, 3.4139e-04],\n",
       "        [8.8026e-05, 2.5581e-07, 7.1621e-02, 3.4118e-02, 5.5016e-05, 4.8775e-06,\n",
       "         1.1739e-09, 2.6688e-07, 7.4357e-03, 2.5268e-06, 4.8143e-06, 1.3056e-05,\n",
       "         8.8533e-01, 7.8366e-05, 3.5744e-05, 1.3144e-04, 1.0327e-03, 1.8905e-05,\n",
       "         1.7456e-09, 9.4071e-07, 2.5120e-08, 3.3934e-06, 2.7207e-06, 5.3918e-08,\n",
       "         9.1954e-06, 7.7676e-06, 1.0835e-06],\n",
       "        [1.8669e-04, 9.9258e-01, 3.3682e-08, 1.0097e-04, 2.1239e-04, 4.7247e-05,\n",
       "         6.7485e-08, 2.0143e-03, 7.0533e-05, 2.1891e-03, 1.9847e-03, 1.6896e-04,\n",
       "         6.5186e-05, 2.0487e-05, 4.0153e-06, 4.4338e-05, 2.5487e-05, 2.5967e-08,\n",
       "         1.0211e-06, 1.7362e-07, 3.9618e-06, 1.5966e-04, 4.3476e-07, 9.8558e-05,\n",
       "         4.2014e-06, 1.8163e-07, 1.6945e-05],\n",
       "        [2.2905e-05, 8.3749e-01, 1.8959e-07, 7.0631e-09, 9.5866e-05, 1.5921e-04,\n",
       "         5.0364e-04, 4.5157e-02, 1.8765e-08, 3.7206e-03, 1.0756e-01, 1.8377e-05,\n",
       "         1.0762e-07, 1.3204e-04, 1.5186e-06, 1.4431e-03, 2.4600e-05, 5.7316e-07,\n",
       "         3.0937e-03, 5.4486e-05, 3.2787e-05, 1.0279e-06, 1.6575e-05, 2.5941e-04,\n",
       "         3.6398e-06, 9.1707e-06, 1.9002e-04],\n",
       "        [8.4022e-07, 1.7384e-01, 1.7884e-08, 2.5931e-09, 5.4716e-06, 1.1273e-04,\n",
       "         9.6951e-03, 2.9413e-01, 3.7550e-10, 8.7380e-03, 1.7428e-04, 9.1385e-05,\n",
       "         4.0373e-09, 7.8077e-07, 7.2263e-06, 4.5684e-05, 2.7658e-08, 2.2115e-06,\n",
       "         5.1254e-01, 8.7566e-07, 9.8288e-05, 3.7128e-06, 2.5078e-06, 6.4922e-05,\n",
       "         3.7107e-08, 1.4426e-04, 2.9844e-04],\n",
       "        [8.8463e-07, 8.3787e-01, 3.4714e-11, 4.1150e-10, 1.1216e-06, 3.0881e-04,\n",
       "         3.4309e-02, 8.7465e-02, 6.0466e-11, 1.9206e-03, 8.0719e-05, 4.1111e-05,\n",
       "         6.3358e-11, 4.9928e-07, 3.2467e-05, 6.6597e-06, 2.2714e-08, 7.0808e-07,\n",
       "         3.5363e-02, 4.2251e-06, 4.3762e-04, 1.4925e-05, 1.0537e-07, 1.3597e-03,\n",
       "         9.6835e-08, 2.2055e-06, 7.7658e-04],\n",
       "        [5.1621e-07, 2.7475e-02, 1.0660e-08, 5.8723e-10, 2.2707e-05, 1.1439e-03,\n",
       "         3.9078e-01, 2.4578e-01, 2.3176e-10, 1.0671e-03, 1.5861e-03, 3.3078e-06,\n",
       "         1.4893e-10, 1.7182e-06, 3.6567e-05, 1.5641e-04, 5.9267e-07, 4.4216e-07,\n",
       "         3.3075e-01, 5.8321e-06, 2.2970e-04, 3.8431e-06, 2.1117e-06, 3.1015e-04,\n",
       "         5.9916e-07, 3.6280e-04, 2.7927e-04],\n",
       "        [4.2062e-07, 2.4526e-01, 2.8635e-10, 5.7750e-10, 6.0116e-06, 7.2293e-04,\n",
       "         1.1039e-01, 1.9491e-01, 9.5191e-11, 2.9966e-03, 1.8418e-04, 4.3595e-05,\n",
       "         4.2987e-10, 2.5344e-07, 1.7908e-05, 1.1290e-05, 6.6025e-08, 1.6181e-06,\n",
       "         4.4313e-01, 1.4145e-06, 1.0193e-03, 1.3669e-05, 6.5447e-07, 3.8534e-04,\n",
       "         3.8104e-08, 2.1521e-04, 6.9464e-04],\n",
       "        [8.8026e-05, 2.5581e-07, 7.1621e-02, 3.4118e-02, 5.5016e-05, 4.8775e-06,\n",
       "         1.1739e-09, 2.6688e-07, 7.4357e-03, 2.5268e-06, 4.8143e-06, 1.3056e-05,\n",
       "         8.8533e-01, 7.8366e-05, 3.5744e-05, 1.3144e-04, 1.0327e-03, 1.8905e-05,\n",
       "         1.7456e-09, 9.4071e-07, 2.5120e-08, 3.3934e-06, 2.7207e-06, 5.3918e-08,\n",
       "         9.1954e-06, 7.7676e-06, 1.0835e-06],\n",
       "        [2.5424e-03, 4.3916e-04, 1.6089e-02, 7.7562e-01, 2.9900e-04, 2.1588e-03,\n",
       "         7.4658e-07, 7.6353e-03, 2.6285e-03, 2.2440e-04, 6.5211e-04, 4.2335e-05,\n",
       "         1.4160e-01, 4.3519e-03, 7.7127e-04, 3.6733e-03, 4.0879e-02, 3.0722e-05,\n",
       "         2.9519e-07, 1.5386e-06, 1.4241e-05, 1.2067e-04, 1.1452e-05, 1.3875e-05,\n",
       "         7.1469e-06, 1.7656e-04, 1.7108e-05],\n",
       "        [3.1409e-06, 9.6936e-01, 2.4085e-11, 1.0560e-09, 9.4792e-08, 1.5243e-04,\n",
       "         6.9620e-04, 2.4821e-02, 3.2150e-10, 2.3349e-03, 6.5009e-05, 3.0623e-06,\n",
       "         1.8979e-11, 1.1514e-06, 7.5364e-06, 4.5058e-06, 1.3823e-07, 9.8491e-09,\n",
       "         3.9787e-04, 1.2248e-05, 1.4656e-04, 1.3555e-05, 7.0716e-08, 1.9140e-03,\n",
       "         9.5016e-08, 2.6693e-08, 6.5811e-05],\n",
       "        [2.3341e-07, 4.1014e-02, 2.9146e-10, 9.0615e-11, 9.8691e-06, 5.6514e-04,\n",
       "         5.1855e-01, 2.3226e-01, 3.2159e-11, 7.7770e-04, 2.2058e-04, 9.3161e-06,\n",
       "         3.1610e-11, 3.5942e-07, 2.7962e-05, 4.0456e-05, 1.2813e-07, 2.0649e-07,\n",
       "         2.0515e-01, 1.3592e-06, 3.8699e-04, 1.8231e-06, 2.8731e-07, 4.4766e-04,\n",
       "         2.9345e-07, 1.1944e-04, 4.1633e-04],\n",
       "        [8.8026e-05, 2.5581e-07, 7.1621e-02, 3.4118e-02, 5.5016e-05, 4.8775e-06,\n",
       "         1.1739e-09, 2.6688e-07, 7.4357e-03, 2.5268e-06, 4.8143e-06, 1.3056e-05,\n",
       "         8.8533e-01, 7.8366e-05, 3.5744e-05, 1.3144e-04, 1.0327e-03, 1.8905e-05,\n",
       "         1.7456e-09, 9.4071e-07, 2.5120e-08, 3.3934e-06, 2.7207e-06, 5.3918e-08,\n",
       "         9.1954e-06, 7.7676e-06, 1.0835e-06],\n",
       "        [7.7244e-04, 2.4973e-05, 4.3680e-02, 3.7992e-01, 2.7238e-04, 1.7336e-04,\n",
       "         5.4452e-08, 1.4592e-04, 4.6467e-03, 6.1491e-05, 1.5176e-04, 4.0016e-05,\n",
       "         5.5430e-01, 6.2441e-04, 3.0076e-04, 1.1793e-03, 1.3509e-02, 4.6490e-05,\n",
       "         5.1631e-08, 1.7533e-06, 8.2636e-07, 4.1859e-05, 6.1139e-06, 1.4188e-06,\n",
       "         7.2628e-06, 8.4439e-05, 6.2418e-06],\n",
       "        [4.4646e-04, 2.8250e-09, 9.1272e-01, 2.9290e-02, 6.9043e-07, 6.5757e-05,\n",
       "         4.5719e-09, 3.4109e-07, 5.4446e-03, 2.4996e-07, 6.9557e-07, 2.3246e-07,\n",
       "         3.8089e-02, 5.5131e-04, 1.5507e-05, 2.2107e-04, 1.3011e-02, 2.5276e-05,\n",
       "         4.2084e-09, 1.1758e-06, 3.6209e-07, 6.5655e-07, 3.1822e-06, 1.2801e-07,\n",
       "         8.2715e-06, 1.0306e-04, 3.1438e-07],\n",
       "        [5.6776e-02, 3.2362e-07, 3.9282e-04, 4.1757e-01, 1.2334e-05, 1.0726e-01,\n",
       "         2.8387e-05, 3.1732e-06, 5.0930e-02, 1.5607e-05, 7.8825e-06, 7.5373e-06,\n",
       "         6.5605e-03, 3.1008e-05, 6.0006e-05, 4.5656e-06, 1.2913e-03, 7.5151e-05,\n",
       "         3.7271e-08, 3.5214e-07, 1.9123e-04, 3.5802e-01, 9.6571e-06, 5.5211e-07,\n",
       "         1.2675e-06, 7.5804e-04, 9.9586e-07],\n",
       "        [8.2079e-04, 1.0063e-07, 4.2155e-03, 4.3756e-02, 3.9127e-04, 7.6607e-04,\n",
       "         1.7216e-07, 1.0568e-06, 3.2100e-03, 3.2316e-07, 2.3116e-04, 1.6929e-06,\n",
       "         2.2390e-02, 4.9793e-04, 7.7263e-07, 1.9612e-04, 9.2248e-01, 3.8955e-05,\n",
       "         2.6437e-10, 9.0180e-07, 3.2035e-07, 1.7359e-06, 7.1717e-05, 5.8762e-04,\n",
       "         1.2802e-04, 2.1050e-04, 5.7322e-07],\n",
       "        [4.8453e-06, 5.1461e-01, 6.6654e-11, 1.5211e-08, 7.0632e-06, 3.7014e-03,\n",
       "         3.3747e-02, 4.1235e-01, 6.2018e-10, 1.1698e-02, 1.9417e-04, 9.9966e-05,\n",
       "         2.9305e-10, 4.6269e-06, 9.5295e-05, 4.4623e-05, 3.5841e-06, 1.3668e-07,\n",
       "         1.4152e-02, 7.3807e-07, 4.6849e-04, 2.5091e-05, 9.0545e-07, 5.4625e-03,\n",
       "         6.6421e-07, 1.2288e-05, 3.3131e-03],\n",
       "        [2.4991e-06, 3.3316e-01, 1.1558e-09, 9.7049e-10, 7.3101e-06, 1.2633e-03,\n",
       "         1.6887e-01, 6.0749e-02, 3.1965e-10, 5.7855e-03, 4.5191e-03, 1.3944e-05,\n",
       "         1.3433e-10, 8.9428e-07, 1.6758e-05, 7.0308e-05, 2.7006e-07, 1.4262e-06,\n",
       "         4.2377e-01, 3.3917e-05, 4.8593e-04, 5.5517e-05, 7.4874e-06, 7.3754e-04,\n",
       "         4.1987e-07, 1.0530e-04, 3.3819e-04],\n",
       "        [6.2301e-07, 4.7008e-01, 2.3340e-10, 7.3157e-10, 3.4720e-05, 2.3973e-04,\n",
       "         1.8590e-01, 9.2996e-02, 2.8527e-10, 4.9597e-03, 4.6896e-04, 1.2918e-04,\n",
       "         8.1620e-10, 5.4279e-07, 1.1970e-05, 1.0468e-05, 7.4714e-08, 1.4234e-06,\n",
       "         2.4202e-01, 1.4429e-06, 7.4599e-04, 8.0157e-06, 6.4949e-07, 4.8928e-04,\n",
       "         4.2554e-07, 5.9947e-05, 1.8424e-03],\n",
       "        [2.7721e-06, 2.2955e-01, 8.9202e-08, 2.8436e-08, 6.5726e-06, 5.1816e-04,\n",
       "         2.1422e-03, 6.6550e-01, 2.3153e-09, 5.0747e-03, 4.3487e-04, 7.4633e-05,\n",
       "         1.1309e-08, 1.5507e-05, 1.0639e-05, 1.2201e-04, 1.7855e-07, 1.4282e-06,\n",
       "         9.6060e-02, 9.6086e-07, 7.5056e-05, 3.6925e-06, 2.4361e-06, 1.4158e-04,\n",
       "         6.0320e-08, 4.1315e-05, 2.1642e-04],\n",
       "        [8.8026e-05, 2.5581e-07, 7.1621e-02, 3.4118e-02, 5.5016e-05, 4.8775e-06,\n",
       "         1.1739e-09, 2.6688e-07, 7.4357e-03, 2.5268e-06, 4.8143e-06, 1.3056e-05,\n",
       "         8.8533e-01, 7.8366e-05, 3.5744e-05, 1.3144e-04, 1.0327e-03, 1.8905e-05,\n",
       "         1.7456e-09, 9.4071e-07, 2.5120e-08, 3.3934e-06, 2.7207e-06, 5.3918e-08,\n",
       "         9.1954e-06, 7.7676e-06, 1.0835e-06],\n",
       "        [6.6435e-04, 3.6423e-09, 4.8877e-01, 1.4461e-01, 2.1837e-06, 3.9474e-06,\n",
       "         7.3497e-10, 6.0965e-08, 6.7943e-02, 1.0139e-07, 4.0523e-07, 1.2969e-06,\n",
       "         2.9152e-01, 4.3952e-04, 8.3747e-06, 1.3536e-04, 5.8367e-03, 1.4789e-05,\n",
       "         3.6454e-10, 2.0855e-07, 1.2932e-07, 9.5988e-07, 2.2580e-06, 5.0241e-08,\n",
       "         1.6781e-05, 1.8611e-05, 5.1161e-07],\n",
       "        [1.6030e-03, 1.7889e-05, 1.5609e-06, 8.2211e-03, 6.8504e-06, 2.0303e-04,\n",
       "         1.0382e-07, 4.0141e-06, 1.8852e-02, 1.7184e-05, 7.6109e-05, 2.5754e-05,\n",
       "         5.2901e-03, 2.0134e-06, 4.0240e-06, 5.3990e-06, 4.9803e-06, 1.8625e-06,\n",
       "         1.4802e-08, 9.1563e-08, 2.6241e-07, 9.6566e-01, 1.6582e-06, 3.1859e-07,\n",
       "         3.4233e-08, 5.6342e-06, 2.1883e-07],\n",
       "        [7.1322e-05, 9.1833e-05, 7.5846e-03, 1.1794e-04, 8.0074e-02, 7.0816e-05,\n",
       "         5.5937e-08, 1.2059e-07, 1.4788e-04, 1.3013e-04, 7.7304e-04, 8.2126e-06,\n",
       "         8.9289e-02, 1.4354e-03, 1.9944e-07, 9.4467e-03, 2.9361e-01, 1.0180e-06,\n",
       "         1.8358e-07, 1.2598e-05, 1.4055e-08, 3.6538e-09, 3.7644e-05, 1.0351e-03,\n",
       "         5.1605e-01, 5.7624e-06, 4.2074e-06],\n",
       "        [2.5462e-07, 4.5806e-01, 3.1345e-11, 2.8131e-09, 7.3624e-06, 4.7277e-05,\n",
       "         1.2810e-03, 4.9538e-01, 8.7929e-11, 2.3806e-04, 2.0944e-05, 9.9213e-05,\n",
       "         2.5603e-10, 1.2743e-05, 1.0465e-05, 5.4346e-06, 4.6960e-09, 5.1572e-07,\n",
       "         4.3809e-02, 8.0379e-07, 4.2985e-04, 9.2365e-07, 7.2931e-09, 2.7686e-04,\n",
       "         1.3854e-08, 8.9519e-07, 3.1968e-04],\n",
       "        [6.5075e-06, 1.9308e-01, 1.3065e-06, 2.8394e-09, 1.1191e-07, 9.7522e-04,\n",
       "         1.7043e-02, 4.8110e-01, 3.1427e-09, 2.8665e-02, 2.1244e-04, 4.9597e-07,\n",
       "         9.7533e-11, 3.7784e-06, 4.2412e-06, 5.1339e-04, 5.1332e-09, 1.3120e-04,\n",
       "         2.7219e-01, 2.5627e-04, 1.5209e-05, 3.1493e-03, 1.2871e-03, 5.2883e-04,\n",
       "         3.0242e-10, 8.3957e-04, 9.3588e-09],\n",
       "        [1.0038e-05, 8.4502e-02, 7.3080e-11, 3.2113e-08, 1.1531e-05, 8.6548e-03,\n",
       "         4.0069e-01, 2.7901e-01, 1.8381e-09, 1.9669e-02, 3.1899e-04, 1.8338e-04,\n",
       "         1.5551e-10, 6.8992e-07, 8.8516e-05, 6.4712e-05, 1.8480e-05, 1.3890e-07,\n",
       "         1.9447e-01, 2.2562e-08, 1.4283e-03, 4.3716e-05, 3.6454e-06, 1.3235e-03,\n",
       "         3.6712e-06, 1.2364e-03, 8.2746e-03]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:25.934633Z",
     "start_time": "2025-07-16T14:07:25.931047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Probs gives us the probability of what character comes next in the sequence, higher probability = high chance of coming next.\n",
    "# so now i want to see what is the predicted probability for the right label i.e., we know true o/p from training data\n",
    "# now we pluck out what model gave the probability for tru o/p\n",
    "# that can be achieved by for o/p 1 its row 1 and column is going to be the true o/p form emma its e i.r., 5\n",
    "# so in prob matrix to see what is the probability of predicting e is [1,5]\n",
    "# for second o/p its [2,11]\n",
    "# for third o/p its [3,5]\n",
    "\n",
    "#inshortcut this can be written as [torch.arange(32),Y] because y has its all labels"
   ],
   "id": "3a4b95f079b3088f",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:26.084051Z",
     "start_time": "2025-07-16T14:07:26.078699Z"
    }
   },
   "cell_type": "code",
   "source": "probs[torch.arange(32), Y] # this is given the predicted probabilities for true label",
   "id": "478c5de482d1d67e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.8775e-06, 1.0998e-03, 1.0876e-06, 4.4988e-01, 1.1040e-06, 1.3144e-04,\n",
       "        6.5186e-05, 3.7206e-03, 2.5078e-06, 1.9206e-03, 2.7475e-02, 4.2062e-07,\n",
       "        2.5581e-07, 1.1452e-05, 9.6936e-01, 2.3341e-07, 2.5268e-06, 1.7533e-06,\n",
       "        2.8250e-09, 3.9282e-04, 7.6607e-04, 2.9305e-10, 1.3433e-10, 4.7008e-01,\n",
       "        2.7721e-06, 9.4071e-07, 1.3536e-04, 4.9803e-06, 1.4788e-04, 2.3806e-04,\n",
       "        1.9308e-01, 1.0038e-05])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:26.380446Z",
     "start_time": "2025-07-16T14:07:26.373640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss = -probs[torch.arange(32), Y].log().mean()\n",
    "loss # this is the loss of correct prediction"
   ],
   "id": "9522075bb7c83bbc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.4728)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:07:58.807719Z",
     "start_time": "2025-07-16T14:07:58.804047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Summarizing everything nd writing together\n",
    "X#input"
   ],
   "id": "466dbbf976dfc201",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:08:14.594055Z",
     "start_time": "2025-07-16T14:08:14.588319Z"
    }
   },
   "cell_type": "code",
   "source": "Y #o/p",
   "id": "df832b8a3a12a150",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:53:13.688575Z",
     "start_time": "2025-07-16T14:53:13.681732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # this is used for generating same weights from the tutorial\n",
    "C = torch.rand((27,2), generator=g) #Embedding layer( we choosed 2 dimensional embedding for 27 charcters\n",
    "W1 = torch.randn((6,100), generator=g)\n",
    "B1 = torch.rand(100, generator = g)\n",
    "W2 = torch.rand((100,27), generator = g)\n",
    "B2 = torch.rand(27, generator = g)\n",
    "parameters = [C, W1, B1, W2, B2]\n",
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ],
   "id": "6411b32771043a1e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:39:33.718721Z",
     "start_time": "2025-07-16T14:39:33.711209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding = C[X] #(32,3,2)\n",
    "Hidden_Layer = torch.tanh(embedding.view(-1,6)@ W1 + B1) #(32, 100)\n",
    "logits = Hidden_Layer @ W2 + B2 # (32,27)\n",
    "counts = logits.exp()\n",
    "probability = counts/counts.sum(1, keepdims = True)\n",
    "loss = -probability[torch.arange(32),Y].log().mean()\n",
    "loss"
   ],
   "id": "45b83a4e5f5b88b2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.5415)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:39:34.081343Z",
     "start_time": "2025-07-16T14:39:34.075822Z"
    }
   },
   "cell_type": "code",
   "source": "F.cross_entropy(logits,Y)# this step is same step we do for calculating the loss",
   "id": "7cc0313d760c5b6e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.5415)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:39:34.620336Z",
     "start_time": "2025-07-16T14:39:34.615470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\"\n",
    "counts = logits.exp()\n",
    "probability = counts/counts.sum(1, keepdims = True)\n",
    "loss = -probability[torch.arange(32),Y].log().mean()\n",
    "\n",
    "=\n",
    "\n",
    "F.cross_entropy(logits,Y)# this step is same step we do for calculating the loss\n",
    "\n",
    "# but F.cross_entrophy is very effective the above stepo we did is very ineffective but only to understading we did that\n",
    "\n",
    "in future allway use F.cross_entrophy which is memory efficenta nd does all calculations correctly\n",
    "\n",
    "\n",
    "make sure you allways use cross_entrophy ( because in many scenarios our manual calculation of loss fails)\n",
    "\n",
    "Forward and  backward pass will be much efficient when we us ethe cross entrophy\n",
    "\n",
    "\"\"\""
   ],
   "id": "cd50caae0fb69eec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\ncounts = logits.exp()\\nprobability = counts/counts.sum(1, keepdims = True)\\nloss = -probability[torch.arange(32),Y].log().mean()\\n\\n=\\n\\nF.cross_entropy(logits,Y)# this step is same step we do for calculating the loss\\n\\n# but F.cross_entrophy is very effective the above stepo we did is very ineffective but only to understading we did that\\n\\nin future allway use F.cross_entrophy which is memory efficenta nd does all calculations correctly\\n\\n\\nmake sure you allways use cross_entrophy ( because in many scenarios our manual calculation of loss fails)\\n\\nForward and  backward pass will be much efficient when we us ethe cross entrophy\\n\\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cleaner version of modeling",
   "id": "42860d664ea65822"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T18:17:58.434009Z",
     "start_time": "2025-07-16T18:17:58.429020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # this is used for generating same weights from the tutorial\n",
    "C = torch.rand((27,2), generator=g) #Embedding layer( we choosed 2 dimensional embedding for 27 charcters\n",
    "W1 = torch.randn((6,100), generator=g)\n",
    "B1 = torch.rand(100, generator = g)\n",
    "W2 = torch.rand((100,27), generator = g)\n",
    "B2 = torch.rand(27, generator = g)\n",
    "parameters = [C, W1, B1, W2, B2]\n",
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ],
   "id": "37e8329bbc0cf804",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T18:17:59.557293Z",
     "start_time": "2025-07-16T18:17:59.552954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ],
   "id": "32e0cefb203b231f",
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:39:36.475370Z",
     "start_time": "2025-07-16T14:39:36.471088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Forward pass\n",
    "embedding = C[X] #(32,3,2)\n",
    "Hidden_Layer = torch.tanh(embedding.view(-1,6)@ W1 + B1) #(32, 100)\n",
    "logits = Hidden_Layer @ W2 + B2 # (32,27)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "print(loss.item())\n",
    "\n",
    "# backward pass\n",
    "\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "loss.backward()\n",
    "# update the weights\n",
    "for p in parameters:\n",
    "    p.data = p.data + (-0.1 * p.grad)\n"
   ],
   "id": "3eae73b043b745f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.54152774810791\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:40:10.084069Z",
     "start_time": "2025-07-16T14:40:09.653820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# so every time we run this the weights keeps on updating\n",
    "\n",
    "for _ in range(1000):\n",
    "    # Forward pass\n",
    "    embedding = C[X] #(32,3,2)\n",
    "    Hidden_Layer = torch.tanh(embedding.view(-1,6)@ W1 + B1) #(32, 100)\n",
    "    logits = Hidden_Layer @ W2 + B2 # (32,27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update the weights\n",
    "    for p in parameters:\n",
    "        p.data = p.data + (-0.1 * p.grad)"
   ],
   "id": "9c1a1f69f10e256f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3187161684036255\n",
      "1.3123947381973267\n",
      "1.306126356124878\n",
      "1.2999099493026733\n",
      "1.2937445640563965\n",
      "1.2876286506652832\n",
      "1.2815616130828857\n",
      "1.2755415439605713\n",
      "1.2695677280426025\n",
      "1.2636388540267944\n",
      "1.2577543258666992\n",
      "1.2519116401672363\n",
      "1.246111273765564\n",
      "1.240350604057312\n",
      "1.2346291542053223\n",
      "1.2289457321166992\n",
      "1.2232999801635742\n",
      "1.217689037322998\n",
      "1.2121131420135498\n",
      "1.206571102142334\n",
      "1.201061725616455\n",
      "1.1955833435058594\n",
      "1.1901358366012573\n",
      "1.1847176551818848\n",
      "1.1793279647827148\n",
      "1.1739660501480103\n",
      "1.168630838394165\n",
      "1.1633211374282837\n",
      "1.1580369472503662\n",
      "1.1527764797210693\n",
      "1.147539496421814\n",
      "1.1423258781433105\n",
      "1.1371334791183472\n",
      "1.1319626569747925\n",
      "1.1268126964569092\n",
      "1.1216825246810913\n",
      "1.1165722608566284\n",
      "1.1114810705184937\n",
      "1.1064084768295288\n",
      "1.1013541221618652\n",
      "1.0963175296783447\n",
      "1.0912986993789673\n",
      "1.0862971544265747\n",
      "1.0813122987747192\n",
      "1.076344609260559\n",
      "1.0713934898376465\n",
      "1.0664587020874023\n",
      "1.0615407228469849\n",
      "1.0566387176513672\n",
      "1.0517534017562866\n",
      "1.046884298324585\n",
      "1.0420315265655518\n",
      "1.0371956825256348\n",
      "1.0323762893676758\n",
      "1.0275739431381226\n",
      "1.0227885246276855\n",
      "1.0180203914642334\n",
      "1.0132694244384766\n",
      "1.0085363388061523\n",
      "1.0038211345672607\n",
      "0.99912428855896\n",
      "0.9944460391998291\n",
      "0.9897865056991577\n",
      "0.9851459860801697\n",
      "0.9805251359939575\n",
      "0.9759241342544556\n",
      "0.9713430404663086\n",
      "0.96678227186203\n",
      "0.9622424244880676\n",
      "0.9577233791351318\n",
      "0.9532257318496704\n",
      "0.9487501978874207\n",
      "0.9442960023880005\n",
      "0.9398642182350159\n",
      "0.9354547262191772\n",
      "0.9310682415962219\n",
      "0.9267047047615051\n",
      "0.9223639965057373\n",
      "0.9180466532707214\n",
      "0.9137534499168396\n",
      "0.9094836115837097\n",
      "0.9052375555038452\n",
      "0.9010158777236938\n",
      "0.8968178629875183\n",
      "0.8926446437835693\n",
      "0.888495922088623\n",
      "0.8843715786933899\n",
      "0.8802716732025146\n",
      "0.8761969208717346\n",
      "0.8721467852592468\n",
      "0.868121862411499\n",
      "0.8641215562820435\n",
      "0.8601463437080383\n",
      "0.8561960458755493\n",
      "0.8522711992263794\n",
      "0.8483709692955017\n",
      "0.8444961309432983\n",
      "0.8406464457511902\n",
      "0.8368218541145325\n",
      "0.8330227732658386\n",
      "0.8292486071586609\n",
      "0.8254996538162231\n",
      "0.8217757940292358\n",
      "0.8180772066116333\n",
      "0.8144036531448364\n",
      "0.8107553124427795\n",
      "0.8071317672729492\n",
      "0.8035334348678589\n",
      "0.7999600172042847\n",
      "0.7964115738868713\n",
      "0.7928879857063293\n",
      "0.7893891930580139\n",
      "0.7859151363372803\n",
      "0.7824655771255493\n",
      "0.7790408134460449\n",
      "0.7756405472755432\n",
      "0.7722644805908203\n",
      "0.768912672996521\n",
      "0.76558518409729\n",
      "0.7622817754745483\n",
      "0.759002149105072\n",
      "0.7557461261749268\n",
      "0.7525140643119812\n",
      "0.7493054270744324\n",
      "0.7461198568344116\n",
      "0.7429577112197876\n",
      "0.7398183345794678\n",
      "0.7367022633552551\n",
      "0.7336084842681885\n",
      "0.7305373549461365\n",
      "0.7274886965751648\n",
      "0.7244619131088257\n",
      "0.721457302570343\n",
      "0.7184743881225586\n",
      "0.7155131101608276\n",
      "0.7125732898712158\n",
      "0.7096545100212097\n",
      "0.7067569494247437\n",
      "0.7038798332214355\n",
      "0.7010236978530884\n",
      "0.6981878876686096\n",
      "0.695372462272644\n",
      "0.69257652759552\n",
      "0.6898009181022644\n",
      "0.6870447993278503\n",
      "0.6843081116676331\n",
      "0.6815903782844543\n",
      "0.6788921356201172\n",
      "0.6762124300003052\n",
      "0.6735513210296631\n",
      "0.6709085702896118\n",
      "0.6682842969894409\n",
      "0.6656780242919922\n",
      "0.6630894541740417\n",
      "0.6605187058448792\n",
      "0.6579652428627014\n",
      "0.6554295420646667\n",
      "0.652910590171814\n",
      "0.6504084467887878\n",
      "0.6479232907295227\n",
      "0.6454545259475708\n",
      "0.6430025100708008\n",
      "0.6405665874481201\n",
      "0.6381465792655945\n",
      "0.6357429027557373\n",
      "0.6333547234535217\n",
      "0.6309823989868164\n",
      "0.6286256313323975\n",
      "0.6262840032577515\n",
      "0.6239573359489441\n",
      "0.6216464042663574\n",
      "0.6193500757217407\n",
      "0.6170688271522522\n",
      "0.6148021221160889\n",
      "0.6125502586364746\n",
      "0.610312819480896\n",
      "0.6080901026725769\n",
      "0.6058814525604248\n",
      "0.6036874055862427\n",
      "0.6015076041221619\n",
      "0.5993419289588928\n",
      "0.5971903800964355\n",
      "0.5950527191162109\n",
      "0.592929482460022\n",
      "0.5908202528953552\n",
      "0.5887248516082764\n",
      "0.5866437554359436\n",
      "0.5845763683319092\n",
      "0.5825233459472656\n",
      "0.5804839730262756\n",
      "0.5784590244293213\n",
      "0.5764478445053101\n",
      "0.5744509100914001\n",
      "0.5724679827690125\n",
      "0.5704993009567261\n",
      "0.5685449838638306\n",
      "0.566604733467102\n",
      "0.5646787285804749\n",
      "0.5627672672271729\n",
      "0.5608699321746826\n",
      "0.5589870810508728\n",
      "0.5571189522743225\n",
      "0.5552651286125183\n",
      "0.5534260272979736\n",
      "0.5516017079353333\n",
      "0.5497917532920837\n",
      "0.5479966998100281\n",
      "0.5462164282798767\n",
      "0.5444508790969849\n",
      "0.5426999926567078\n",
      "0.5409640669822693\n",
      "0.5392428040504456\n",
      "0.5375365018844604\n",
      "0.5358448028564453\n",
      "0.5341680645942688\n",
      "0.5325059294700623\n",
      "0.5308588147163391\n",
      "0.5292260050773621\n",
      "0.527608335018158\n",
      "0.5260050296783447\n",
      "0.5244160890579224\n",
      "0.52284175157547\n",
      "0.521281898021698\n",
      "0.5197361707687378\n",
      "0.5182045698165894\n",
      "0.5166873335838318\n",
      "0.5151840448379517\n",
      "0.513694703578949\n",
      "0.512219250202179\n",
      "0.5107572674751282\n",
      "0.5093088746070862\n",
      "0.507874071598053\n",
      "0.5064525604248047\n",
      "0.5050442218780518\n",
      "0.5036489963531494\n",
      "0.5022667050361633\n",
      "0.5008971095085144\n",
      "0.4995400905609131\n",
      "0.498196005821228\n",
      "0.49686387181282043\n",
      "0.49554407596588135\n",
      "0.4942364990711212\n",
      "0.4929406940937042\n",
      "0.4916567802429199\n",
      "0.49038466811180115\n",
      "0.48912397027015686\n",
      "0.4878748059272766\n",
      "0.4866366982460022\n",
      "0.4854099750518799\n",
      "0.48419398069381714\n",
      "0.48298895359039307\n",
      "0.48179465532302856\n",
      "0.48061099648475647\n",
      "0.47943776845932007\n",
      "0.47827476263046265\n",
      "0.47712209820747375\n",
      "0.4759792983531952\n",
      "0.47484666109085083\n",
      "0.47372373938560486\n",
      "0.4726105034351349\n",
      "0.4715070128440857\n",
      "0.4704127907752991\n",
      "0.4693279266357422\n",
      "0.4682524800300598\n",
      "0.4671861529350281\n",
      "0.4661286771297455\n",
      "0.4650801718235016\n",
      "0.4640403985977173\n",
      "0.4630095362663269\n",
      "0.46198704838752747\n",
      "0.46097323298454285\n",
      "0.4599676728248596\n",
      "0.4589703679084778\n",
      "0.45798149704933167\n",
      "0.4570005536079407\n",
      "0.45602768659591675\n",
      "0.4550626277923584\n",
      "0.45410558581352234\n",
      "0.4531562030315399\n",
      "0.45221441984176636\n",
      "0.45128029584884644\n",
      "0.4503536820411682\n",
      "0.449434369802475\n",
      "0.4485223889350891\n",
      "0.44761788845062256\n",
      "0.44672033190727234\n",
      "0.4458300471305847\n",
      "0.44494664669036865\n",
      "0.4440702795982361\n",
      "0.4432009160518646\n",
      "0.44233810901641846\n",
      "0.4414822459220886\n",
      "0.440633088350296\n",
      "0.43979036808013916\n",
      "0.43895429372787476\n",
      "0.4381246864795685\n",
      "0.43730151653289795\n",
      "0.4364846646785736\n",
      "0.43567410111427307\n",
      "0.43486982583999634\n",
      "0.43407171964645386\n",
      "0.4332795739173889\n",
      "0.4324936866760254\n",
      "0.4317135810852051\n",
      "0.43093955516815186\n",
      "0.43017148971557617\n",
      "0.42940911650657654\n",
      "0.42865246534347534\n",
      "0.42790162563323975\n",
      "0.4271564781665802\n",
      "0.42641693353652954\n",
      "0.42568278312683105\n",
      "0.4249543845653534\n",
      "0.4242312014102936\n",
      "0.4235135316848755\n",
      "0.4228013753890991\n",
      "0.4220942258834839\n",
      "0.4213922917842865\n",
      "0.42069587111473083\n",
      "0.42000436782836914\n",
      "0.4193181097507477\n",
      "0.41863691806793213\n",
      "0.41796064376831055\n",
      "0.4172894060611725\n",
      "0.41662299633026123\n",
      "0.4159615635871887\n",
      "0.415304958820343\n",
      "0.41465309262275696\n",
      "0.4140058755874634\n",
      "0.41336342692375183\n",
      "0.41272562742233276\n",
      "0.4120924174785614\n",
      "0.41146382689476013\n",
      "0.410839706659317\n",
      "0.41022011637687683\n",
      "0.4096048176288605\n",
      "0.4089939594268799\n",
      "0.40838751196861267\n",
      "0.4077852964401245\n",
      "0.40718746185302734\n",
      "0.40659379959106445\n",
      "0.40600430965423584\n",
      "0.40541893243789673\n",
      "0.4048376679420471\n",
      "0.40426045656204224\n",
      "0.4036872386932373\n",
      "0.4031181335449219\n",
      "0.40255284309387207\n",
      "0.4019915759563446\n",
      "0.401434063911438\n",
      "0.4008805453777313\n",
      "0.40033072233200073\n",
      "0.399784654378891\n",
      "0.3992423713207245\n",
      "0.3987037241458893\n",
      "0.3981687128543854\n",
      "0.3976374864578247\n",
      "0.3971096873283386\n",
      "0.39658546447753906\n",
      "0.396064817905426\n",
      "0.3955475687980652\n",
      "0.3950338661670685\n",
      "0.3945234417915344\n",
      "0.39401644468307495\n",
      "0.39351290464401245\n",
      "0.3930126428604126\n",
      "0.392515629529953\n",
      "0.39202192425727844\n",
      "0.39153143763542175\n",
      "0.39104411005973816\n",
      "0.3905600309371948\n",
      "0.39007899165153503\n",
      "0.38960111141204834\n",
      "0.38912639021873474\n",
      "0.38865458965301514\n",
      "0.38818588852882385\n",
      "0.38772010803222656\n",
      "0.38725748658180237\n",
      "0.38679754734039307\n",
      "0.3863406479358673\n",
      "0.3858867585659027\n",
      "0.3854355216026306\n",
      "0.3849872052669525\n",
      "0.3845418393611908\n",
      "0.3840990960597992\n",
      "0.38365912437438965\n",
      "0.38322189450263977\n",
      "0.38278743624687195\n",
      "0.3823556900024414\n",
      "0.3819265067577362\n",
      "0.38149991631507874\n",
      "0.38107597827911377\n",
      "0.3806547224521637\n",
      "0.38023608922958374\n",
      "0.3798198699951172\n",
      "0.37940627336502075\n",
      "0.3789950907230377\n",
      "0.37858647108078003\n",
      "0.37818029522895813\n",
      "0.37777653336524963\n",
      "0.3773752450942993\n",
      "0.37697625160217285\n",
      "0.37657976150512695\n",
      "0.3761855661869049\n",
      "0.3757936954498291\n",
      "0.3754041790962219\n",
      "0.3750168979167938\n",
      "0.37463200092315674\n",
      "0.37424933910369873\n",
      "0.3738688826560974\n",
      "0.3734906315803528\n",
      "0.37311461567878723\n",
      "0.37274083495140076\n",
      "0.3723691999912262\n",
      "0.37199968099594116\n",
      "0.37163233757019043\n",
      "0.371267169713974\n",
      "0.37090402841567993\n",
      "0.3705430030822754\n",
      "0.37018394470214844\n",
      "0.3698269724845886\n",
      "0.36947211623191833\n",
      "0.3691192865371704\n",
      "0.3687683045864105\n",
      "0.36841946840286255\n",
      "0.368072509765625\n",
      "0.36772748827934265\n",
      "0.36738449335098267\n",
      "0.3670434355735779\n",
      "0.36670416593551636\n",
      "0.3663668632507324\n",
      "0.3660315275192261\n",
      "0.3656979203224182\n",
      "0.36536622047424316\n",
      "0.3650363087654114\n",
      "0.36470827460289\n",
      "0.36438196897506714\n",
      "0.3640575706958771\n",
      "0.3637348413467407\n",
      "0.36341392993927\n",
      "0.3630947172641754\n",
      "0.3627772927284241\n",
      "0.3624616265296936\n",
      "0.36214762926101685\n",
      "0.3618353307247162\n",
      "0.36152470111846924\n",
      "0.3612157106399536\n",
      "0.36090850830078125\n",
      "0.36060285568237305\n",
      "0.36029884219169617\n",
      "0.35999640822410583\n",
      "0.3596956729888916\n",
      "0.35939645767211914\n",
      "0.359098881483078\n",
      "0.3588028848171234\n",
      "0.35850846767425537\n",
      "0.3582155406475067\n",
      "0.3579241633415222\n",
      "0.3576343357563019\n",
      "0.35734593868255615\n",
      "0.35705915093421936\n",
      "0.35677385330200195\n",
      "0.35649001598358154\n",
      "0.35620760917663574\n",
      "0.35592666268348694\n",
      "0.3556472659111023\n",
      "0.3553691804409027\n",
      "0.3550925850868225\n",
      "0.35481739044189453\n",
      "0.35454365611076355\n",
      "0.35427126288414\n",
      "0.3540002703666687\n",
      "0.3537307381629944\n",
      "0.3534623682498932\n",
      "0.35319554805755615\n",
      "0.3529300093650818\n",
      "0.35266584157943726\n",
      "0.3524029850959778\n",
      "0.3521413803100586\n",
      "0.3518811762332916\n",
      "0.35162222385406494\n",
      "0.3513645529747009\n",
      "0.3511081337928772\n",
      "0.3508530259132385\n",
      "0.3505992293357849\n",
      "0.3503466248512268\n",
      "0.35009533166885376\n",
      "0.34984517097473145\n",
      "0.3495963513851166\n",
      "0.34934866428375244\n",
      "0.349102258682251\n",
      "0.348857045173645\n",
      "0.34861302375793457\n",
      "0.34837010502815247\n",
      "0.34812846779823303\n",
      "0.34788790345191956\n",
      "0.347648561000824\n",
      "0.3474103510379791\n",
      "0.3471733033657074\n",
      "0.3469374179840088\n",
      "0.3467026948928833\n",
      "0.3464690148830414\n",
      "0.3462364077568054\n",
      "0.34600499272346497\n",
      "0.34577468037605286\n",
      "0.3455454707145691\n",
      "0.3453173041343689\n",
      "0.34509027004241943\n",
      "0.3448643088340759\n",
      "0.34463930130004883\n",
      "0.34441545605659485\n",
      "0.3441925644874573\n",
      "0.3439708650112152\n",
      "0.3437500596046448\n",
      "0.3435302972793579\n",
      "0.34331169724464417\n",
      "0.34309396147727966\n",
      "0.3428773581981659\n",
      "0.34266164898872375\n",
      "0.3424470126628876\n",
      "0.3422333002090454\n",
      "0.34202060103416443\n",
      "0.3418090045452118\n",
      "0.341598242521286\n",
      "0.34138840436935425\n",
      "0.34117957949638367\n",
      "0.3409717082977295\n",
      "0.3407648205757141\n",
      "0.34055882692337036\n",
      "0.3403538465499878\n",
      "0.3401497006416321\n",
      "0.3399464786052704\n",
      "0.3397442698478699\n",
      "0.3395428955554962\n",
      "0.33934247493743896\n",
      "0.33914291858673096\n",
      "0.3389442563056946\n",
      "0.33874648809432983\n",
      "0.33854955434799194\n",
      "0.33835363388061523\n",
      "0.3381584882736206\n",
      "0.33796426653862\n",
      "0.33777087926864624\n",
      "0.33757835626602173\n",
      "0.3373865783214569\n",
      "0.33719581365585327\n",
      "0.3370057940483093\n",
      "0.33681657910346985\n",
      "0.3366282880306244\n",
      "0.3364408314228058\n",
      "0.33625417947769165\n",
      "0.3360682427883148\n",
      "0.335883229970932\n",
      "0.3356989026069641\n",
      "0.33551546931266785\n",
      "0.33533284068107605\n",
      "0.3351510167121887\n",
      "0.3349698781967163\n",
      "0.3347896635532379\n",
      "0.3346101641654968\n",
      "0.3344314396381378\n",
      "0.3342534899711609\n",
      "0.33407631516456604\n",
      "0.3338997960090637\n",
      "0.33372414112091064\n",
      "0.33354923129081726\n",
      "0.3333750367164612\n",
      "0.3332015872001648\n",
      "0.3330288827419281\n",
      "0.3328569233417511\n",
      "0.3326856195926666\n",
      "0.3325152099132538\n",
      "0.3323453664779663\n",
      "0.33217617869377136\n",
      "0.33200785517692566\n",
      "0.3318401873111725\n",
      "0.33167320489883423\n",
      "0.3315069377422333\n",
      "0.33134138584136963\n",
      "0.3311765491962433\n",
      "0.3310122489929199\n",
      "0.33084872364997864\n",
      "0.33068591356277466\n",
      "0.3305237293243408\n",
      "0.3303622305393219\n",
      "0.33020147681236267\n",
      "0.3300413191318512\n",
      "0.32988178730010986\n",
      "0.32972297072410583\n",
      "0.3295647203922272\n",
      "0.3294071853160858\n",
      "0.329250305891037\n",
      "0.3290939927101135\n",
      "0.3289383351802826\n",
      "0.3287833631038666\n",
      "0.3286289572715759\n",
      "0.3284752368927002\n",
      "0.32832208275794983\n",
      "0.328169584274292\n",
      "0.3280176818370819\n",
      "0.3278663754463196\n",
      "0.3277157247066498\n",
      "0.32756564021110535\n",
      "0.3274160623550415\n",
      "0.32726728916168213\n",
      "0.3271189033985138\n",
      "0.32697126269340515\n",
      "0.3268241584300995\n",
      "0.326677531003952\n",
      "0.32653164863586426\n",
      "0.3263862133026123\n",
      "0.3262413740158081\n",
      "0.32609713077545166\n",
      "0.3259533941745758\n",
      "0.3258103132247925\n",
      "0.32566767930984497\n",
      "0.3255257308483124\n",
      "0.3253842890262604\n",
      "0.32524341344833374\n",
      "0.3251029849052429\n",
      "0.32496312260627747\n",
      "0.32482388615608215\n",
      "0.32468515634536743\n",
      "0.3245468735694885\n",
      "0.324409157037735\n",
      "0.3242720067501068\n",
      "0.32413536310195923\n",
      "0.323999285697937\n",
      "0.3238636255264282\n",
      "0.3237285614013672\n",
      "0.32359397411346436\n",
      "0.3234598636627197\n",
      "0.3233262896537781\n",
      "0.323193222284317\n",
      "0.32306066155433655\n",
      "0.32292860746383667\n",
      "0.3227969706058502\n",
      "0.32266589999198914\n",
      "0.32253527641296387\n",
      "0.3224051594734192\n",
      "0.3222755789756775\n",
      "0.32214635610580444\n",
      "0.3220175802707672\n",
      "0.3218894600868225\n",
      "0.3217616677284241\n",
      "0.3216343820095062\n",
      "0.32150760293006897\n",
      "0.32138127088546753\n",
      "0.3212553858757019\n",
      "0.3211298882961273\n",
      "0.3210049569606781\n",
      "0.32088038325309753\n",
      "0.32075634598731995\n",
      "0.320632666349411\n",
      "0.32050955295562744\n",
      "0.3203868269920349\n",
      "0.3202644884586334\n",
      "0.32014256715774536\n",
      "0.3200211524963379\n",
      "0.31990015506744385\n",
      "0.31977954506874084\n",
      "0.31965938210487366\n",
      "0.3195396959781647\n",
      "0.31942036747932434\n",
      "0.31930142641067505\n",
      "0.31918296217918396\n",
      "0.3190648853778839\n",
      "0.31894728541374207\n",
      "0.31883007287979126\n",
      "0.31871315836906433\n",
      "0.3185967803001404\n",
      "0.3184807002544403\n",
      "0.31836506724357605\n",
      "0.3182498812675476\n",
      "0.3181350529193878\n",
      "0.31802064180374146\n",
      "0.31790652871131897\n",
      "0.3177928328514099\n",
      "0.31767961382865906\n",
      "0.31756672263145447\n",
      "0.31745418906211853\n",
      "0.31734201312065125\n",
      "0.31723034381866455\n",
      "0.31711891293525696\n",
      "0.31700795888900757\n",
      "0.31689730286598206\n",
      "0.3167870342731476\n",
      "0.31667712330818176\n",
      "0.31656762957572937\n",
      "0.31645846366882324\n",
      "0.31634965538978577\n",
      "0.3162412941455841\n",
      "0.31613317131996155\n",
      "0.31602537631988525\n",
      "0.31591805815696716\n",
      "0.31581100821495056\n",
      "0.3157043755054474\n",
      "0.3155979514122009\n",
      "0.31549200415611267\n",
      "0.31538641452789307\n",
      "0.31528109312057495\n",
      "0.3151761293411255\n",
      "0.31507158279418945\n",
      "0.31496724486351013\n",
      "0.31486332416534424\n",
      "0.3147597312927246\n",
      "0.3146563768386841\n",
      "0.31455346941947937\n",
      "0.31445083022117615\n",
      "0.3143485486507416\n",
      "0.31424665451049805\n",
      "0.31414493918418884\n",
      "0.3140436112880707\n",
      "0.3139426112174988\n",
      "0.31384190917015076\n",
      "0.313741534948349\n",
      "0.3136414587497711\n",
      "0.3135417401790619\n",
      "0.31344228982925415\n",
      "0.3133431673049927\n",
      "0.31324443221092224\n",
      "0.31314584612846375\n",
      "0.3130476772785187\n",
      "0.3129497170448303\n",
      "0.3128521144390106\n",
      "0.31275486946105957\n",
      "0.31265783309936523\n",
      "0.31256112456321716\n",
      "0.3124646544456482\n",
      "0.31236860156059265\n",
      "0.3122727870941162\n",
      "0.31217724084854126\n",
      "0.3120819628238678\n",
      "0.3119870126247406\n",
      "0.3118923604488373\n",
      "0.3117979168891907\n",
      "0.3117038309574127\n",
      "0.31160998344421387\n",
      "0.3115164637565613\n",
      "0.3114232122898102\n",
      "0.31133025884628296\n",
      "0.3112375736236572\n",
      "0.311145156621933\n",
      "0.31105291843414307\n",
      "0.3109610676765442\n",
      "0.3108694851398468\n",
      "0.3107781410217285\n",
      "0.3106870651245117\n",
      "0.3105962574481964\n",
      "0.3105056881904602\n",
      "0.31041544675827026\n",
      "0.31032538414001465\n",
      "0.31023573875427246\n",
      "0.31014618277549744\n",
      "0.31005701422691345\n",
      "0.3099680244922638\n",
      "0.30987927317619324\n",
      "0.30979084968566895\n",
      "0.30970263481140137\n",
      "0.30961471796035767\n",
      "0.3095270097255707\n",
      "0.30943959951400757\n",
      "0.30935239791870117\n",
      "0.30926552414894104\n",
      "0.30917876958847046\n",
      "0.3090924024581909\n",
      "0.30900612473487854\n",
      "0.3089202642440796\n",
      "0.308834433555603\n",
      "0.3087490200996399\n",
      "0.30866384506225586\n",
      "0.30857881903648376\n",
      "0.30849409103393555\n",
      "0.30840957164764404\n",
      "0.30832526087760925\n",
      "0.3082413375377655\n",
      "0.3081575036048889\n",
      "0.3080739378929138\n",
      "0.30799058079719543\n",
      "0.30790746212005615\n",
      "0.30782461166381836\n",
      "0.30774208903312683\n",
      "0.3076596260070801\n",
      "0.3075774312019348\n",
      "0.30749550461769104\n",
      "0.307413786649704\n",
      "0.3073323369026184\n",
      "0.3072510361671448\n",
      "0.307170033454895\n",
      "0.3070891499519348\n",
      "0.3070085644721985\n",
      "0.30692821741104126\n",
      "0.30684804916381836\n",
      "0.30676811933517456\n",
      "0.3066883683204651\n",
      "0.3066088855266571\n",
      "0.30652958154678345\n",
      "0.30645060539245605\n",
      "0.30637168884277344\n",
      "0.3062930405139923\n",
      "0.3062146306037903\n",
      "0.3061363995075226\n",
      "0.30605846643447876\n",
      "0.3059806227684021\n",
      "0.3059030771255493\n",
      "0.30582568049430847\n",
      "0.30574849247932434\n",
      "0.3056715428829193\n",
      "0.3055948317050934\n",
      "0.3055182695388794\n",
      "0.3054419457912445\n",
      "0.30536577105522156\n",
      "0.3052898645401001\n",
      "0.30521416664123535\n",
      "0.30513861775398254\n",
      "0.30506324768066406\n",
      "0.30498820543289185\n",
      "0.3049132823944092\n",
      "0.30483853816986084\n",
      "0.3047640025615692\n",
      "0.3046897053718567\n",
      "0.3046155869960785\n",
      "0.3045416474342346\n",
      "0.30446797609329224\n",
      "0.30439436435699463\n",
      "0.3043210506439209\n",
      "0.30424782633781433\n",
      "0.30417492985725403\n",
      "0.3041021227836609\n",
      "0.30402958393096924\n",
      "0.3039572238922119\n",
      "0.30388501286506653\n",
      "0.30381304025650024\n",
      "0.3037412166595459\n",
      "0.30366963148117065\n",
      "0.3035981357097626\n",
      "0.30352696776390076\n",
      "0.3034558594226837\n",
      "0.30338501930236816\n",
      "0.30331432819366455\n",
      "0.3032437860965729\n",
      "0.3031734526157379\n",
      "0.30310335755348206\n",
      "0.30303338170051575\n",
      "0.30296361446380615\n",
      "0.30289405584335327\n",
      "0.3028246760368347\n",
      "0.3027554154396057\n",
      "0.3026864230632782\n",
      "0.30261754989624023\n",
      "0.30254891514778137\n",
      "0.30248039960861206\n",
      "0.3024120628833771\n",
      "0.30234384536743164\n",
      "0.3022758960723877\n",
      "0.3022081255912781\n",
      "0.302140474319458\n",
      "0.30207303166389465\n",
      "0.3020058572292328\n",
      "0.3019387125968933\n",
      "0.30187177658081055\n",
      "0.3018050491809845\n",
      "0.3017384707927704\n",
      "0.3016720414161682\n",
      "0.30160582065582275\n",
      "0.3015397787094116\n",
      "0.3014739155769348\n",
      "0.30140820145606995\n",
      "0.3013426661491394\n",
      "0.301277220249176\n",
      "0.30121204257011414\n",
      "0.3011470437049866\n",
      "0.30108219385147095\n",
      "0.3010174632072449\n",
      "0.3009529411792755\n",
      "0.3008885979652405\n",
      "0.3008244037628174\n",
      "0.3007603585720062\n",
      "0.3006964921951294\n",
      "0.3006327152252197\n",
      "0.30056923627853394\n",
      "0.3005058467388153\n",
      "0.30044257640838623\n",
      "0.30037954449653625\n",
      "0.3003166615962982\n",
      "0.3002539277076721\n",
      "0.30019137263298035\n",
      "0.3001289665699005\n",
      "0.3000667095184326\n",
      "0.30000463128089905\n",
      "0.2999427318572998\n",
      "0.2998809814453125\n",
      "0.29981935024261475\n",
      "0.2997579276561737\n",
      "0.29969659447669983\n",
      "0.29963546991348267\n",
      "0.29957449436187744\n",
      "0.29951369762420654\n",
      "0.2994530498981476\n",
      "0.2993924915790558\n",
      "0.2993321418762207\n",
      "0.29927200078964233\n",
      "0.29921194911003113\n",
      "0.2991520166397095\n",
      "0.29909226298332214\n",
      "0.29903268814086914\n",
      "0.29897332191467285\n",
      "0.2989140450954437\n",
      "0.2988549470901489\n",
      "0.2987959384918213\n",
      "0.29873713850975037\n",
      "0.298678457736969\n",
      "0.29861992597579956\n",
      "0.29856157302856445\n",
      "0.2985033690929413\n",
      "0.29844531416893005\n",
      "0.298387348651886\n",
      "0.2983296513557434\n",
      "0.29827195405960083\n",
      "0.29821446537971497\n",
      "0.2981571555137634\n",
      "0.29809996485710144\n",
      "0.2980429530143738\n",
      "0.2979860305786133\n",
      "0.2979293465614319\n",
      "0.2978726923465729\n",
      "0.2978162467479706\n",
      "0.2977599799633026\n",
      "0.2977037727832794\n",
      "0.29764774441719055\n",
      "0.297591894865036\n",
      "0.29753613471984863\n",
      "0.29748058319091797\n",
      "0.29742512106895447\n",
      "0.2973697781562805\n",
      "0.2973145842552185\n",
      "0.2972595989704132\n",
      "0.2972047030925751\n",
      "0.2971499562263489\n",
      "0.29709532856941223\n",
      "0.2970408797264099\n",
      "0.29698652029037476\n",
      "0.2969323992729187\n",
      "0.2968783378601074\n",
      "0.2968243956565857\n",
      "0.2967706322669983\n",
      "0.29671695828437805\n",
      "0.29666346311569214\n",
      "0.29661011695861816\n",
      "0.29655689001083374\n",
      "0.29650378227233887\n",
      "0.29645079374313354\n",
      "0.29639798402786255\n",
      "0.2963452637195587\n",
      "0.2962926924228668\n",
      "0.29624029994010925\n",
      "0.29618799686431885\n",
      "0.2961358428001404\n",
      "0.29608380794525146\n",
      "0.2960318922996521\n",
      "0.29598015546798706\n",
      "0.2959285378456116\n",
      "0.29587697982788086\n",
      "0.29582566022872925\n",
      "0.2957744002342224\n",
      "0.2957232594490051\n",
      "0.29567229747772217\n",
      "0.295621395111084\n",
      "0.2955707013607025\n",
      "0.295520156621933\n",
      "0.29546964168548584\n",
      "0.29541927576065063\n",
      "0.295369029045105\n",
      "0.29531896114349365\n",
      "0.2952689826488495\n",
      "0.29521915316581726\n",
      "0.295169472694397\n",
      "0.29511985182762146\n",
      "0.2950704097747803\n",
      "0.29502105712890625\n",
      "0.2949717938899994\n",
      "0.29492276906967163\n",
      "0.2948737144470215\n",
      "0.2948249280452728\n",
      "0.29477623105049133\n",
      "0.2947275936603546\n",
      "0.2946791350841522\n",
      "0.294630765914917\n",
      "0.2945824861526489\n",
      "0.2945344150066376\n",
      "0.2944864332675934\n",
      "0.2944384813308716\n",
      "0.2943907380104065\n",
      "0.29434314370155334\n",
      "0.2942955493927002\n",
      "0.29424816370010376\n",
      "0.2942008972167969\n",
      "0.29415369033813477\n",
      "0.2941066324710846\n",
      "0.29405978322029114\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## our neural n/w has low loss because it overfits because we only gave 32 i/p and 32 o/ps and trained dour model for that we 3k parameters for 32 examples the network will overfit single bath and getting low loss\n",
    "\n",
    "'''\n",
    "why loss is not becoming is zero for over fitting case with so many parameters ?\n",
    "\n",
    "ans) in our training data for  i/p ... we have e, o, a, u, s as o/p, because all the starting character has ... in front of their names\n",
    "due to sAME I/P HAS MULTIPLE O/P OVER MODEL HAS STILL LOSS IN CASE OF OVER FITTING T0O\n",
    "\n",
    "    '''"
   ],
   "id": "270e32ad181c5461"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creating Dataset for all words",
   "id": "b7a013732385e992"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T18:19:33.131879Z",
     "start_time": "2025-07-16T18:19:32.776913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## data set for neural network\n",
    "\n",
    "block_size = 3 #Context_length: how many characters do we take to predict the next one?\n",
    "X,Y =[],[] # X are the input to neural network and Y are  the labels of the neural network\n",
    "\n",
    "for w in words:\n",
    "    context = [0] * block_size\n",
    "\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix]\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "X.shape, Y.shape"
   ],
   "id": "edc73b3a07465fa3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T18:19:35.134856Z",
     "start_time": "2025-07-16T18:19:35.127832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # this is used for generating same weights from the tutorial\n",
    "C = torch.randn((27,2), generator=g) #Embedding layer( we choosed 2 dimensional embedding for 27 charcters\n",
    "W1 = torch.randn((6,100), generator=g)\n",
    "B1 = torch.randn(100, generator = g)\n",
    "W2 = torch.randn((100,27), generator = g)\n",
    "B2 = torch.randn(27, generator = g)\n",
    "parameters = [C, W1, B1, W2, B2]"
   ],
   "id": "caf31249410bcab",
   "outputs": [],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T18:19:35.611636Z",
     "start_time": "2025-07-16T18:19:35.601570Z"
    }
   },
   "cell_type": "code",
   "source": "sum(p.nelement() for p in parameters) # number of parameters in total",
   "id": "b11196a77d54c7e5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T18:19:36.473365Z",
     "start_time": "2025-07-16T18:19:36.468701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ],
   "id": "530c82fb78fd7f2",
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T18:21:25.063966Z",
     "start_time": "2025-07-16T18:19:37.760067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# so every time we run this the weights keeps on updating\n",
    "for _ in range(1000):\n",
    "    # Forward pass\n",
    "    embedding = C[X] #(32,3,2)\n",
    "    Hidden_Layer = torch.tanh(embedding.view(-1,6)@ W1 + B1) #(32, 100)\n",
    "    logits = Hidden_Layer @ W2 + B2 # (32,27)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update the weights\n",
    "    for p in parameters:\n",
    "        p.data = p.data + (-0.1 * p.grad)"
   ],
   "id": "2d7c3a08addca82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.505229949951172\n",
      "17.084484100341797\n",
      "15.776530265808105\n",
      "14.833340644836426\n",
      "14.002604484558105\n",
      "13.253260612487793\n",
      "12.57991886138916\n",
      "11.983101844787598\n",
      "11.47049331665039\n",
      "11.051855087280273\n",
      "10.709586143493652\n",
      "10.407631874084473\n",
      "10.127808570861816\n",
      "9.864364624023438\n",
      "9.614501953125\n",
      "9.376439094543457\n",
      "9.148943901062012\n",
      "8.931109428405762\n",
      "8.722230911254883\n",
      "8.521748542785645\n",
      "8.32922649383545\n",
      "8.144325256347656\n",
      "7.966790676116943\n",
      "7.796449184417725\n",
      "7.633184909820557\n",
      "7.476906776428223\n",
      "7.327520370483398\n",
      "7.184884548187256\n",
      "7.04879093170166\n",
      "6.918951988220215\n",
      "6.795017242431641\n",
      "6.676602840423584\n",
      "6.563317775726318\n",
      "6.454788684844971\n",
      "6.350668430328369\n",
      "6.250642776489258\n",
      "6.154431343078613\n",
      "6.0617852210998535\n",
      "5.972482204437256\n",
      "5.886327743530273\n",
      "5.803146839141846\n",
      "5.722784042358398\n",
      "5.64509391784668\n",
      "5.569945335388184\n",
      "5.497213363647461\n",
      "5.4267802238464355\n",
      "5.358535289764404\n",
      "5.2923760414123535\n",
      "5.228203296661377\n",
      "5.165927886962891\n",
      "5.10546875\n",
      "5.046748161315918\n",
      "4.98969841003418\n",
      "4.934261798858643\n",
      "4.880379676818848\n",
      "4.828005790710449\n",
      "4.777095794677734\n",
      "4.727609634399414\n",
      "4.679514408111572\n",
      "4.632779121398926\n",
      "4.587378025054932\n",
      "4.5432891845703125\n",
      "4.500491619110107\n",
      "4.458967208862305\n",
      "4.418700695037842\n",
      "4.37967586517334\n",
      "4.341878890991211\n",
      "4.30529260635376\n",
      "4.269899845123291\n",
      "4.23568058013916\n",
      "4.202612400054932\n",
      "4.170670032501221\n",
      "4.139825344085693\n",
      "4.110044479370117\n",
      "4.081293106079102\n",
      "4.053532123565674\n",
      "4.026721477508545\n",
      "4.000818252563477\n",
      "3.975780487060547\n",
      "3.9515645503997803\n",
      "3.928129196166992\n",
      "3.9054315090179443\n",
      "3.8834340572357178\n",
      "3.8620991706848145\n",
      "3.8413918018341064\n",
      "3.8212802410125732\n",
      "3.801734685897827\n",
      "3.7827272415161133\n",
      "3.7642335891723633\n",
      "3.746229887008667\n",
      "3.7286956310272217\n",
      "3.71161150932312\n",
      "3.6949596405029297\n",
      "3.6787235736846924\n",
      "3.6628875732421875\n",
      "3.6474380493164062\n",
      "3.6323611736297607\n",
      "3.6176459789276123\n",
      "3.603278875350952\n",
      "3.589249849319458\n",
      "3.575547933578491\n",
      "3.5621628761291504\n",
      "3.5490849018096924\n",
      "3.5363051891326904\n",
      "3.5238142013549805\n",
      "3.511603832244873\n",
      "3.4996652603149414\n",
      "3.487989902496338\n",
      "3.4765706062316895\n",
      "3.4653992652893066\n",
      "3.4544689655303955\n",
      "3.4437713623046875\n",
      "3.4333012104034424\n",
      "3.4230501651763916\n",
      "3.413012981414795\n",
      "3.403182029724121\n",
      "3.393552303314209\n",
      "3.384117603302002\n",
      "3.3748719692230225\n",
      "3.365809917449951\n",
      "3.3569271564483643\n",
      "3.348217248916626\n",
      "3.3396761417388916\n",
      "3.331299304962158\n",
      "3.3230812549591064\n",
      "3.315018653869629\n",
      "3.3071062564849854\n",
      "3.2993409633636475\n",
      "3.2917182445526123\n",
      "3.2842342853546143\n",
      "3.276885509490967\n",
      "3.2696683406829834\n",
      "3.2625787258148193\n",
      "3.2556145191192627\n",
      "3.2487707138061523\n",
      "3.2420458793640137\n",
      "3.2354350090026855\n",
      "3.2289373874664307\n",
      "3.222547769546509\n",
      "3.2162647247314453\n",
      "3.2100844383239746\n",
      "3.204005241394043\n",
      "3.198024034500122\n",
      "3.192138195037842\n",
      "3.1863443851470947\n",
      "3.1806414127349854\n",
      "3.1750266551971436\n",
      "3.1694977283477783\n",
      "3.1640517711639404\n",
      "3.1586875915527344\n",
      "3.1534018516540527\n",
      "3.148193597793579\n",
      "3.1430606842041016\n",
      "3.13800048828125\n",
      "3.133012056350708\n",
      "3.1280932426452637\n",
      "3.123242139816284\n",
      "3.118457078933716\n",
      "3.113736629486084\n",
      "3.1090786457061768\n",
      "3.1044833660125732\n",
      "3.099947690963745\n",
      "3.095470905303955\n",
      "3.0910513401031494\n",
      "3.0866875648498535\n",
      "3.0823795795440674\n",
      "3.078125476837158\n",
      "3.073923349380493\n",
      "3.0697736740112305\n",
      "3.065674304962158\n",
      "3.061624526977539\n",
      "3.057624101638794\n",
      "3.05367112159729\n",
      "3.0497655868530273\n",
      "3.0459067821502686\n",
      "3.04209303855896\n",
      "3.038325071334839\n",
      "3.0346009731292725\n",
      "3.03092098236084\n",
      "3.0272843837738037\n",
      "3.0236899852752686\n",
      "3.020138740539551\n",
      "3.016627788543701\n",
      "3.0131585597991943\n",
      "3.009730577468872\n",
      "3.006342649459839\n",
      "3.0029945373535156\n",
      "2.9996860027313232\n",
      "2.9964170455932617\n",
      "2.993187189102173\n",
      "2.9899957180023193\n",
      "2.9868428707122803\n",
      "2.983727216720581\n",
      "2.980649948120117\n",
      "2.977609157562256\n",
      "2.974606513977051\n",
      "2.971640110015869\n",
      "2.96871018409729\n",
      "2.9658164978027344\n",
      "2.962959051132202\n",
      "2.960136651992798\n",
      "2.9573493003845215\n",
      "2.954597234725952\n",
      "2.9518797397613525\n",
      "2.9491968154907227\n",
      "2.946547031402588\n",
      "2.9439315795898438\n",
      "2.9413487911224365\n",
      "2.938798666000366\n",
      "2.936281442642212\n",
      "2.933795928955078\n",
      "2.931342124938965\n",
      "2.9289188385009766\n",
      "2.9265270233154297\n",
      "2.9241650104522705\n",
      "2.92183256149292\n",
      "2.919529676437378\n",
      "2.917255401611328\n",
      "2.915009021759033\n",
      "2.912790536880493\n",
      "2.9105992317199707\n",
      "2.908435106277466\n",
      "2.9062962532043457\n",
      "2.9041836261749268\n",
      "2.9020960330963135\n",
      "2.900033473968506\n",
      "2.8979945182800293\n",
      "2.895979404449463\n",
      "2.8939876556396484\n",
      "2.8920180797576904\n",
      "2.890070915222168\n",
      "2.8881452083587646\n",
      "2.8862409591674805\n",
      "2.884357452392578\n",
      "2.8824944496154785\n",
      "2.880650758743286\n",
      "2.8788270950317383\n",
      "2.8770222663879395\n",
      "2.8752357959747314\n",
      "2.8734681606292725\n",
      "2.871718406677246\n",
      "2.8699865341186523\n",
      "2.868271589279175\n",
      "2.8665738105773926\n",
      "2.8648927211761475\n",
      "2.863227605819702\n",
      "2.8615787029266357\n",
      "2.8599460124969482\n",
      "2.858328342437744\n",
      "2.8567264080047607\n",
      "2.8551390171051025\n",
      "2.853566884994507\n",
      "2.8520090579986572\n",
      "2.850465774536133\n",
      "2.8489363193511963\n",
      "2.847421169281006\n",
      "2.845919609069824\n",
      "2.844430923461914\n",
      "2.842956304550171\n",
      "2.8414947986602783\n",
      "2.8400468826293945\n",
      "2.838611125946045\n",
      "2.8371877670288086\n",
      "2.835777759552002\n",
      "2.8343799114227295\n",
      "2.832993984222412\n",
      "2.831620454788208\n",
      "2.830258846282959\n",
      "2.828909158706665\n",
      "2.827571153640747\n",
      "2.826244592666626\n",
      "2.8249294757843018\n",
      "2.8236258029937744\n",
      "2.822333335876465\n",
      "2.821052074432373\n",
      "2.81978178024292\n",
      "2.8185222148895264\n",
      "2.8172738552093506\n",
      "2.816035509109497\n",
      "2.814807891845703\n",
      "2.813591241836548\n",
      "2.812384605407715\n",
      "2.811187505722046\n",
      "2.8100013732910156\n",
      "2.8088245391845703\n",
      "2.8076579570770264\n",
      "2.8065009117126465\n",
      "2.805354118347168\n",
      "2.8042163848876953\n",
      "2.8030881881713867\n",
      "2.8019697666168213\n",
      "2.8008599281311035\n",
      "2.799760103225708\n",
      "2.798668622970581\n",
      "2.7975869178771973\n",
      "2.796513319015503\n",
      "2.7954490184783936\n",
      "2.7943930625915527\n",
      "2.7933452129364014\n",
      "2.792307138442993\n",
      "2.791276454925537\n",
      "2.7902543544769287\n",
      "2.789240837097168\n",
      "2.7882349491119385\n",
      "2.7872371673583984\n",
      "2.7862472534179688\n",
      "2.7852654457092285\n",
      "2.7842907905578613\n",
      "2.7833242416381836\n",
      "2.782365560531616\n",
      "2.7814137935638428\n",
      "2.7804698944091797\n",
      "2.7795329093933105\n",
      "2.7786028385162354\n",
      "2.7776801586151123\n",
      "2.7767646312713623\n",
      "2.7758562564849854\n",
      "2.774954319000244\n",
      "2.7740590572357178\n",
      "2.7731709480285645\n",
      "2.772289514541626\n",
      "2.771414279937744\n",
      "2.770545482635498\n",
      "2.7696831226348877\n",
      "2.768826961517334\n",
      "2.767977237701416\n",
      "2.7671332359313965\n",
      "2.7662956714630127\n",
      "2.7654643058776855\n",
      "2.7646384239196777\n",
      "2.7638187408447266\n",
      "2.7630045413970947\n",
      "2.7621958255767822\n",
      "2.7613935470581055\n",
      "2.76059627532959\n",
      "2.7598042488098145\n",
      "2.7590181827545166\n",
      "2.758237600326538\n",
      "2.7574617862701416\n",
      "2.7566916942596436\n",
      "2.7559263706207275\n",
      "2.7551660537719727\n",
      "2.754411458969116\n",
      "2.7536609172821045\n",
      "2.752915859222412\n",
      "2.752176284790039\n",
      "2.7514405250549316\n",
      "2.7507097721099854\n",
      "2.749983549118042\n",
      "2.7492623329162598\n",
      "2.7485458850860596\n",
      "2.747833728790283\n",
      "2.7471256256103516\n",
      "2.746422290802002\n",
      "2.745723247528076\n",
      "2.7450287342071533\n",
      "2.744338274002075\n",
      "2.743652105331421\n",
      "2.7429699897766113\n",
      "2.7422919273376465\n",
      "2.7416179180145264\n",
      "2.74094820022583\n",
      "2.7402822971343994\n",
      "2.7396202087402344\n",
      "2.738961935043335\n",
      "2.738307476043701\n",
      "2.737657070159912\n",
      "2.7370100021362305\n",
      "2.7363667488098145\n",
      "2.735727310180664\n",
      "2.735090970993042\n",
      "2.7344586849212646\n",
      "2.733829975128174\n",
      "2.7332043647766113\n",
      "2.7325820922851562\n",
      "2.7319629192352295\n",
      "2.7313480377197266\n",
      "2.7307357788085938\n",
      "2.7301268577575684\n",
      "2.729520797729492\n",
      "2.7289185523986816\n",
      "2.7283191680908203\n",
      "2.727722644805908\n",
      "2.7271292209625244\n",
      "2.726538896560669\n",
      "2.7259514331817627\n",
      "2.725367307662964\n",
      "2.724785566329956\n",
      "2.7242069244384766\n",
      "2.723630905151367\n",
      "2.7230584621429443\n",
      "2.722487688064575\n",
      "2.7219197750091553\n",
      "2.7213547229766846\n",
      "2.720792293548584\n",
      "2.7202320098876953\n",
      "2.719674825668335\n",
      "2.7191200256347656\n",
      "2.7185676097869873\n",
      "2.718018054962158\n",
      "2.717470169067383\n",
      "2.7169249057769775\n",
      "2.7163822650909424\n",
      "2.715841770172119\n",
      "2.715303421020508\n",
      "2.7147674560546875\n",
      "2.7142333984375\n",
      "2.7137019634246826\n",
      "2.713172435760498\n",
      "2.7126450538635254\n",
      "2.7121198177337646\n",
      "2.7115964889526367\n",
      "2.7110750675201416\n",
      "2.7105560302734375\n",
      "2.710038423538208\n",
      "2.7095232009887695\n",
      "2.7090091705322266\n",
      "2.7084977626800537\n",
      "2.7079882621765137\n",
      "2.7074801921844482\n",
      "2.7069737911224365\n",
      "2.7064692974090576\n",
      "2.7059662342071533\n",
      "2.70546555519104\n",
      "2.704965829849243\n",
      "2.7044677734375\n",
      "2.7039716243743896\n",
      "2.703477144241333\n",
      "2.702984094619751\n",
      "2.7024922370910645\n",
      "2.7020022869110107\n",
      "2.7015137672424316\n",
      "2.701026678085327\n",
      "2.7005410194396973\n",
      "2.700056314468384\n",
      "2.699573516845703\n",
      "2.699092149734497\n",
      "2.6986117362976074\n",
      "2.6981327533721924\n",
      "2.697655439376831\n",
      "2.697179079055786\n",
      "2.696704387664795\n",
      "2.696230173110962\n",
      "2.6957576274871826\n",
      "2.6952860355377197\n",
      "2.6948158740997314\n",
      "2.6943466663360596\n",
      "2.693878650665283\n",
      "2.6934118270874023\n",
      "2.692945957183838\n",
      "2.6924808025360107\n",
      "2.6920173168182373\n",
      "2.691554069519043\n",
      "2.6910927295684814\n",
      "2.690631866455078\n",
      "2.6901724338531494\n",
      "2.689713478088379\n",
      "2.689255714416504\n",
      "2.688798666000366\n",
      "2.688343048095703\n",
      "2.6878879070281982\n",
      "2.687433958053589\n",
      "2.6869804859161377\n",
      "2.686527967453003\n",
      "2.6860766410827637\n",
      "2.6856255531311035\n",
      "2.685175895690918\n",
      "2.6847267150878906\n",
      "2.6842784881591797\n",
      "2.683830976486206\n",
      "2.6833841800689697\n",
      "2.682938575744629\n",
      "2.6824934482574463\n",
      "2.682048797607422\n",
      "2.681605100631714\n",
      "2.681162118911743\n",
      "2.680720329284668\n",
      "2.680278778076172\n",
      "2.679837942123413\n",
      "2.6793973445892334\n",
      "2.678957939147949\n",
      "2.6785197257995605\n",
      "2.678081512451172\n",
      "2.6776440143585205\n",
      "2.6772074699401855\n",
      "2.6767711639404297\n",
      "2.6763358116149902\n",
      "2.67590069770813\n",
      "2.675466775894165\n",
      "2.6750330924987793\n",
      "2.67460036277771\n",
      "2.674168348312378\n",
      "2.673736333847046\n",
      "2.6733055114746094\n",
      "2.672874927520752\n",
      "2.672445297241211\n",
      "2.672015905380249\n",
      "2.6715874671936035\n",
      "2.6711597442626953\n",
      "2.670732259750366\n",
      "2.6703057289123535\n",
      "2.66987943649292\n",
      "2.6694540977478027\n",
      "2.6690289974212646\n",
      "2.668604850769043\n",
      "2.6681809425354004\n",
      "2.667757987976074\n",
      "2.6673355102539062\n",
      "2.6669139862060547\n",
      "2.6664929389953613\n",
      "2.666072368621826\n",
      "2.66565203666687\n",
      "2.6652326583862305\n",
      "2.6648142337799072\n",
      "2.664395809173584\n",
      "2.663978338241577\n",
      "2.6635611057281494\n",
      "2.6631453037261963\n",
      "2.662729024887085\n",
      "2.6623144149780273\n",
      "2.661900281906128\n",
      "2.6614859104156494\n",
      "2.6610734462738037\n",
      "2.6606605052948\n",
      "2.6602487564086914\n",
      "2.6598379611968994\n",
      "2.6594271659851074\n",
      "2.659017324447632\n",
      "2.6586084365844727\n",
      "2.6582000255584717\n",
      "2.657792329788208\n",
      "2.6573848724365234\n",
      "2.6569786071777344\n",
      "2.6565732955932617\n",
      "2.656167984008789\n",
      "2.655764102935791\n",
      "2.655360460281372\n",
      "2.6549580097198486\n",
      "2.6545562744140625\n",
      "2.6541547775268555\n",
      "2.653754234313965\n",
      "2.6533546447753906\n",
      "2.6529555320739746\n",
      "2.652557373046875\n",
      "2.6521599292755127\n",
      "2.6517632007598877\n",
      "2.6513671875\n",
      "2.650972366333008\n",
      "2.650578022003174\n",
      "2.6501848697662354\n",
      "2.649791955947876\n",
      "2.649399995803833\n",
      "2.6490087509155273\n",
      "2.6486191749572754\n",
      "2.6482295989990234\n",
      "2.647841453552246\n",
      "2.647453784942627\n",
      "2.6470673084259033\n",
      "2.646681308746338\n",
      "2.646296739578247\n",
      "2.6459126472473145\n",
      "2.645529270172119\n",
      "2.6451470851898193\n",
      "2.644766092300415\n",
      "2.644385814666748\n",
      "2.6440067291259766\n",
      "2.6436281204223633\n",
      "2.6432509422302246\n",
      "2.642874240875244\n",
      "2.642498731613159\n",
      "2.6421239376068115\n",
      "2.6417505741119385\n",
      "2.6413779258728027\n",
      "2.6410062313079834\n",
      "2.6406357288360596\n",
      "2.6402666568756104\n",
      "2.6398978233337402\n",
      "2.6395301818847656\n",
      "2.6391637325286865\n",
      "2.638798236846924\n",
      "2.6384341716766357\n",
      "2.6380703449249268\n",
      "2.6377081871032715\n",
      "2.6373469829559326\n",
      "2.63698673248291\n",
      "2.6366279125213623\n",
      "2.6362698078155518\n",
      "2.6359128952026367\n",
      "2.6355576515197754\n",
      "2.635202646255493\n",
      "2.6348490715026855\n",
      "2.6344969272613525\n",
      "2.634145498275757\n",
      "2.6337950229644775\n",
      "2.633445978164673\n",
      "2.6330983638763428\n",
      "2.632751226425171\n",
      "2.632405996322632\n",
      "2.632061243057251\n",
      "2.6317179203033447\n",
      "2.631376028060913\n",
      "2.631035089492798\n",
      "2.630695104598999\n",
      "2.6303563117980957\n",
      "2.630019187927246\n",
      "2.629682779312134\n",
      "2.629347801208496\n",
      "2.629013776779175\n",
      "2.6286814212799072\n",
      "2.628349781036377\n",
      "2.628019332885742\n",
      "2.627690553665161\n",
      "2.6273624897003174\n",
      "2.6270360946655273\n",
      "2.626710891723633\n",
      "2.6263864040374756\n",
      "2.626063346862793\n",
      "2.625741958618164\n",
      "2.6254212856292725\n",
      "2.6251018047332764\n",
      "2.624783515930176\n",
      "2.624467372894287\n",
      "2.6241512298583984\n",
      "2.6238369941711426\n",
      "2.6235239505767822\n",
      "2.6232123374938965\n",
      "2.622901678085327\n",
      "2.622591972351074\n",
      "2.622283697128296\n",
      "2.621976613998413\n",
      "2.621670961380005\n",
      "2.6213667392730713\n",
      "2.621063709259033\n",
      "2.6207613945007324\n",
      "2.6204609870910645\n",
      "2.620161533355713\n",
      "2.619863271713257\n",
      "2.6195662021636963\n",
      "2.6192703247070312\n",
      "2.618975877761841\n",
      "2.618682861328125\n",
      "2.6183910369873047\n",
      "2.618100166320801\n",
      "2.6178104877471924\n",
      "2.6175222396850586\n",
      "2.6172351837158203\n",
      "2.6169495582580566\n",
      "2.6166648864746094\n",
      "2.6163814067840576\n",
      "2.6160993576049805\n",
      "2.615818738937378\n",
      "2.6155385971069336\n",
      "2.615260362625122\n",
      "2.614983081817627\n",
      "2.6147069931030273\n",
      "2.6144320964813232\n",
      "2.6141586303710938\n",
      "2.6138858795166016\n",
      "2.613614797592163\n",
      "2.61334490776062\n",
      "2.6130762100219727\n",
      "2.6128084659576416\n",
      "2.612542152404785\n",
      "2.612277030944824\n",
      "2.6120128631591797\n",
      "2.611750364303589\n",
      "2.6114883422851562\n",
      "2.6112277507781982\n",
      "2.6109683513641357\n",
      "2.610710620880127\n",
      "2.6104536056518555\n",
      "2.6101977825164795\n",
      "2.609943389892578\n",
      "2.609689950942993\n",
      "2.6094369888305664\n",
      "2.6091861724853516\n",
      "2.608936071395874\n",
      "2.608686923980713\n",
      "2.608438730239868\n",
      "2.6081924438476562\n",
      "2.6079468727111816\n",
      "2.6077022552490234\n",
      "2.6074585914611816\n",
      "2.6072163581848145\n",
      "2.6069750785827637\n",
      "2.6067349910736084\n",
      "2.6064958572387695\n",
      "2.6062581539154053\n",
      "2.6060211658477783\n",
      "2.605785369873047\n",
      "2.605550527572632\n",
      "2.6053168773651123\n",
      "2.605084180831909\n",
      "2.6048524379730225\n",
      "2.6046221256256104\n",
      "2.6043925285339355\n",
      "2.604163885116577\n",
      "2.6039366722106934\n",
      "2.6037099361419678\n",
      "2.603484630584717\n",
      "2.603260040283203\n",
      "2.603036880493164\n",
      "2.602814197540283\n",
      "2.602592706680298\n",
      "2.602372884750366\n",
      "2.6021530628204346\n",
      "2.6019344329833984\n",
      "2.601716995239258\n",
      "2.6015002727508545\n",
      "2.6012840270996094\n",
      "2.601069927215576\n",
      "2.600856304168701\n",
      "2.600642681121826\n",
      "2.600430965423584\n",
      "2.600220203399658\n",
      "2.6000099182128906\n",
      "2.5998005867004395\n",
      "2.5995922088623047\n",
      "2.5993847846984863\n",
      "2.5991783142089844\n",
      "2.5989725589752197\n",
      "2.5987679958343506\n",
      "2.5985639095306396\n",
      "2.598360776901245\n",
      "2.598158359527588\n",
      "2.597956895828247\n",
      "2.597756862640381\n",
      "2.5975570678710938\n",
      "2.597358226776123\n",
      "2.5971598625183105\n",
      "2.5969626903533936\n",
      "2.596766471862793\n",
      "2.5965707302093506\n",
      "2.5963757038116455\n",
      "2.596181869506836\n",
      "2.5959887504577637\n",
      "2.5957961082458496\n",
      "2.595604658126831\n",
      "2.5954136848449707\n",
      "2.5952231884002686\n",
      "2.59503436088562\n",
      "2.5948455333709717\n",
      "2.5946578979492188\n",
      "2.5944700241088867\n",
      "2.5942838191986084\n",
      "2.5940988063812256\n",
      "2.5939135551452637\n",
      "2.593729257583618\n",
      "2.59354567527771\n",
      "2.59336256980896\n",
      "2.5931804180145264\n",
      "2.592999219894409\n",
      "2.59281849861145\n",
      "2.5926382541656494\n",
      "2.592458963394165\n",
      "2.592280387878418\n",
      "2.592102289199829\n",
      "2.5919246673583984\n",
      "2.591747999191284\n",
      "2.591571807861328\n",
      "2.5913963317871094\n",
      "2.591221570968628\n",
      "2.591047525405884\n",
      "2.5908737182617188\n",
      "2.590700626373291\n",
      "2.590528726577759\n",
      "2.5903565883636475\n",
      "2.5901856422424316\n",
      "2.590015172958374\n",
      "2.5898454189300537\n",
      "2.5896761417388916\n",
      "2.589507579803467\n",
      "2.589339256286621\n",
      "2.5891714096069336\n",
      "2.5890049934387207\n",
      "2.5888383388519287\n",
      "2.588672637939453\n",
      "2.5885071754455566\n",
      "2.5883426666259766\n",
      "2.5881781578063965\n",
      "2.588014841079712\n",
      "2.5878515243530273\n",
      "2.5876893997192383\n",
      "2.5875275135040283\n",
      "2.5873658657073975\n",
      "2.587205410003662\n",
      "2.5870444774627686\n",
      "2.5868849754333496\n",
      "2.5867257118225098\n",
      "2.58656644821167\n",
      "2.5864081382751465\n",
      "2.5862507820129395\n",
      "2.5860931873321533\n",
      "2.5859363079071045\n",
      "2.585780143737793\n",
      "2.5856244564056396\n",
      "2.5854690074920654\n",
      "2.5853142738342285\n",
      "2.58516001701355\n",
      "2.5850062370300293\n",
      "2.584852695465088\n",
      "2.584699869155884\n",
      "2.584547519683838\n",
      "2.584395408630371\n",
      "2.5842440128326416\n",
      "2.584092855453491\n",
      "2.583942174911499\n",
      "2.583791971206665\n",
      "2.5836422443389893\n",
      "2.5834927558898926\n",
      "2.5833442211151123\n",
      "2.583195686340332\n",
      "2.583047866821289\n",
      "2.582900285720825\n",
      "2.5827529430389404\n",
      "2.582606315612793\n",
      "2.5824601650238037\n",
      "2.5823140144348145\n",
      "2.5821690559387207\n",
      "2.582023859024048\n",
      "2.5818793773651123\n",
      "2.581735134124756\n",
      "2.5815913677215576\n",
      "2.5814478397369385\n",
      "2.5813047885894775\n",
      "2.581162691116333\n",
      "2.5810201168060303\n",
      "2.580878257751465\n",
      "2.5807368755340576\n",
      "2.5805962085723877\n",
      "2.5804555416107178\n",
      "2.580315351486206\n",
      "2.5801753997802734\n",
      "2.580036163330078\n",
      "2.579897403717041\n",
      "2.579758405685425\n",
      "2.579620122909546\n",
      "2.579482078552246\n",
      "2.5793445110321045\n",
      "2.579207420349121\n",
      "2.579070568084717\n",
      "2.5789341926574707\n",
      "2.5787975788116455\n",
      "2.578662395477295\n",
      "2.578526735305786\n",
      "2.5783917903900146\n",
      "2.5782570838928223\n",
      "2.578122615814209\n",
      "2.577989101409912\n",
      "2.577855348587036\n",
      "2.5777218341827393\n",
      "2.5775890350341797\n",
      "2.577456474304199\n",
      "2.577324151992798\n",
      "2.5771920680999756\n",
      "2.5770604610443115\n",
      "2.5769293308258057\n",
      "2.576798677444458\n",
      "2.5766680240631104\n",
      "2.576537847518921\n",
      "2.5764076709747314\n",
      "2.5762782096862793\n",
      "2.5761489868164062\n",
      "2.5760202407836914\n",
      "2.5758912563323975\n",
      "2.57576322555542\n",
      "2.5756351947784424\n",
      "2.575507640838623\n",
      "2.575380563735962\n",
      "2.57525372505188\n",
      "2.575126886367798\n",
      "2.575000524520874\n",
      "2.57487416267395\n",
      "2.5747485160827637\n",
      "2.5746233463287354\n",
      "2.574497699737549\n",
      "2.5743727684020996\n",
      "2.5742485523223877\n",
      "2.574124336242676\n",
      "2.574000358581543\n",
      "2.5738768577575684\n",
      "2.5737531185150146\n",
      "2.5736300945281982\n",
      "2.57350754737854\n",
      "2.573385000228882\n",
      "2.573262929916382\n",
      "2.573140859603882\n",
      "2.573019504547119\n",
      "2.5728981494903564\n",
      "2.572777509689331\n",
      "2.5726563930511475\n",
      "2.572535753250122\n",
      "2.572415828704834\n",
      "2.572296142578125\n",
      "2.572176456451416\n",
      "2.5720572471618652\n",
      "2.5719380378723145\n",
      "2.5718190670013428\n",
      "2.5717008113861084\n",
      "2.571582555770874\n",
      "2.5714645385742188\n",
      "2.5713467597961426\n",
      "2.5712292194366455\n",
      "2.5711123943328857\n",
      "2.570995330810547\n",
      "2.570878744125366\n",
      "2.5707626342773438\n",
      "2.5706467628479004\n",
      "2.570531129837036\n",
      "2.5704152584075928\n",
      "2.5702998638153076\n",
      "2.5701849460601807\n",
      "2.570070266723633\n",
      "2.569955825805664\n",
      "2.5698416233062744\n",
      "2.569727659225464\n",
      "2.5696139335632324\n",
      "2.56950044631958\n",
      "2.569387197494507\n",
      "2.5692741870880127\n",
      "2.5691616535186768\n",
      "2.56904935836792\n",
      "2.568936824798584\n",
      "2.5688252449035645\n",
      "2.5687131881713867\n",
      "2.5686018466949463\n",
      "2.568490505218506\n",
      "2.5683794021606445\n",
      "2.5682687759399414\n",
      "2.5681583881378174\n",
      "2.5680482387542725\n",
      "2.5679380893707275\n",
      "2.5678281784057617\n",
      "2.567718982696533\n",
      "2.5676095485687256\n",
      "2.567500591278076\n",
      "2.5673916339874268\n",
      "2.5672831535339355\n",
      "2.5671749114990234\n",
      "2.5670666694641113\n",
      "2.5669591426849365\n",
      "2.5668511390686035\n",
      "2.5667436122894287\n",
      "2.566636800765991\n",
      "2.5665297508239746\n",
      "2.566422700881958\n",
      "2.5663161277770996\n",
      "2.5662100315093994\n",
      "2.566103935241699\n",
      "2.565998077392578\n",
      "2.565892457962036\n",
      "2.5657870769500732\n",
      "2.5656816959381104\n",
      "2.5655767917633057\n",
      "2.56547212600708\n",
      "2.5653679370880127\n",
      "2.565263271331787\n",
      "2.565159320831299\n",
      "2.5650553703308105\n",
      "2.5649518966674805\n",
      "2.5648486614227295\n",
      "2.5647449493408203\n",
      "2.5646417140960693\n",
      "2.5645391941070557\n",
      "2.564436197280884\n",
      "2.564333915710449\n",
      "2.5642316341400146\n",
      "2.5641298294067383\n",
      "2.564028024673462\n",
      "2.5639262199401855\n",
      "2.5638251304626465\n",
      "2.5637240409851074\n",
      "2.5636231899261475\n",
      "2.5635223388671875\n",
      "2.5634214878082275\n",
      "2.563321352005005\n",
      "2.563220977783203\n",
      "2.5631213188171387\n",
      "2.563020944595337\n",
      "2.5629217624664307\n",
      "2.5628223419189453\n",
      "2.562723159790039\n",
      "2.562624454498291\n",
      "2.562525510787964\n",
      "2.562426805496216\n",
      "2.562328338623047\n",
      "2.562230110168457\n",
      "2.5621321201324463\n",
      "2.5620346069335938\n",
      "2.561936378479004\n",
      "2.5618393421173096\n",
      "2.561741828918457\n",
      "2.5616445541381836\n",
      "2.5615475177764893\n",
      "2.561450719833374\n",
      "2.561354398727417\n",
      "2.561257839202881\n",
      "2.561161756515503\n",
      "2.561065673828125\n",
      "2.5609700679779053\n"
     ]
    }
   ],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T14:58:52.319061Z",
     "start_time": "2025-07-16T14:58:52.313347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## So training taking long time so in poytorch generally people will consider lot of mini batches and process them in to multiple small batche s\n",
    "\n",
    "## to get mini batch we will do this\n",
    "torch.randint(0, X.shape[0], (32,))"
   ],
   "id": "87c6646e5116f8dc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([178201,  19598, 189969, 180573,  93173, 227570, 124706, 217284, 181533,\n",
       "          9905, 178848,  36764,  91069,  61365, 217668,  18011,  81101,  99325,\n",
       "        149333, 222166,  88646,  41626, 166278,  20113,  63472, 170630, 146087,\n",
       "        150304,  82268,  10966,   9765, 214269])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T15:09:13.272247Z",
     "start_time": "2025-07-16T15:09:13.268248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ],
   "id": "d4770bb5ba9cf473",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T15:10:20.200775Z",
     "start_time": "2025-07-16T15:10:20.116049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# so every time we run this the weights keeps on updating\n",
    "\n",
    "for _ in range(100):\n",
    "\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    # Forward pass\n",
    "    embedding = C[X[ix]] #(32,3,2)\n",
    "    Hidden_Layer = torch.tanh(embedding.view(-1,6)@ W1 + B1) #(32, 100)\n",
    "    logits = Hidden_Layer @ W2 + B2 # (32,27)\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update the weights\n",
    "    for p in parameters:\n",
    "        p.data = p.data + (-0.1 * p.grad)\n"
   ],
   "id": "263ab752c89bffa2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.102996826171875\n",
      "72.2281494140625\n",
      "77.70462799072266\n",
      "53.06869888305664\n",
      "71.79202270507812\n",
      "63.936954498291016\n",
      "76.86680603027344\n",
      "50.71491622924805\n",
      "52.626609802246094\n",
      "57.2064323425293\n",
      "63.42100524902344\n",
      "63.934715270996094\n",
      "55.886627197265625\n",
      "61.298431396484375\n",
      "59.826133728027344\n",
      "67.75978088378906\n",
      "51.124176025390625\n",
      "74.03736114501953\n",
      "58.24430847167969\n",
      "44.324581146240234\n",
      "72.9111328125\n",
      "56.85130310058594\n",
      "67.76238250732422\n",
      "50.566490173339844\n",
      "57.61076736450195\n",
      "68.787353515625\n",
      "60.94881057739258\n",
      "50.06743240356445\n",
      "41.504974365234375\n",
      "50.62968444824219\n",
      "43.43655014038086\n",
      "56.62358474731445\n",
      "49.547393798828125\n",
      "49.7483024597168\n",
      "38.577606201171875\n",
      "53.53091812133789\n",
      "52.3691520690918\n",
      "60.785789489746094\n",
      "50.25499725341797\n",
      "52.74367141723633\n",
      "44.358863830566406\n",
      "43.72895812988281\n",
      "53.458587646484375\n",
      "57.12788772583008\n",
      "48.348880767822266\n",
      "58.23482894897461\n",
      "36.028221130371094\n",
      "37.15681076049805\n",
      "51.946388244628906\n",
      "40.8278694152832\n",
      "39.22008514404297\n",
      "41.145408630371094\n",
      "42.253726959228516\n",
      "44.29958724975586\n",
      "47.7703742980957\n",
      "52.81755828857422\n",
      "44.32319641113281\n",
      "47.41299819946289\n",
      "48.864295959472656\n",
      "45.03642654418945\n",
      "29.030550003051758\n",
      "38.23058319091797\n",
      "44.068359375\n",
      "55.26129150390625\n",
      "47.15221405029297\n",
      "49.00358963012695\n",
      "45.76423645019531\n",
      "44.249298095703125\n",
      "47.047935485839844\n",
      "36.919822692871094\n",
      "47.65021896362305\n",
      "42.21303176879883\n",
      "38.688514709472656\n",
      "36.49137496948242\n",
      "48.989593505859375\n",
      "50.06684875488281\n",
      "51.87882995605469\n",
      "37.191715240478516\n",
      "42.0115966796875\n",
      "36.5540771484375\n",
      "27.203413009643555\n",
      "36.990116119384766\n",
      "30.670503616333008\n",
      "36.292030334472656\n",
      "35.3455696105957\n",
      "48.1274299621582\n",
      "47.47547149658203\n",
      "38.949554443359375\n",
      "33.11769104003906\n",
      "45.5216064453125\n",
      "40.78708267211914\n",
      "43.116207122802734\n",
      "28.945816040039062\n",
      "42.877376556396484\n",
      "40.7488899230957\n",
      "34.367191314697266\n",
      "33.1829948425293\n",
      "35.00229263305664\n",
      "37.887062072753906\n",
      "40.26958084106445\n"
     ]
    }
   ],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T15:10:20.507021Z",
     "start_time": "2025-07-16T15:10:20.456885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#loss of entire model or loss of whole training data set\n",
    "embedding = C[X] #(32,3,2)\n",
    "Hidden_Layer = torch.tanh(embedding.view(-1,6)@ W1 + B1) #(32, 100)\n",
    "logits = Hidden_Layer @ W2 + B2 # (32,27)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ],
   "id": "51ff2cb33b21c2ee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(36.8598, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T19:06:01.922148Z",
     "start_time": "2025-07-16T19:06:01.913033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Finding the optimal Learning rate\n",
    "learning_rates = 10**(torch.linspace(-3,0,1000))\n",
    "learning_rates"
   ],
   "id": "5f82b3ed088012e9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n",
       "        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n",
       "        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
       "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
       "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
       "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n",
       "        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n",
       "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n",
       "        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n",
       "        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
       "        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n",
       "        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n",
       "        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
       "        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n",
       "        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
       "        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n",
       "        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n",
       "        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n",
       "        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n",
       "        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n",
       "        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n",
       "        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n",
       "        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n",
       "        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n",
       "        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n",
       "        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n",
       "        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n",
       "        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n",
       "        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n",
       "        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n",
       "        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n",
       "        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n",
       "        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n",
       "        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n",
       "        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n",
       "        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n",
       "        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n",
       "        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n",
       "        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n",
       "        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n",
       "        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n",
       "        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n",
       "        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n",
       "        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n",
       "        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n",
       "        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n",
       "        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n",
       "        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n",
       "        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n",
       "        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n",
       "        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n",
       "        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n",
       "        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n",
       "        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n",
       "        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n",
       "        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n",
       "        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n",
       "        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n",
       "        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n",
       "        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n",
       "        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n",
       "        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n",
       "        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n",
       "        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n",
       "        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n",
       "        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n",
       "        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n",
       "        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n",
       "        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n",
       "        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n",
       "        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n",
       "        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n",
       "        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n",
       "        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n",
       "        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n",
       "        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n",
       "        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n",
       "        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n",
       "        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n",
       "        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n",
       "        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n",
       "        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n",
       "        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n",
       "        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n",
       "        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n",
       "        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n",
       "        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n",
       "        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n",
       "        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n",
       "        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n",
       "        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n",
       "        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n",
       "        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n",
       "        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n",
       "        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n",
       "        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n",
       "        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n",
       "        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n",
       "        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n",
       "        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n",
       "        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n",
       "        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n",
       "        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n",
       "        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n",
       "        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n",
       "        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T19:09:04.383090Z",
     "start_time": "2025-07-16T19:09:03.866629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# so every time we run this the weights keeps on updating\n",
    "lri = []\n",
    "lossi = []\n",
    "\n",
    "for i in range(1000):\n",
    "    #mini batch\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "\n",
    "    # Forward pass\n",
    "    embedding = C[X[ix]] #(32,3,2)\n",
    "    Hidden_Layer = torch.tanh(embedding.view(-1,6)@ W1 + B1) #(32, 100)\n",
    "    logits = Hidden_Layer @ W2 + B2 # (32,27)\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update the weights\n",
    "    lr = learning_rates[i]\n",
    "    for p in parameters:\n",
    "        p.data = p.data + (-lr * p.grad)\n",
    "    lri.append(lr)\n",
    "    lossi.append(loss.item())\n"
   ],
   "id": "1e2f84f56608c445",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5044195652008057\n",
      "2.605086088180542\n",
      "2.5266778469085693\n",
      "2.8355984687805176\n",
      "2.394714593887329\n",
      "2.6737442016601562\n",
      "2.5800387859344482\n",
      "2.655266046524048\n",
      "2.640887498855591\n",
      "2.7048661708831787\n",
      "2.5208592414855957\n",
      "2.7826919555664062\n",
      "2.189084529876709\n",
      "2.724280834197998\n",
      "3.0088329315185547\n",
      "2.461857318878174\n",
      "2.6590371131896973\n",
      "2.6130051612854004\n",
      "2.556684970855713\n",
      "2.6731133460998535\n",
      "2.498938798904419\n",
      "2.184372901916504\n",
      "2.391960859298706\n",
      "2.2629942893981934\n",
      "2.490084171295166\n",
      "2.6231496334075928\n",
      "2.448230743408203\n",
      "2.8153786659240723\n",
      "2.6792073249816895\n",
      "2.2862510681152344\n",
      "2.2232565879821777\n",
      "2.7774198055267334\n",
      "2.4445290565490723\n",
      "2.3220794200897217\n",
      "2.687422037124634\n",
      "2.614575147628784\n",
      "2.136293411254883\n",
      "3.021648406982422\n",
      "2.7669873237609863\n",
      "2.6312499046325684\n",
      "2.4933176040649414\n",
      "2.169806480407715\n",
      "2.3141815662384033\n",
      "2.464982509613037\n",
      "3.055375337600708\n",
      "2.4708337783813477\n",
      "2.4383184909820557\n",
      "2.6338961124420166\n",
      "2.767061710357666\n",
      "2.9400711059570312\n",
      "2.561983108520508\n",
      "2.332606077194214\n",
      "2.4984805583953857\n",
      "2.6300060749053955\n",
      "2.41021466255188\n",
      "2.4050965309143066\n",
      "2.186357259750366\n",
      "2.7186455726623535\n",
      "2.764401912689209\n",
      "2.731679677963257\n",
      "2.6042003631591797\n",
      "2.5161550045013428\n",
      "2.783421754837036\n",
      "2.7048161029815674\n",
      "2.779796838760376\n",
      "3.041167736053467\n",
      "2.624143362045288\n",
      "2.5935986042022705\n",
      "2.8084871768951416\n",
      "2.5567524433135986\n",
      "2.485051393508911\n",
      "2.541823387145996\n",
      "2.5786819458007812\n",
      "2.44484806060791\n",
      "2.4674103260040283\n",
      "2.432718515396118\n",
      "2.5251331329345703\n",
      "2.3216841220855713\n",
      "2.6500842571258545\n",
      "2.1650564670562744\n",
      "3.1720478534698486\n",
      "2.3902101516723633\n",
      "2.6742100715637207\n",
      "2.5754287242889404\n",
      "2.484482526779175\n",
      "2.77103590965271\n",
      "2.4888112545013428\n",
      "2.793422222137451\n",
      "2.604999542236328\n",
      "2.887827157974243\n",
      "2.820312976837158\n",
      "3.052868127822876\n",
      "2.3283228874206543\n",
      "2.5102226734161377\n",
      "2.3891236782073975\n",
      "2.543384552001953\n",
      "2.77447247505188\n",
      "2.525284767150879\n",
      "2.4969751834869385\n",
      "2.69490122795105\n",
      "2.4884462356567383\n",
      "2.6485443115234375\n",
      "2.5776896476745605\n",
      "2.341357946395874\n",
      "2.6474099159240723\n",
      "2.7256362438201904\n",
      "2.478088855743408\n",
      "2.374361991882324\n",
      "2.2437832355499268\n",
      "2.402968406677246\n",
      "2.906775951385498\n",
      "2.67033052444458\n",
      "2.1401450634002686\n",
      "2.454019546508789\n",
      "3.2365591526031494\n",
      "2.5470776557922363\n",
      "2.5635437965393066\n",
      "3.0840413570404053\n",
      "2.6196508407592773\n",
      "2.5360593795776367\n",
      "2.5478687286376953\n",
      "3.0076804161071777\n",
      "2.466249942779541\n",
      "2.732372999191284\n",
      "2.427826166152954\n",
      "2.6380856037139893\n",
      "2.599639415740967\n",
      "2.458918571472168\n",
      "2.78711199760437\n",
      "2.4878926277160645\n",
      "2.566663980484009\n",
      "2.6760833263397217\n",
      "2.254283905029297\n",
      "2.940263032913208\n",
      "2.5086488723754883\n",
      "2.819082736968994\n",
      "2.4590799808502197\n",
      "2.6137731075286865\n",
      "2.530132293701172\n",
      "3.1167123317718506\n",
      "2.16873836517334\n",
      "2.3415260314941406\n",
      "2.563464641571045\n",
      "2.2109577655792236\n",
      "2.389596462249756\n",
      "2.746001720428467\n",
      "2.7681925296783447\n",
      "2.3069448471069336\n",
      "2.6057562828063965\n",
      "2.6690561771392822\n",
      "2.601593017578125\n",
      "2.8128445148468018\n",
      "2.333996057510376\n",
      "2.34806227684021\n",
      "2.946467876434326\n",
      "2.4664480686187744\n",
      "2.3647923469543457\n",
      "2.6060147285461426\n",
      "2.4423558712005615\n",
      "2.515711545944214\n",
      "2.657515287399292\n",
      "2.526561737060547\n",
      "2.4704816341400146\n",
      "2.5176987648010254\n",
      "2.729576587677002\n",
      "2.3151724338531494\n",
      "2.570439100265503\n",
      "2.702486753463745\n",
      "2.437040090560913\n",
      "2.662249803543091\n",
      "2.4360148906707764\n",
      "2.3746495246887207\n",
      "2.554234266281128\n",
      "2.5643060207366943\n",
      "2.6297097206115723\n",
      "2.5483946800231934\n",
      "2.106585741043091\n",
      "2.433999538421631\n",
      "2.755725622177124\n",
      "2.325716018676758\n",
      "2.565551280975342\n",
      "2.4215822219848633\n",
      "2.73604416847229\n",
      "2.6988327503204346\n",
      "2.6870498657226562\n",
      "2.76383113861084\n",
      "2.6284680366516113\n",
      "2.5770394802093506\n",
      "2.5263352394104004\n",
      "2.581040859222412\n",
      "2.9240517616271973\n",
      "2.3280763626098633\n",
      "2.59401798248291\n",
      "2.5611767768859863\n",
      "2.7659831047058105\n",
      "2.561138153076172\n",
      "2.655503749847412\n",
      "2.4438023567199707\n",
      "2.6288132667541504\n",
      "3.083345413208008\n",
      "2.6993861198425293\n",
      "2.3061606884002686\n",
      "2.4628961086273193\n",
      "2.712371826171875\n",
      "2.8707005977630615\n",
      "2.7765915393829346\n",
      "2.3047707080841064\n",
      "2.6183395385742188\n",
      "2.616182327270508\n",
      "2.424960136413574\n",
      "2.5741477012634277\n",
      "2.429080009460449\n",
      "2.1153762340545654\n",
      "2.3084375858306885\n",
      "2.6142561435699463\n",
      "2.4825384616851807\n",
      "2.822115182876587\n",
      "2.752134084701538\n",
      "2.2284514904022217\n",
      "2.5471510887145996\n",
      "2.818369150161743\n",
      "2.2895147800445557\n",
      "2.4883179664611816\n",
      "2.6851305961608887\n",
      "2.4741852283477783\n",
      "2.625218152999878\n",
      "2.7718048095703125\n",
      "2.615565299987793\n",
      "2.269162893295288\n",
      "2.8376376628875732\n",
      "2.284484386444092\n",
      "2.4962456226348877\n",
      "2.860646963119507\n",
      "2.3119678497314453\n",
      "2.9386754035949707\n",
      "2.659541606903076\n",
      "2.3948774337768555\n",
      "2.8059980869293213\n",
      "2.565610647201538\n",
      "2.578463077545166\n",
      "2.4081363677978516\n",
      "2.8062236309051514\n",
      "2.7854881286621094\n",
      "2.4700334072113037\n",
      "2.9830894470214844\n",
      "2.4037113189697266\n",
      "2.3455607891082764\n",
      "2.787489175796509\n",
      "2.32438063621521\n",
      "2.2516732215881348\n",
      "2.459929943084717\n",
      "2.6730189323425293\n",
      "2.848453998565674\n",
      "2.4424147605895996\n",
      "2.6059842109680176\n",
      "2.4721832275390625\n",
      "2.3510656356811523\n",
      "2.426837921142578\n",
      "2.494560956954956\n",
      "2.2879960536956787\n",
      "2.8556082248687744\n",
      "2.250986099243164\n",
      "2.217391014099121\n",
      "2.269052743911743\n",
      "2.5197534561157227\n",
      "2.581387996673584\n",
      "2.633023500442505\n",
      "2.555213689804077\n",
      "2.5838499069213867\n",
      "2.3785552978515625\n",
      "2.7362565994262695\n",
      "2.4795408248901367\n",
      "2.879493236541748\n",
      "2.6501729488372803\n",
      "2.2982394695281982\n",
      "2.6501615047454834\n",
      "2.3666865825653076\n",
      "2.12414288520813\n",
      "2.7625744342803955\n",
      "2.7434468269348145\n",
      "2.401641845703125\n",
      "2.431412696838379\n",
      "2.2619216442108154\n",
      "2.513023614883423\n",
      "2.6977696418762207\n",
      "2.4687421321868896\n",
      "2.514568567276001\n",
      "2.773075580596924\n",
      "3.1755800247192383\n",
      "2.6830220222473145\n",
      "2.5360727310180664\n",
      "2.4512269496917725\n",
      "2.695005178451538\n",
      "2.7156972885131836\n",
      "2.5820484161376953\n",
      "2.582756996154785\n",
      "2.5405707359313965\n",
      "2.75500750541687\n",
      "2.820528507232666\n",
      "2.4633798599243164\n",
      "2.5540292263031006\n",
      "2.3714025020599365\n",
      "2.7534379959106445\n",
      "2.454982042312622\n",
      "2.4746148586273193\n",
      "2.6578433513641357\n",
      "2.587925434112549\n",
      "2.7777297496795654\n",
      "2.474774122238159\n",
      "2.57550048828125\n",
      "2.426936149597168\n",
      "2.5570361614227295\n",
      "2.4296979904174805\n",
      "2.5557548999786377\n",
      "2.5900492668151855\n",
      "2.9626588821411133\n",
      "2.594357490539551\n",
      "2.6942665576934814\n",
      "2.195923328399658\n",
      "2.7170486450195312\n",
      "2.6778151988983154\n",
      "2.9199941158294678\n",
      "2.616602659225464\n",
      "2.7070698738098145\n",
      "2.216799736022949\n",
      "2.5516915321350098\n",
      "2.6601414680480957\n",
      "2.8428573608398438\n",
      "2.583436965942383\n",
      "2.6787428855895996\n",
      "2.8393383026123047\n",
      "2.4515645503997803\n",
      "2.555676221847534\n",
      "2.519368886947632\n",
      "2.4472856521606445\n",
      "2.7207396030426025\n",
      "2.563554525375366\n",
      "2.5661087036132812\n",
      "2.45133113861084\n",
      "2.6284618377685547\n",
      "2.391911268234253\n",
      "2.642791986465454\n",
      "2.5795376300811768\n",
      "2.279959201812744\n",
      "2.2751612663269043\n",
      "2.7730162143707275\n",
      "2.4455761909484863\n",
      "2.5329482555389404\n",
      "2.6609342098236084\n",
      "2.901860237121582\n",
      "2.933880567550659\n",
      "2.3963727951049805\n",
      "2.401358127593994\n",
      "2.4001450538635254\n",
      "2.6029951572418213\n",
      "2.595221757888794\n",
      "2.6370811462402344\n",
      "2.3346939086914062\n",
      "2.6949844360351562\n",
      "2.7414443492889404\n",
      "2.2395670413970947\n",
      "2.6048173904418945\n",
      "2.588397741317749\n",
      "2.5342447757720947\n",
      "3.098332405090332\n",
      "2.328372001647949\n",
      "2.5987393856048584\n",
      "2.4542789459228516\n",
      "2.3509538173675537\n",
      "2.552262544631958\n",
      "2.644714117050171\n",
      "2.6595828533172607\n",
      "2.5167224407196045\n",
      "2.388047456741333\n",
      "2.579615831375122\n",
      "2.425981283187866\n",
      "2.658824920654297\n",
      "2.7054526805877686\n",
      "2.589223861694336\n",
      "2.600628614425659\n",
      "2.833707332611084\n",
      "2.793574333190918\n",
      "2.674589157104492\n",
      "2.772909641265869\n",
      "2.3139421939849854\n",
      "2.8571128845214844\n",
      "2.566636562347412\n",
      "2.673476457595825\n",
      "2.5836710929870605\n",
      "2.1897075176239014\n",
      "2.8295068740844727\n",
      "2.6975245475769043\n",
      "2.6393635272979736\n",
      "2.5559213161468506\n",
      "2.5856857299804688\n",
      "2.8509747982025146\n",
      "2.2449331283569336\n",
      "3.164663314819336\n",
      "2.673036813735962\n",
      "2.488328695297241\n",
      "2.889646291732788\n",
      "2.3450257778167725\n",
      "2.7530486583709717\n",
      "2.763901710510254\n",
      "2.8779053688049316\n",
      "2.5280497074127197\n",
      "2.6260154247283936\n",
      "2.436962366104126\n",
      "2.6732163429260254\n",
      "2.4979617595672607\n",
      "2.4313740730285645\n",
      "2.6805989742279053\n",
      "2.443802833557129\n",
      "2.3703582286834717\n",
      "2.4926505088806152\n",
      "2.7756872177124023\n",
      "2.773549795150757\n",
      "2.5220916271209717\n",
      "2.7605087757110596\n",
      "2.45585036277771\n",
      "2.3255553245544434\n",
      "2.6814348697662354\n",
      "2.606451988220215\n",
      "2.49709415435791\n",
      "2.6697235107421875\n",
      "2.643195390701294\n",
      "2.4592742919921875\n",
      "2.752019166946411\n",
      "2.5256454944610596\n",
      "2.766678810119629\n",
      "2.495816707611084\n",
      "2.456744909286499\n",
      "3.0136518478393555\n",
      "2.583606243133545\n",
      "2.795625925064087\n",
      "2.3700239658355713\n",
      "2.5866153240203857\n",
      "2.5027847290039062\n",
      "2.58369779586792\n",
      "2.4977197647094727\n",
      "2.5184078216552734\n",
      "2.5202763080596924\n",
      "2.298980236053467\n",
      "2.195937156677246\n",
      "2.584846258163452\n",
      "2.872283458709717\n",
      "2.627563714981079\n",
      "2.6522135734558105\n",
      "2.449972152709961\n",
      "2.448187828063965\n",
      "2.532780885696411\n",
      "2.5439085960388184\n",
      "2.5462942123413086\n",
      "2.6774401664733887\n",
      "2.6122894287109375\n",
      "2.8531930446624756\n",
      "2.6309926509857178\n",
      "2.6996803283691406\n",
      "2.3170528411865234\n",
      "2.7969157695770264\n",
      "2.5177674293518066\n",
      "2.523059129714966\n",
      "2.265272617340088\n",
      "2.540808916091919\n",
      "2.2550907135009766\n",
      "2.506608009338379\n",
      "2.2658491134643555\n",
      "2.432325601577759\n",
      "2.6243808269500732\n",
      "2.6551618576049805\n",
      "2.8830504417419434\n",
      "2.6065421104431152\n",
      "2.6172804832458496\n",
      "2.7104904651641846\n",
      "2.5519025325775146\n",
      "2.6317224502563477\n",
      "2.775010824203491\n",
      "2.6516361236572266\n",
      "2.886007785797119\n",
      "2.66445255279541\n",
      "2.6541736125946045\n",
      "2.492936849594116\n",
      "2.2717912197113037\n",
      "2.4596147537231445\n",
      "2.254178524017334\n",
      "2.406397819519043\n",
      "2.8676986694335938\n",
      "2.519160747528076\n",
      "2.7731635570526123\n",
      "2.3139097690582275\n",
      "2.60431170463562\n",
      "2.5649704933166504\n",
      "2.37996244430542\n",
      "2.6580328941345215\n",
      "2.565312623977661\n",
      "2.358922004699707\n",
      "2.9277307987213135\n",
      "2.924962282180786\n",
      "2.415886402130127\n",
      "2.743135690689087\n",
      "2.646327257156372\n",
      "2.7268226146698\n",
      "3.1712183952331543\n",
      "2.6166608333587646\n",
      "2.5634124279022217\n",
      "2.5152487754821777\n",
      "2.7765698432922363\n",
      "2.280900001525879\n",
      "2.7848639488220215\n",
      "2.6157689094543457\n",
      "2.5108983516693115\n",
      "2.3738675117492676\n",
      "2.3389041423797607\n",
      "2.6658987998962402\n",
      "2.1582140922546387\n",
      "2.4862706661224365\n",
      "2.5182647705078125\n",
      "2.2835183143615723\n",
      "2.4313042163848877\n",
      "2.5427322387695312\n",
      "2.3923563957214355\n",
      "2.861759901046753\n",
      "2.699854612350464\n",
      "2.8265113830566406\n",
      "2.59382700920105\n",
      "2.5975871086120605\n",
      "2.5373027324676514\n",
      "2.2997779846191406\n",
      "3.0501322746276855\n",
      "2.350926399230957\n",
      "2.5662167072296143\n",
      "2.5119545459747314\n",
      "2.5512726306915283\n",
      "2.3559117317199707\n",
      "2.8236749172210693\n",
      "2.3198604583740234\n",
      "2.3096842765808105\n",
      "2.850189208984375\n",
      "2.815617561340332\n",
      "2.6455233097076416\n",
      "2.740341901779175\n",
      "2.7155299186706543\n",
      "2.505363941192627\n",
      "2.653918743133545\n",
      "2.560945749282837\n",
      "2.6363236904144287\n",
      "2.815053939819336\n",
      "2.504509687423706\n",
      "2.890225887298584\n",
      "3.2102158069610596\n",
      "2.6401479244232178\n",
      "2.780977964401245\n",
      "2.4274208545684814\n",
      "2.2587404251098633\n",
      "2.7332019805908203\n",
      "2.5616486072540283\n",
      "2.5804967880249023\n",
      "2.609682559967041\n",
      "2.3514294624328613\n",
      "2.49515962600708\n",
      "2.606980562210083\n",
      "2.7307567596435547\n",
      "2.326205253601074\n",
      "2.717107057571411\n",
      "2.741671562194824\n",
      "2.5247714519500732\n",
      "3.0209851264953613\n",
      "2.7667765617370605\n",
      "3.0467865467071533\n",
      "2.3059797286987305\n",
      "2.365173101425171\n",
      "3.1603431701660156\n",
      "3.0162625312805176\n",
      "2.791790246963501\n",
      "2.3757760524749756\n",
      "2.429957866668701\n",
      "2.4944958686828613\n",
      "2.832521677017212\n",
      "2.4515795707702637\n",
      "2.481067180633545\n",
      "2.6487178802490234\n",
      "2.2682251930236816\n",
      "2.586682081222534\n",
      "2.7268564701080322\n",
      "2.237518787384033\n",
      "2.551992654800415\n",
      "2.3599774837493896\n",
      "2.662811040878296\n",
      "2.5149571895599365\n",
      "2.5497336387634277\n",
      "2.768061637878418\n",
      "2.6998159885406494\n",
      "2.6443004608154297\n",
      "2.354890823364258\n",
      "2.6004881858825684\n",
      "2.709778070449829\n",
      "2.897961139678955\n",
      "2.6928443908691406\n",
      "2.3624627590179443\n",
      "2.787935972213745\n",
      "2.413865804672241\n",
      "2.8797473907470703\n",
      "2.632072687149048\n",
      "2.59045147895813\n",
      "2.5933315753936768\n",
      "2.6713714599609375\n",
      "2.5933706760406494\n",
      "2.4642927646636963\n",
      "2.9424595832824707\n",
      "2.9922773838043213\n",
      "2.5738682746887207\n",
      "2.669280767440796\n",
      "2.774561643600464\n",
      "2.769256114959717\n",
      "2.990971088409424\n",
      "2.261874198913574\n",
      "2.645841360092163\n",
      "2.4422414302825928\n",
      "2.485337018966675\n",
      "2.328960418701172\n",
      "2.675912618637085\n",
      "2.851984977722168\n",
      "2.668914318084717\n",
      "2.4428131580352783\n",
      "2.4938554763793945\n",
      "2.405888557434082\n",
      "2.364372968673706\n",
      "2.4371113777160645\n",
      "2.707387924194336\n",
      "2.3883514404296875\n",
      "2.596233606338501\n",
      "2.4219820499420166\n",
      "2.578582286834717\n",
      "2.8479204177856445\n",
      "2.4791598320007324\n",
      "2.4781787395477295\n",
      "2.527010917663574\n",
      "2.7655229568481445\n",
      "2.7305045127868652\n",
      "2.5981807708740234\n",
      "2.43286395072937\n",
      "2.58581280708313\n",
      "2.1364846229553223\n",
      "2.279693603515625\n",
      "2.4245693683624268\n",
      "2.763902425765991\n",
      "2.452604055404663\n",
      "2.7171897888183594\n",
      "2.7167277336120605\n",
      "3.0632662773132324\n",
      "2.4808430671691895\n",
      "2.511359930038452\n",
      "2.8754799365997314\n",
      "2.5978341102600098\n",
      "2.718942165374756\n",
      "2.700256586074829\n",
      "2.454660177230835\n",
      "2.358532428741455\n",
      "2.52256178855896\n",
      "2.696981430053711\n",
      "3.244936943054199\n",
      "2.6206259727478027\n",
      "2.9233455657958984\n",
      "2.731825590133667\n",
      "2.631150722503662\n",
      "2.7807631492614746\n",
      "2.5933353900909424\n",
      "2.4565556049346924\n",
      "2.791839599609375\n",
      "2.5871365070343018\n",
      "2.713240385055542\n",
      "2.627143383026123\n",
      "2.363201141357422\n",
      "2.9295694828033447\n",
      "3.066178798675537\n",
      "2.5434348583221436\n",
      "2.748070001602173\n",
      "2.5300865173339844\n",
      "2.980254650115967\n",
      "2.204102039337158\n",
      "2.6525652408599854\n",
      "2.8683645725250244\n",
      "3.0927298069000244\n",
      "2.8832240104675293\n",
      "2.517143487930298\n",
      "2.726585865020752\n",
      "3.371673345565796\n",
      "2.9409101009368896\n",
      "2.4557008743286133\n",
      "2.282258987426758\n",
      "2.8882198333740234\n",
      "2.0892813205718994\n",
      "2.618091106414795\n",
      "2.781355857849121\n",
      "2.794976234436035\n",
      "2.881737470626831\n",
      "2.54357647895813\n",
      "2.7645833492279053\n",
      "2.7551498413085938\n",
      "2.282782793045044\n",
      "2.606234073638916\n",
      "2.1823813915252686\n",
      "2.686657190322876\n",
      "3.1350021362304688\n",
      "2.35642147064209\n",
      "2.6155648231506348\n",
      "2.703561305999756\n",
      "3.0828840732574463\n",
      "2.4717578887939453\n",
      "2.3334403038024902\n",
      "2.5796432495117188\n",
      "2.8957555294036865\n",
      "2.783615827560425\n",
      "2.932157516479492\n",
      "2.5045855045318604\n",
      "2.6101531982421875\n",
      "2.6473448276519775\n",
      "2.890509843826294\n",
      "2.7719860076904297\n",
      "2.721282482147217\n",
      "2.7299342155456543\n",
      "2.96218204498291\n",
      "2.7691729068756104\n",
      "2.908111333847046\n",
      "2.5821876525878906\n",
      "2.840733289718628\n",
      "2.6610400676727295\n",
      "2.35860276222229\n",
      "2.786565065383911\n",
      "2.9778835773468018\n",
      "2.9045302867889404\n",
      "2.7394840717315674\n",
      "2.6984519958496094\n",
      "2.8387017250061035\n",
      "3.066462993621826\n",
      "2.7558743953704834\n",
      "2.557765245437622\n",
      "2.7801527976989746\n",
      "2.9032480716705322\n",
      "2.5825870037078857\n",
      "2.9480109214782715\n",
      "2.7753641605377197\n",
      "3.13700270652771\n",
      "2.5495970249176025\n",
      "2.954550266265869\n",
      "2.5221166610717773\n",
      "2.993727207183838\n",
      "2.877544641494751\n",
      "2.8195536136627197\n",
      "2.9429233074188232\n",
      "2.923422336578369\n",
      "2.47930645942688\n",
      "2.786153554916382\n",
      "2.5813159942626953\n",
      "2.6364357471466064\n",
      "3.0659124851226807\n",
      "3.5664851665496826\n",
      "3.669398546218872\n",
      "2.9799089431762695\n",
      "3.0003769397735596\n",
      "2.5993659496307373\n",
      "2.8340859413146973\n",
      "2.9154181480407715\n",
      "2.86881160736084\n",
      "2.2778499126434326\n",
      "3.1066651344299316\n",
      "2.9355196952819824\n",
      "3.3199191093444824\n",
      "3.1696810722351074\n",
      "3.125520944595337\n",
      "2.793729066848755\n",
      "2.6396031379699707\n",
      "2.7981903553009033\n",
      "2.8836123943328857\n",
      "2.6292903423309326\n",
      "2.9517126083374023\n",
      "3.1484577655792236\n",
      "2.773249387741089\n",
      "3.439940929412842\n",
      "3.1555416584014893\n",
      "2.905716896057129\n",
      "2.8340582847595215\n",
      "2.741304874420166\n",
      "3.8625802993774414\n",
      "3.7001891136169434\n",
      "3.092181921005249\n",
      "2.641594409942627\n",
      "2.8537817001342773\n",
      "3.208078622817993\n",
      "2.9597134590148926\n",
      "2.7834455966949463\n",
      "3.2595105171203613\n",
      "3.336095094680786\n",
      "3.065124750137329\n",
      "3.450840473175049\n",
      "3.3550851345062256\n",
      "3.0768821239471436\n",
      "2.9591734409332275\n",
      "3.057673692703247\n",
      "3.1813907623291016\n",
      "3.2400903701782227\n",
      "3.2618985176086426\n",
      "3.302281379699707\n",
      "2.936216115951538\n",
      "3.2029004096984863\n",
      "3.4098315238952637\n",
      "2.9260854721069336\n",
      "3.2500667572021484\n",
      "2.9053807258605957\n",
      "2.904707431793213\n",
      "3.2068049907684326\n",
      "2.875102996826172\n",
      "2.663327932357788\n",
      "3.5713822841644287\n",
      "3.801652193069458\n",
      "3.2141659259796143\n",
      "3.371941328048706\n",
      "3.3269314765930176\n",
      "3.4479217529296875\n",
      "2.995840072631836\n",
      "2.872690439224243\n",
      "3.038038492202759\n",
      "3.186959981918335\n",
      "2.973416805267334\n",
      "3.7320940494537354\n",
      "3.8984854221343994\n",
      "3.547360420227051\n",
      "3.749753475189209\n",
      "3.065692663192749\n",
      "3.382138967514038\n",
      "3.1412901878356934\n",
      "3.2028987407684326\n",
      "4.225534915924072\n",
      "3.623171329498291\n",
      "2.930133104324341\n",
      "4.397397994995117\n",
      "6.627689838409424\n",
      "3.7188425064086914\n",
      "2.8027124404907227\n",
      "3.539475440979004\n",
      "3.582106590270996\n",
      "3.526055097579956\n",
      "2.5030617713928223\n",
      "3.4705729484558105\n",
      "4.197458267211914\n",
      "3.7640492916107178\n",
      "4.171442031860352\n",
      "4.6489081382751465\n",
      "4.098506927490234\n",
      "4.348698139190674\n",
      "6.269989013671875\n",
      "2.7183735370635986\n",
      "2.92693829536438\n",
      "3.569701910018921\n",
      "3.348029613494873\n",
      "4.004700183868408\n",
      "3.6378495693206787\n",
      "4.300676345825195\n",
      "2.989067792892456\n",
      "3.7003111839294434\n",
      "3.12221622467041\n",
      "3.0985796451568604\n",
      "4.596161365509033\n",
      "4.5249223709106445\n",
      "4.079718589782715\n",
      "3.802676200866699\n",
      "3.4345760345458984\n",
      "4.463191032409668\n",
      "4.508098125457764\n",
      "4.217094898223877\n",
      "3.66221284866333\n",
      "4.148366451263428\n",
      "3.742318630218506\n",
      "3.572326898574829\n",
      "5.629659175872803\n",
      "4.176334857940674\n",
      "3.746568441390991\n",
      "3.4265737533569336\n",
      "3.9684529304504395\n",
      "4.663218021392822\n",
      "4.6346611976623535\n",
      "3.799272298812866\n",
      "3.798677682876587\n",
      "3.859027862548828\n",
      "3.6359617710113525\n",
      "4.126898765563965\n",
      "3.913814067840576\n",
      "5.007375240325928\n",
      "5.27132511138916\n",
      "4.705297470092773\n",
      "2.8141536712646484\n",
      "4.087805271148682\n",
      "4.685781478881836\n",
      "4.1032819747924805\n",
      "3.9574363231658936\n",
      "4.46645975112915\n",
      "3.866384506225586\n",
      "4.259509086608887\n",
      "5.4830427169799805\n",
      "4.485713005065918\n",
      "4.091485500335693\n",
      "4.39585018157959\n",
      "3.9560091495513916\n",
      "4.734218120574951\n",
      "3.7368884086608887\n",
      "4.640747547149658\n",
      "5.3416008949279785\n",
      "7.027010917663574\n",
      "7.871242046356201\n",
      "4.340160369873047\n",
      "6.488714218139648\n",
      "4.551805019378662\n",
      "3.902722120285034\n",
      "3.0063202381134033\n",
      "5.14277458190918\n",
      "5.461502552032471\n",
      "6.21954345703125\n",
      "4.737695693969727\n",
      "4.221527099609375\n",
      "6.100430011749268\n",
      "7.1236891746521\n",
      "4.792016983032227\n",
      "3.927213191986084\n",
      "5.403700828552246\n",
      "5.242435932159424\n",
      "5.548103332519531\n",
      "5.109302520751953\n",
      "5.625058174133301\n",
      "4.330166339874268\n",
      "5.383699417114258\n",
      "4.195777893066406\n",
      "5.397459030151367\n",
      "5.144838333129883\n",
      "5.547060012817383\n",
      "4.64023494720459\n",
      "4.7050580978393555\n",
      "5.807024002075195\n",
      "5.186412334442139\n",
      "3.3072280883789062\n",
      "4.081564903259277\n",
      "5.396304130554199\n",
      "6.723133087158203\n",
      "8.496538162231445\n",
      "6.279280185699463\n",
      "7.238388538360596\n",
      "7.326385021209717\n",
      "5.444126129150391\n",
      "5.246634006500244\n",
      "8.029230117797852\n",
      "6.32320499420166\n",
      "7.4239349365234375\n",
      "5.5333733558654785\n",
      "6.8328776359558105\n",
      "6.449943542480469\n",
      "6.436117649078369\n",
      "4.71887731552124\n",
      "5.146304130554199\n",
      "5.665753364562988\n",
      "4.703555107116699\n",
      "5.311215400695801\n",
      "5.539636135101318\n",
      "5.588830947875977\n",
      "6.17918586730957\n",
      "5.703680038452148\n",
      "6.053303241729736\n",
      "6.930912971496582\n",
      "6.361300468444824\n",
      "6.321281909942627\n",
      "6.92774772644043\n",
      "9.807449340820312\n",
      "8.148164749145508\n",
      "6.424483776092529\n",
      "5.477701663970947\n",
      "7.778824806213379\n",
      "6.5075764656066895\n",
      "7.500138759613037\n",
      "6.196828365325928\n",
      "7.609488487243652\n",
      "7.4950175285339355\n",
      "5.423868656158447\n",
      "8.291481971740723\n",
      "6.588149547576904\n",
      "11.984015464782715\n",
      "6.132692337036133\n",
      "6.131196022033691\n",
      "6.175904750823975\n",
      "6.464745998382568\n",
      "9.233644485473633\n",
      "9.488242149353027\n",
      "8.89775276184082\n",
      "8.611867904663086\n",
      "7.336833953857422\n",
      "6.802204132080078\n",
      "5.539957523345947\n",
      "6.984454154968262\n",
      "8.802907943725586\n",
      "9.367476463317871\n",
      "9.931681632995605\n",
      "6.5832648277282715\n",
      "7.203020095825195\n"
     ]
    }
   ],
   "execution_count": 148
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T19:09:35.426197Z",
     "start_time": "2025-07-16T19:09:35.403032Z"
    }
   },
   "cell_type": "code",
   "source": "plt.plot(lri, lossi)",
   "id": "5bf4410ec2ba0a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1cbd4827d10>]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 150
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T19:09:39.770436Z",
     "start_time": "2025-07-16T19:09:39.633997Z"
    }
   },
   "cell_type": "code",
   "source": "lri",
   "id": "12d960bd637ea755",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0010),\n",
       " tensor(0.0010),\n",
       " tensor(0.0010),\n",
       " tensor(0.0010),\n",
       " tensor(0.0010),\n",
       " tensor(0.0010),\n",
       " tensor(0.0010),\n",
       " tensor(0.0010),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0011),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0012),\n",
       " tensor(0.0013),\n",
       " tensor(0.0013),\n",
       " tensor(0.0013),\n",
       " tensor(0.0013),\n",
       " tensor(0.0013),\n",
       " tensor(0.0013),\n",
       " tensor(0.0013),\n",
       " tensor(0.0013),\n",
       " tensor(0.0013),\n",
       " tensor(0.0013),\n",
       " tensor(0.0013),\n",
       " tensor(0.0014),\n",
       " tensor(0.0014),\n",
       " tensor(0.0014),\n",
       " tensor(0.0014),\n",
       " tensor(0.0014),\n",
       " tensor(0.0014),\n",
       " tensor(0.0014),\n",
       " tensor(0.0014),\n",
       " tensor(0.0014),\n",
       " tensor(0.0014),\n",
       " tensor(0.0015),\n",
       " tensor(0.0015),\n",
       " tensor(0.0015),\n",
       " tensor(0.0015),\n",
       " tensor(0.0015),\n",
       " tensor(0.0015),\n",
       " tensor(0.0015),\n",
       " tensor(0.0015),\n",
       " tensor(0.0015),\n",
       " tensor(0.0015),\n",
       " tensor(0.0016),\n",
       " tensor(0.0016),\n",
       " tensor(0.0016),\n",
       " tensor(0.0016),\n",
       " tensor(0.0016),\n",
       " tensor(0.0016),\n",
       " tensor(0.0016),\n",
       " tensor(0.0016),\n",
       " tensor(0.0016),\n",
       " tensor(0.0017),\n",
       " tensor(0.0017),\n",
       " tensor(0.0017),\n",
       " tensor(0.0017),\n",
       " tensor(0.0017),\n",
       " tensor(0.0017),\n",
       " tensor(0.0017),\n",
       " tensor(0.0017),\n",
       " tensor(0.0018),\n",
       " tensor(0.0018),\n",
       " tensor(0.0018),\n",
       " tensor(0.0018),\n",
       " tensor(0.0018),\n",
       " tensor(0.0018),\n",
       " tensor(0.0018),\n",
       " tensor(0.0018),\n",
       " tensor(0.0019),\n",
       " tensor(0.0019),\n",
       " tensor(0.0019),\n",
       " tensor(0.0019),\n",
       " tensor(0.0019),\n",
       " tensor(0.0019),\n",
       " tensor(0.0019),\n",
       " tensor(0.0019),\n",
       " tensor(0.0020),\n",
       " tensor(0.0020),\n",
       " tensor(0.0020),\n",
       " tensor(0.0020),\n",
       " tensor(0.0020),\n",
       " tensor(0.0020),\n",
       " tensor(0.0020),\n",
       " tensor(0.0021),\n",
       " tensor(0.0021),\n",
       " tensor(0.0021),\n",
       " tensor(0.0021),\n",
       " tensor(0.0021),\n",
       " tensor(0.0021),\n",
       " tensor(0.0021),\n",
       " tensor(0.0022),\n",
       " tensor(0.0022),\n",
       " tensor(0.0022),\n",
       " tensor(0.0022),\n",
       " tensor(0.0022),\n",
       " tensor(0.0022),\n",
       " tensor(0.0022),\n",
       " tensor(0.0023),\n",
       " tensor(0.0023),\n",
       " tensor(0.0023),\n",
       " tensor(0.0023),\n",
       " tensor(0.0023),\n",
       " tensor(0.0023),\n",
       " tensor(0.0024),\n",
       " tensor(0.0024),\n",
       " tensor(0.0024),\n",
       " tensor(0.0024),\n",
       " tensor(0.0024),\n",
       " tensor(0.0024),\n",
       " tensor(0.0025),\n",
       " tensor(0.0025),\n",
       " tensor(0.0025),\n",
       " tensor(0.0025),\n",
       " tensor(0.0025),\n",
       " tensor(0.0025),\n",
       " tensor(0.0026),\n",
       " tensor(0.0026),\n",
       " tensor(0.0026),\n",
       " tensor(0.0026),\n",
       " tensor(0.0026),\n",
       " tensor(0.0027),\n",
       " tensor(0.0027),\n",
       " tensor(0.0027),\n",
       " tensor(0.0027),\n",
       " tensor(0.0027),\n",
       " tensor(0.0027),\n",
       " tensor(0.0028),\n",
       " tensor(0.0028),\n",
       " tensor(0.0028),\n",
       " tensor(0.0028),\n",
       " tensor(0.0028),\n",
       " tensor(0.0029),\n",
       " tensor(0.0029),\n",
       " tensor(0.0029),\n",
       " tensor(0.0029),\n",
       " tensor(0.0029),\n",
       " tensor(0.0030),\n",
       " tensor(0.0030),\n",
       " tensor(0.0030),\n",
       " tensor(0.0030),\n",
       " tensor(0.0030),\n",
       " tensor(0.0031),\n",
       " tensor(0.0031),\n",
       " tensor(0.0031),\n",
       " tensor(0.0031),\n",
       " tensor(0.0032),\n",
       " tensor(0.0032),\n",
       " tensor(0.0032),\n",
       " tensor(0.0032),\n",
       " tensor(0.0032),\n",
       " tensor(0.0033),\n",
       " tensor(0.0033),\n",
       " tensor(0.0033),\n",
       " tensor(0.0033),\n",
       " tensor(0.0034),\n",
       " tensor(0.0034),\n",
       " tensor(0.0034),\n",
       " tensor(0.0034),\n",
       " tensor(0.0034),\n",
       " tensor(0.0035),\n",
       " tensor(0.0035),\n",
       " tensor(0.0035),\n",
       " tensor(0.0035),\n",
       " tensor(0.0036),\n",
       " tensor(0.0036),\n",
       " tensor(0.0036),\n",
       " tensor(0.0036),\n",
       " tensor(0.0037),\n",
       " tensor(0.0037),\n",
       " tensor(0.0037),\n",
       " tensor(0.0037),\n",
       " tensor(0.0038),\n",
       " tensor(0.0038),\n",
       " tensor(0.0038),\n",
       " tensor(0.0039),\n",
       " tensor(0.0039),\n",
       " tensor(0.0039),\n",
       " tensor(0.0039),\n",
       " tensor(0.0040),\n",
       " tensor(0.0040),\n",
       " tensor(0.0040),\n",
       " tensor(0.0040),\n",
       " tensor(0.0041),\n",
       " tensor(0.0041),\n",
       " tensor(0.0041),\n",
       " tensor(0.0042),\n",
       " tensor(0.0042),\n",
       " tensor(0.0042),\n",
       " tensor(0.0042),\n",
       " tensor(0.0043),\n",
       " tensor(0.0043),\n",
       " tensor(0.0043),\n",
       " tensor(0.0044),\n",
       " tensor(0.0044),\n",
       " tensor(0.0044),\n",
       " tensor(0.0045),\n",
       " tensor(0.0045),\n",
       " tensor(0.0045),\n",
       " tensor(0.0045),\n",
       " tensor(0.0046),\n",
       " tensor(0.0046),\n",
       " tensor(0.0046),\n",
       " tensor(0.0047),\n",
       " tensor(0.0047),\n",
       " tensor(0.0047),\n",
       " tensor(0.0048),\n",
       " tensor(0.0048),\n",
       " tensor(0.0048),\n",
       " tensor(0.0049),\n",
       " tensor(0.0049),\n",
       " tensor(0.0049),\n",
       " tensor(0.0050),\n",
       " tensor(0.0050),\n",
       " tensor(0.0050),\n",
       " tensor(0.0051),\n",
       " tensor(0.0051),\n",
       " tensor(0.0051),\n",
       " tensor(0.0052),\n",
       " tensor(0.0052),\n",
       " tensor(0.0053),\n",
       " tensor(0.0053),\n",
       " tensor(0.0053),\n",
       " tensor(0.0054),\n",
       " tensor(0.0054),\n",
       " tensor(0.0054),\n",
       " tensor(0.0055),\n",
       " tensor(0.0055),\n",
       " tensor(0.0056),\n",
       " tensor(0.0056),\n",
       " tensor(0.0056),\n",
       " tensor(0.0057),\n",
       " tensor(0.0057),\n",
       " tensor(0.0058),\n",
       " tensor(0.0058),\n",
       " tensor(0.0058),\n",
       " tensor(0.0059),\n",
       " tensor(0.0059),\n",
       " tensor(0.0060),\n",
       " tensor(0.0060),\n",
       " tensor(0.0060),\n",
       " tensor(0.0061),\n",
       " tensor(0.0061),\n",
       " tensor(0.0062),\n",
       " tensor(0.0062),\n",
       " tensor(0.0062),\n",
       " tensor(0.0063),\n",
       " tensor(0.0063),\n",
       " tensor(0.0064),\n",
       " tensor(0.0064),\n",
       " tensor(0.0065),\n",
       " tensor(0.0065),\n",
       " tensor(0.0066),\n",
       " tensor(0.0066),\n",
       " tensor(0.0067),\n",
       " tensor(0.0067),\n",
       " tensor(0.0067),\n",
       " tensor(0.0068),\n",
       " tensor(0.0068),\n",
       " tensor(0.0069),\n",
       " tensor(0.0069),\n",
       " tensor(0.0070),\n",
       " tensor(0.0070),\n",
       " tensor(0.0071),\n",
       " tensor(0.0071),\n",
       " tensor(0.0072),\n",
       " tensor(0.0072),\n",
       " tensor(0.0073),\n",
       " tensor(0.0073),\n",
       " tensor(0.0074),\n",
       " tensor(0.0074),\n",
       " tensor(0.0075),\n",
       " tensor(0.0075),\n",
       " tensor(0.0076),\n",
       " tensor(0.0076),\n",
       " tensor(0.0077),\n",
       " tensor(0.0077),\n",
       " tensor(0.0078),\n",
       " tensor(0.0079),\n",
       " tensor(0.0079),\n",
       " tensor(0.0080),\n",
       " tensor(0.0080),\n",
       " tensor(0.0081),\n",
       " tensor(0.0081),\n",
       " tensor(0.0082),\n",
       " tensor(0.0082),\n",
       " tensor(0.0083),\n",
       " tensor(0.0084),\n",
       " tensor(0.0084),\n",
       " tensor(0.0085),\n",
       " tensor(0.0085),\n",
       " tensor(0.0086),\n",
       " tensor(0.0086),\n",
       " tensor(0.0087),\n",
       " tensor(0.0088),\n",
       " tensor(0.0088),\n",
       " tensor(0.0089),\n",
       " tensor(0.0090),\n",
       " tensor(0.0090),\n",
       " tensor(0.0091),\n",
       " tensor(0.0091),\n",
       " tensor(0.0092),\n",
       " tensor(0.0093),\n",
       " tensor(0.0093),\n",
       " tensor(0.0094),\n",
       " tensor(0.0095),\n",
       " tensor(0.0095),\n",
       " tensor(0.0096),\n",
       " tensor(0.0097),\n",
       " tensor(0.0097),\n",
       " tensor(0.0098),\n",
       " tensor(0.0099),\n",
       " tensor(0.0099),\n",
       " tensor(0.0100),\n",
       " tensor(0.0101),\n",
       " tensor(0.0101),\n",
       " tensor(0.0102),\n",
       " tensor(0.0103),\n",
       " tensor(0.0104),\n",
       " tensor(0.0104),\n",
       " tensor(0.0105),\n",
       " tensor(0.0106),\n",
       " tensor(0.0106),\n",
       " tensor(0.0107),\n",
       " tensor(0.0108),\n",
       " tensor(0.0109),\n",
       " tensor(0.0109),\n",
       " tensor(0.0110),\n",
       " tensor(0.0111),\n",
       " tensor(0.0112),\n",
       " tensor(0.0112),\n",
       " tensor(0.0113),\n",
       " tensor(0.0114),\n",
       " tensor(0.0115),\n",
       " tensor(0.0116),\n",
       " tensor(0.0116),\n",
       " tensor(0.0117),\n",
       " tensor(0.0118),\n",
       " tensor(0.0119),\n",
       " tensor(0.0120),\n",
       " tensor(0.0121),\n",
       " tensor(0.0121),\n",
       " tensor(0.0122),\n",
       " tensor(0.0123),\n",
       " tensor(0.0124),\n",
       " tensor(0.0125),\n",
       " tensor(0.0126),\n",
       " tensor(0.0127),\n",
       " tensor(0.0127),\n",
       " tensor(0.0128),\n",
       " tensor(0.0129),\n",
       " tensor(0.0130),\n",
       " tensor(0.0131),\n",
       " tensor(0.0132),\n",
       " tensor(0.0133),\n",
       " tensor(0.0134),\n",
       " tensor(0.0135),\n",
       " tensor(0.0136),\n",
       " tensor(0.0137),\n",
       " tensor(0.0137),\n",
       " tensor(0.0138),\n",
       " tensor(0.0139),\n",
       " tensor(0.0140),\n",
       " tensor(0.0141),\n",
       " tensor(0.0142),\n",
       " tensor(0.0143),\n",
       " tensor(0.0144),\n",
       " tensor(0.0145),\n",
       " tensor(0.0146),\n",
       " tensor(0.0147),\n",
       " tensor(0.0148),\n",
       " tensor(0.0149),\n",
       " tensor(0.0150),\n",
       " tensor(0.0151),\n",
       " tensor(0.0152),\n",
       " tensor(0.0154),\n",
       " tensor(0.0155),\n",
       " tensor(0.0156),\n",
       " tensor(0.0157),\n",
       " tensor(0.0158),\n",
       " tensor(0.0159),\n",
       " tensor(0.0160),\n",
       " tensor(0.0161),\n",
       " tensor(0.0162),\n",
       " tensor(0.0163),\n",
       " tensor(0.0165),\n",
       " tensor(0.0166),\n",
       " tensor(0.0167),\n",
       " tensor(0.0168),\n",
       " tensor(0.0169),\n",
       " tensor(0.0170),\n",
       " tensor(0.0171),\n",
       " tensor(0.0173),\n",
       " tensor(0.0174),\n",
       " tensor(0.0175),\n",
       " tensor(0.0176),\n",
       " tensor(0.0178),\n",
       " tensor(0.0179),\n",
       " tensor(0.0180),\n",
       " tensor(0.0181),\n",
       " tensor(0.0182),\n",
       " tensor(0.0184),\n",
       " tensor(0.0185),\n",
       " tensor(0.0186),\n",
       " tensor(0.0188),\n",
       " tensor(0.0189),\n",
       " tensor(0.0190),\n",
       " tensor(0.0192),\n",
       " tensor(0.0193),\n",
       " tensor(0.0194),\n",
       " tensor(0.0196),\n",
       " tensor(0.0197),\n",
       " tensor(0.0198),\n",
       " tensor(0.0200),\n",
       " tensor(0.0201),\n",
       " tensor(0.0202),\n",
       " tensor(0.0204),\n",
       " tensor(0.0205),\n",
       " tensor(0.0207),\n",
       " tensor(0.0208),\n",
       " tensor(0.0210),\n",
       " tensor(0.0211),\n",
       " tensor(0.0212),\n",
       " tensor(0.0214),\n",
       " tensor(0.0215),\n",
       " tensor(0.0217),\n",
       " tensor(0.0218),\n",
       " tensor(0.0220),\n",
       " tensor(0.0221),\n",
       " tensor(0.0223),\n",
       " tensor(0.0225),\n",
       " tensor(0.0226),\n",
       " tensor(0.0228),\n",
       " tensor(0.0229),\n",
       " tensor(0.0231),\n",
       " tensor(0.0232),\n",
       " tensor(0.0234),\n",
       " tensor(0.0236),\n",
       " tensor(0.0237),\n",
       " tensor(0.0239),\n",
       " tensor(0.0241),\n",
       " tensor(0.0242),\n",
       " tensor(0.0244),\n",
       " tensor(0.0246),\n",
       " tensor(0.0247),\n",
       " tensor(0.0249),\n",
       " tensor(0.0251),\n",
       " tensor(0.0253),\n",
       " tensor(0.0254),\n",
       " tensor(0.0256),\n",
       " tensor(0.0258),\n",
       " tensor(0.0260),\n",
       " tensor(0.0261),\n",
       " tensor(0.0263),\n",
       " tensor(0.0265),\n",
       " tensor(0.0267),\n",
       " tensor(0.0269),\n",
       " tensor(0.0271),\n",
       " tensor(0.0273),\n",
       " tensor(0.0274),\n",
       " tensor(0.0276),\n",
       " tensor(0.0278),\n",
       " tensor(0.0280),\n",
       " tensor(0.0282),\n",
       " tensor(0.0284),\n",
       " tensor(0.0286),\n",
       " tensor(0.0288),\n",
       " tensor(0.0290),\n",
       " tensor(0.0292),\n",
       " tensor(0.0294),\n",
       " tensor(0.0296),\n",
       " tensor(0.0298),\n",
       " tensor(0.0300),\n",
       " tensor(0.0302),\n",
       " tensor(0.0304),\n",
       " tensor(0.0307),\n",
       " tensor(0.0309),\n",
       " tensor(0.0311),\n",
       " tensor(0.0313),\n",
       " tensor(0.0315),\n",
       " tensor(0.0317),\n",
       " tensor(0.0320),\n",
       " tensor(0.0322),\n",
       " tensor(0.0324),\n",
       " tensor(0.0326),\n",
       " tensor(0.0328),\n",
       " tensor(0.0331),\n",
       " tensor(0.0333),\n",
       " tensor(0.0335),\n",
       " tensor(0.0338),\n",
       " tensor(0.0340),\n",
       " tensor(0.0342),\n",
       " tensor(0.0345),\n",
       " tensor(0.0347),\n",
       " tensor(0.0350),\n",
       " tensor(0.0352),\n",
       " tensor(0.0354),\n",
       " tensor(0.0357),\n",
       " tensor(0.0359),\n",
       " tensor(0.0362),\n",
       " tensor(0.0364),\n",
       " tensor(0.0367),\n",
       " tensor(0.0369),\n",
       " tensor(0.0372),\n",
       " tensor(0.0375),\n",
       " tensor(0.0377),\n",
       " tensor(0.0380),\n",
       " tensor(0.0382),\n",
       " tensor(0.0385),\n",
       " tensor(0.0388),\n",
       " tensor(0.0390),\n",
       " tensor(0.0393),\n",
       " tensor(0.0396),\n",
       " tensor(0.0399),\n",
       " tensor(0.0401),\n",
       " tensor(0.0404),\n",
       " tensor(0.0407),\n",
       " tensor(0.0410),\n",
       " tensor(0.0413),\n",
       " tensor(0.0416),\n",
       " tensor(0.0418),\n",
       " tensor(0.0421),\n",
       " tensor(0.0424),\n",
       " tensor(0.0427),\n",
       " tensor(0.0430),\n",
       " tensor(0.0433),\n",
       " tensor(0.0436),\n",
       " tensor(0.0439),\n",
       " tensor(0.0442),\n",
       " tensor(0.0445),\n",
       " tensor(0.0448),\n",
       " tensor(0.0451),\n",
       " tensor(0.0455),\n",
       " tensor(0.0458),\n",
       " tensor(0.0461),\n",
       " tensor(0.0464),\n",
       " tensor(0.0467),\n",
       " tensor(0.0471),\n",
       " tensor(0.0474),\n",
       " tensor(0.0477),\n",
       " tensor(0.0480),\n",
       " tensor(0.0484),\n",
       " tensor(0.0487),\n",
       " tensor(0.0491),\n",
       " tensor(0.0494),\n",
       " tensor(0.0497),\n",
       " tensor(0.0501),\n",
       " tensor(0.0504),\n",
       " tensor(0.0508),\n",
       " tensor(0.0511),\n",
       " tensor(0.0515),\n",
       " tensor(0.0518),\n",
       " tensor(0.0522),\n",
       " tensor(0.0526),\n",
       " tensor(0.0529),\n",
       " tensor(0.0533),\n",
       " tensor(0.0537),\n",
       " tensor(0.0540),\n",
       " tensor(0.0544),\n",
       " tensor(0.0548),\n",
       " tensor(0.0552),\n",
       " tensor(0.0556),\n",
       " tensor(0.0559),\n",
       " tensor(0.0563),\n",
       " tensor(0.0567),\n",
       " tensor(0.0571),\n",
       " tensor(0.0575),\n",
       " tensor(0.0579),\n",
       " tensor(0.0583),\n",
       " tensor(0.0587),\n",
       " tensor(0.0591),\n",
       " tensor(0.0595),\n",
       " tensor(0.0599),\n",
       " tensor(0.0604),\n",
       " tensor(0.0608),\n",
       " tensor(0.0612),\n",
       " tensor(0.0616),\n",
       " tensor(0.0621),\n",
       " tensor(0.0625),\n",
       " tensor(0.0629),\n",
       " tensor(0.0634),\n",
       " tensor(0.0638),\n",
       " tensor(0.0642),\n",
       " tensor(0.0647),\n",
       " tensor(0.0651),\n",
       " tensor(0.0656),\n",
       " tensor(0.0660),\n",
       " tensor(0.0665),\n",
       " tensor(0.0670),\n",
       " tensor(0.0674),\n",
       " tensor(0.0679),\n",
       " tensor(0.0684),\n",
       " tensor(0.0688),\n",
       " tensor(0.0693),\n",
       " tensor(0.0698),\n",
       " tensor(0.0703),\n",
       " tensor(0.0708),\n",
       " tensor(0.0713),\n",
       " tensor(0.0718),\n",
       " tensor(0.0723),\n",
       " tensor(0.0728),\n",
       " tensor(0.0733),\n",
       " tensor(0.0738),\n",
       " tensor(0.0743),\n",
       " tensor(0.0748),\n",
       " tensor(0.0753),\n",
       " tensor(0.0758),\n",
       " tensor(0.0764),\n",
       " tensor(0.0769),\n",
       " tensor(0.0774),\n",
       " tensor(0.0780),\n",
       " tensor(0.0785),\n",
       " tensor(0.0790),\n",
       " tensor(0.0796),\n",
       " tensor(0.0802),\n",
       " tensor(0.0807),\n",
       " tensor(0.0813),\n",
       " tensor(0.0818),\n",
       " tensor(0.0824),\n",
       " tensor(0.0830),\n",
       " tensor(0.0835),\n",
       " tensor(0.0841),\n",
       " tensor(0.0847),\n",
       " tensor(0.0853),\n",
       " tensor(0.0859),\n",
       " tensor(0.0865),\n",
       " tensor(0.0871),\n",
       " tensor(0.0877),\n",
       " tensor(0.0883),\n",
       " tensor(0.0889),\n",
       " tensor(0.0895),\n",
       " tensor(0.0901),\n",
       " tensor(0.0908),\n",
       " tensor(0.0914),\n",
       " tensor(0.0920),\n",
       " tensor(0.0927),\n",
       " tensor(0.0933),\n",
       " tensor(0.0940),\n",
       " tensor(0.0946),\n",
       " tensor(0.0953),\n",
       " tensor(0.0959),\n",
       " tensor(0.0966),\n",
       " tensor(0.0973),\n",
       " tensor(0.0979),\n",
       " tensor(0.0986),\n",
       " tensor(0.0993),\n",
       " tensor(0.1000),\n",
       " tensor(0.1007),\n",
       " tensor(0.1014),\n",
       " tensor(0.1021),\n",
       " tensor(0.1028),\n",
       " tensor(0.1035),\n",
       " tensor(0.1042),\n",
       " tensor(0.1050),\n",
       " tensor(0.1057),\n",
       " tensor(0.1064),\n",
       " tensor(0.1072),\n",
       " tensor(0.1079),\n",
       " tensor(0.1087),\n",
       " tensor(0.1094),\n",
       " tensor(0.1102),\n",
       " tensor(0.1109),\n",
       " tensor(0.1117),\n",
       " tensor(0.1125),\n",
       " tensor(0.1133),\n",
       " tensor(0.1140),\n",
       " tensor(0.1148),\n",
       " tensor(0.1156),\n",
       " tensor(0.1164),\n",
       " tensor(0.1172),\n",
       " tensor(0.1181),\n",
       " tensor(0.1189),\n",
       " tensor(0.1197),\n",
       " tensor(0.1205),\n",
       " tensor(0.1214),\n",
       " tensor(0.1222),\n",
       " tensor(0.1231),\n",
       " tensor(0.1239),\n",
       " tensor(0.1248),\n",
       " tensor(0.1256),\n",
       " tensor(0.1265),\n",
       " tensor(0.1274),\n",
       " tensor(0.1283),\n",
       " tensor(0.1292),\n",
       " tensor(0.1301),\n",
       " tensor(0.1310),\n",
       " tensor(0.1319),\n",
       " tensor(0.1328),\n",
       " tensor(0.1337),\n",
       " tensor(0.1346),\n",
       " tensor(0.1356),\n",
       " tensor(0.1365),\n",
       " tensor(0.1374),\n",
       " tensor(0.1384),\n",
       " tensor(0.1394),\n",
       " tensor(0.1403),\n",
       " tensor(0.1413),\n",
       " tensor(0.1423),\n",
       " tensor(0.1433),\n",
       " tensor(0.1443),\n",
       " tensor(0.1453),\n",
       " tensor(0.1463),\n",
       " tensor(0.1473),\n",
       " tensor(0.1483),\n",
       " tensor(0.1493),\n",
       " tensor(0.1504),\n",
       " tensor(0.1514),\n",
       " tensor(0.1525),\n",
       " tensor(0.1535),\n",
       " tensor(0.1546),\n",
       " tensor(0.1557),\n",
       " tensor(0.1567),\n",
       " tensor(0.1578),\n",
       " tensor(0.1589),\n",
       " tensor(0.1600),\n",
       " tensor(0.1611),\n",
       " tensor(0.1623),\n",
       " tensor(0.1634),\n",
       " tensor(0.1645),\n",
       " tensor(0.1657),\n",
       " tensor(0.1668),\n",
       " tensor(0.1680),\n",
       " tensor(0.1691),\n",
       " tensor(0.1703),\n",
       " tensor(0.1715),\n",
       " tensor(0.1727),\n",
       " tensor(0.1739),\n",
       " tensor(0.1751),\n",
       " tensor(0.1763),\n",
       " tensor(0.1775),\n",
       " tensor(0.1788),\n",
       " tensor(0.1800),\n",
       " tensor(0.1812),\n",
       " tensor(0.1825),\n",
       " tensor(0.1838),\n",
       " tensor(0.1850),\n",
       " tensor(0.1863),\n",
       " tensor(0.1876),\n",
       " tensor(0.1889),\n",
       " tensor(0.1902),\n",
       " tensor(0.1916),\n",
       " tensor(0.1929),\n",
       " tensor(0.1942),\n",
       " tensor(0.1956),\n",
       " tensor(0.1969),\n",
       " tensor(0.1983),\n",
       " tensor(0.1997),\n",
       " tensor(0.2010),\n",
       " tensor(0.2024),\n",
       " tensor(0.2038),\n",
       " tensor(0.2053),\n",
       " tensor(0.2067),\n",
       " tensor(0.2081),\n",
       " tensor(0.2096),\n",
       " tensor(0.2110),\n",
       " tensor(0.2125),\n",
       " tensor(0.2140),\n",
       " tensor(0.2154),\n",
       " tensor(0.2169),\n",
       " tensor(0.2184),\n",
       " tensor(0.2200),\n",
       " tensor(0.2215),\n",
       " tensor(0.2230),\n",
       " tensor(0.2246),\n",
       " tensor(0.2261),\n",
       " tensor(0.2277),\n",
       " tensor(0.2293),\n",
       " tensor(0.2309),\n",
       " tensor(0.2325),\n",
       " tensor(0.2341),\n",
       " tensor(0.2357),\n",
       " tensor(0.2373),\n",
       " tensor(0.2390),\n",
       " tensor(0.2406),\n",
       " tensor(0.2423),\n",
       " tensor(0.2440),\n",
       " tensor(0.2457),\n",
       " tensor(0.2474),\n",
       " tensor(0.2491),\n",
       " tensor(0.2508),\n",
       " tensor(0.2526),\n",
       " tensor(0.2543),\n",
       " tensor(0.2561),\n",
       " tensor(0.2579),\n",
       " tensor(0.2597),\n",
       " tensor(0.2615),\n",
       " tensor(0.2633),\n",
       " tensor(0.2651),\n",
       " tensor(0.2669),\n",
       " tensor(0.2688),\n",
       " tensor(0.2707),\n",
       " tensor(0.2725),\n",
       " tensor(0.2744),\n",
       " tensor(0.2763),\n",
       " tensor(0.2783),\n",
       " tensor(0.2802),\n",
       " tensor(0.2821),\n",
       " tensor(0.2841),\n",
       " tensor(0.2861),\n",
       " tensor(0.2880),\n",
       " tensor(0.2900),\n",
       " tensor(0.2921),\n",
       " tensor(0.2941),\n",
       " tensor(0.2961),\n",
       " tensor(0.2982),\n",
       " tensor(0.3002),\n",
       " tensor(0.3023),\n",
       " tensor(0.3044),\n",
       " tensor(0.3065),\n",
       " tensor(0.3087),\n",
       " tensor(0.3108),\n",
       " tensor(0.3130),\n",
       " tensor(0.3151),\n",
       " tensor(0.3173),\n",
       " tensor(0.3195),\n",
       " tensor(0.3217),\n",
       " tensor(0.3240),\n",
       " tensor(0.3262),\n",
       " tensor(0.3285),\n",
       " tensor(0.3308),\n",
       " tensor(0.3331),\n",
       " tensor(0.3354),\n",
       " tensor(0.3377),\n",
       " tensor(0.3400),\n",
       " tensor(0.3424),\n",
       " tensor(0.3448),\n",
       " tensor(0.3472),\n",
       " tensor(0.3496),\n",
       " tensor(0.3520),\n",
       " tensor(0.3544),\n",
       " tensor(0.3569),\n",
       " tensor(0.3594),\n",
       " tensor(0.3619),\n",
       " tensor(0.3644),\n",
       " tensor(0.3669),\n",
       " tensor(0.3695),\n",
       " tensor(0.3720),\n",
       " tensor(0.3746),\n",
       " tensor(0.3772),\n",
       " tensor(0.3798),\n",
       " tensor(0.3825),\n",
       " tensor(0.3851),\n",
       " tensor(0.3878),\n",
       " tensor(0.3905),\n",
       " tensor(0.3932),\n",
       " tensor(0.3959),\n",
       " tensor(0.3987),\n",
       " tensor(0.4014),\n",
       " tensor(0.4042),\n",
       " tensor(0.4070),\n",
       " tensor(0.4098),\n",
       " tensor(0.4127),\n",
       " tensor(0.4155),\n",
       " tensor(0.4184),\n",
       " tensor(0.4213),\n",
       " tensor(0.4243),\n",
       " tensor(0.4272),\n",
       " tensor(0.4302),\n",
       " tensor(0.4331),\n",
       " tensor(0.4362),\n",
       " tensor(0.4392),\n",
       " tensor(0.4422),\n",
       " tensor(0.4453),\n",
       " tensor(0.4484),\n",
       " tensor(0.4515),\n",
       " tensor(0.4546),\n",
       " tensor(0.4578),\n",
       " tensor(0.4610),\n",
       " tensor(0.4642),\n",
       " tensor(0.4674),\n",
       " tensor(0.4706),\n",
       " tensor(0.4739),\n",
       " tensor(0.4772),\n",
       " tensor(0.4805),\n",
       " tensor(0.4838),\n",
       " tensor(0.4872),\n",
       " tensor(0.4906),\n",
       " tensor(0.4940),\n",
       " tensor(0.4974),\n",
       " tensor(0.5008),\n",
       " tensor(0.5043),\n",
       " tensor(0.5078),\n",
       " tensor(0.5113),\n",
       " tensor(0.5149),\n",
       " tensor(0.5185),\n",
       " tensor(0.5221),\n",
       " tensor(0.5257),\n",
       " tensor(0.5293),\n",
       " tensor(0.5330),\n",
       " tensor(0.5367),\n",
       " tensor(0.5404),\n",
       " tensor(0.5442),\n",
       " tensor(0.5479),\n",
       " tensor(0.5517),\n",
       " tensor(0.5556),\n",
       " tensor(0.5594),\n",
       " tensor(0.5633),\n",
       " tensor(0.5672),\n",
       " tensor(0.5712),\n",
       " tensor(0.5751),\n",
       " tensor(0.5791),\n",
       " tensor(0.5831),\n",
       " tensor(0.5872),\n",
       " tensor(0.5913),\n",
       " tensor(0.5954),\n",
       " tensor(0.5995),\n",
       " tensor(0.6036),\n",
       " tensor(0.6078),\n",
       " tensor(0.6120),\n",
       " tensor(0.6163),\n",
       " tensor(0.6206),\n",
       " tensor(0.6249),\n",
       " tensor(0.6292),\n",
       " tensor(0.6336),\n",
       " tensor(0.6380),\n",
       " tensor(0.6424),\n",
       " tensor(0.6469),\n",
       " tensor(0.6513),\n",
       " tensor(0.6559),\n",
       " tensor(0.6604),\n",
       " tensor(0.6650),\n",
       " tensor(0.6696),\n",
       " tensor(0.6743),\n",
       " tensor(0.6789),\n",
       " tensor(0.6837),\n",
       " tensor(0.6884),\n",
       " tensor(0.6932),\n",
       " tensor(0.6980),\n",
       " tensor(0.7028),\n",
       " tensor(0.7077),\n",
       " tensor(0.7126),\n",
       " tensor(0.7176),\n",
       " tensor(0.7225),\n",
       " tensor(0.7275),\n",
       " tensor(0.7326),\n",
       " tensor(0.7377),\n",
       " tensor(0.7428),\n",
       " tensor(0.7480),\n",
       " tensor(0.7531),\n",
       " tensor(0.7584),\n",
       " tensor(0.7636),\n",
       " tensor(0.7689),\n",
       " tensor(0.7743),\n",
       " tensor(0.7796),\n",
       " tensor(0.7850),\n",
       " tensor(0.7905),\n",
       " tensor(0.7960),\n",
       " tensor(0.8015),\n",
       " tensor(0.8071),\n",
       " tensor(0.8127),\n",
       " tensor(0.8183),\n",
       " tensor(0.8240),\n",
       " tensor(0.8297),\n",
       " tensor(0.8355),\n",
       " tensor(0.8412),\n",
       " tensor(0.8471),\n",
       " tensor(0.8530),\n",
       " tensor(0.8589),\n",
       " tensor(0.8648),\n",
       " tensor(0.8708),\n",
       " tensor(0.8769),\n",
       " tensor(0.8830),\n",
       " tensor(0.8891),\n",
       " tensor(0.8953),\n",
       " tensor(0.9015),\n",
       " tensor(0.9077),\n",
       " tensor(0.9140),\n",
       " tensor(0.9204),\n",
       " tensor(0.9268),\n",
       " tensor(0.9332),\n",
       " tensor(0.9397),\n",
       " tensor(0.9462),\n",
       " tensor(0.9528),\n",
       " tensor(0.9594),\n",
       " tensor(0.9660),\n",
       " tensor(0.9727),\n",
       " tensor(0.9795),\n",
       " tensor(0.9863),\n",
       " tensor(0.9931),\n",
       " tensor(1.)]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 151
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b4f0ee6335a1a163"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
